{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "de78760afabf4fe3acbad747baa457c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_994ac0d021b04353bed46769cd958f16",
              "IPY_MODEL_1dfc5404bf6846db97446a45c39dbeba",
              "IPY_MODEL_6000bc4460ce4892b2ba926003187ddb"
            ],
            "layout": "IPY_MODEL_7f91e102e92540919ffe4dc14e16675e"
          }
        },
        "994ac0d021b04353bed46769cd958f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eab1adc2269441198c6ff75ff20a034c",
            "placeholder": "​",
            "style": "IPY_MODEL_f4c1918a5073422cb24f1f82e0c7e2da",
            "value": "model.safetensors: 100%"
          }
        },
        "1dfc5404bf6846db97446a45c39dbeba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42bd6d6b19894b318f28d52659a6b5e6",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b974f992c2b84c6f94610a2d61337bff",
            "value": 548105171
          }
        },
        "6000bc4460ce4892b2ba926003187ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0bbc6ae61d9442881c1327e940d1957",
            "placeholder": "​",
            "style": "IPY_MODEL_9289b5a499c144959bc6f96c04ab21e1",
            "value": " 548M/548M [00:08&lt;00:00, 122MB/s]"
          }
        },
        "7f91e102e92540919ffe4dc14e16675e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eab1adc2269441198c6ff75ff20a034c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c1918a5073422cb24f1f82e0c7e2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42bd6d6b19894b318f28d52659a6b5e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b974f992c2b84c6f94610a2d61337bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0bbc6ae61d9442881c1327e940d1957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9289b5a499c144959bc6f96c04ab21e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/04-model-quantization/04_gpt_2_quantization_with_auto_gptq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-bit Quantization of GPT-2 with Auto-GPTQ\n"
      ],
      "metadata": {
        "id": "8XxP44PHQUvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code in this notebook is to introduce readers to 4-bit quantization of a decoder-only language model, [GPT-2](https://huggingface.co/openai-community/gpt2), using the [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) library. It requires hardware acceleration.  "
      ],
      "metadata": {
        "id": "N_Q_NSZPrg7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing dependecies (AutGPTQ only)."
      ],
      "metadata": {
        "id": "We_YmB7LQ7Nf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhufqqQAaz6e"
      },
      "outputs": [],
      "source": [
        "!export BUILD_CUDA_EXT=0\n",
        "!pip install auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Force the upgrade to the latest HF's Dataset package. A runtime restart would be probably needed when completed."
      ],
      "metadata": {
        "id": "_9wyW462Cblt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall datasets"
      ],
      "metadata": {
        "id": "rVh_suZSBByc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required classes/packages"
      ],
      "metadata": {
        "id": "Vn0PGpphRCsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import TextGenerationPipeline\n",
        "\n",
        "# from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig"
      ],
      "metadata": {
        "id": "mXM8TuXx60Af"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to load and prepare the test set to be used for model quantization and validation."
      ],
      "metadata": {
        "id": "TPoQu-aORKPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikitext2(nsamples, seed, seqlen, tokenizer):\n",
        "    # set seed\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "\n",
        "    # load dataset and preprocess\n",
        "    traindata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "    testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "    trainenc = tokenizer(\"\\n\\n\".join(traindata[\"text\"]), return_tensors=\"pt\")\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata[\"text\"]), return_tensors=\"pt\")\n",
        "\n",
        "    traindataset = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        attention_mask = torch.ones_like(inp)\n",
        "        traindataset.append({\"input_ids\": inp, \"attention_mask\": attention_mask})\n",
        "    return traindataset, testenc"
      ],
      "metadata": {
        "id": "7v5B-ZQ9bFY-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the model ID in the HF's Hub and the destination directory where to save the quantized model."
      ],
      "metadata": {
        "id": "7QIwiY77RWwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"openai-community/gpt2\"\n",
        "quantized_model_dir = \"gpt-2-4bit\""
      ],
      "metadata": {
        "id": "0gpUPSLx64VY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the model tokenizer from the HF's Hub."
      ],
      "metadata": {
        "id": "tW_4tyJJRft3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
        "except Exception:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
      ],
      "metadata": {
        "id": "j7ccCFIj66RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the quantization configuration."
      ],
      "metadata": {
        "id": "LkoWAXj8RrQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantize_config = GPTQConfig(bits=4, dataset = \"wikitext2\", tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "6Te62THzSEHh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=4,\n",
        "    group_size=128,\n",
        "    desc_act=False,\n",
        ")"
      ],
      "metadata": {
        "id": "ZjdUlqdh6_-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the un-quantized model (it is forced to be loaded into CPU). Then get the maximum sequence lenght for it."
      ],
      "metadata": {
        "id": "2tuQOxD5RvXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantize_config)\n",
        "model_config = model.config.to_dict()\n",
        "seq_len_keys = [\"max_position_embeddings\", \"seq_length\", \"n_positions\"]\n",
        "if any(k in model_config for k in seq_len_keys):\n",
        "    for key in seq_len_keys:\n",
        "        if key in model_config:\n",
        "            model.seqlen = model_config[key]\n",
        "            break\n",
        "else:\n",
        "    print(\"The model's sequence length cannot be retrieved from its configuration. It will then be set to 2048.\")\n",
        "    model.seqlen = 2048"
      ],
      "metadata": {
        "id": "vyYnnSGdbgj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432,
          "referenced_widgets": [
            "de78760afabf4fe3acbad747baa457c8",
            "994ac0d021b04353bed46769cd958f16",
            "1dfc5404bf6846db97446a45c39dbeba",
            "6000bc4460ce4892b2ba926003187ddb",
            "7f91e102e92540919ffe4dc14e16675e",
            "eab1adc2269441198c6ff75ff20a034c",
            "f4c1918a5073422cb24f1f82e0c7e2da",
            "42bd6d6b19894b318f28d52659a6b5e6",
            "b974f992c2b84c6f94610a2d61337bff",
            "d0bbc6ae61d9442881c1327e940d1957",
            "9289b5a499c144959bc6f96c04ab21e1"
          ]
        },
        "outputId": "92dc3ac0-87fb-4c88-81b4-13004e1f167b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de78760afabf4fe3acbad747baa457c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "GPT2LMHeadModel.__init__() takes 2 positional arguments but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3312611274.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mseq_len_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"max_position_embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seq_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_positions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_len_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_len_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5104\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5105\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5106\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5108\u001b[0m         \u001b[0;31m# Make sure to tie the weights correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: GPT2LMHeadModel.__init__() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and prepare the dataset for the quantization process."
      ],
      "metadata": {
        "id": "nJTiZV_fSBJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traindataset, testenc = get_wikitext2(128, 0, model.seqlen, tokenizer)"
      ],
      "metadata": {
        "id": "_uDvU4epblHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantize the model. The examples used should be a list of dict whose keys contains \"input_ids\" and \"attention_mask\"."
      ],
      "metadata": {
        "id": "B2d4N36XSIXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.quantize(traindataset, use_triton=False)"
      ],
      "metadata": {
        "id": "GxjJUWR27Dw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the quantized model to disk."
      ],
      "metadata": {
        "id": "uybeFbU4SWOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_quantized(quantized_model_dir, use_safetensors=True)"
      ],
      "metadata": {
        "id": "Jp1BxVlY7KLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the saved safetensors is 1.02 GB."
      ],
      "metadata": {
        "id": "S-ug6e4mhbFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the quantized model."
      ],
      "metadata": {
        "id": "3S_i8roH7SH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
        "                                           device=\"cuda:0\", use_triton=False)"
      ],
      "metadata": {
        "id": "htdp8my37NJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do inference with the quantized model."
      ],
      "metadata": {
        "id": "iY1IUgluSsHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Auto-GPTQ is\"\n",
        "output = tokenizer.decode(\n",
        "    quantized_model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\"))[0])\n",
        "print(output)"
      ],
      "metadata": {
        "id": "RymnhESo7RmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HF Transformers pipelines are supported too for inference with the 4-bit quantized model."
      ],
      "metadata": {
        "id": "5whCVKeZSuvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer, device=\"cuda:0\")\n",
        "print(pipeline(prompt)[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "61W5tKt4_3V1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight Comparison.\n",
        "The following 5 code cells are meant to display the weights of the original and the 4-bit quantized model in a histogram chart, same way as for the 8-bit quantization case presented in the CH05_NB02_Iozzia.ipynb notebook. Please refer to it for more details."
      ],
      "metadata": {
        "id": "mejncb-TvQdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)"
      ],
      "metadata": {
        "id": "lfS-dS3KyjEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [param.data.clone() for param in model.parameters()]\n",
        "weights_int8 = [param.data.clone() for param in quantized_model.parameters()]"
      ],
      "metadata": {
        "id": "DVdvfHYdvVNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.concatenate([t.cpu().numpy().flatten() for t in weights])\n",
        "weights_int8 = np.concatenate([t.cpu().numpy().flatten() for t in weights_int8])"
      ],
      "metadata": {
        "id": "_a5SLYL7wXbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "metadata": {
        "id": "gWPx93LBv2oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set background style\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Create figure and axes\n",
        "fig, axs = plt.subplots(1, figsize=(10,10), dpi=300, sharex=True)\n",
        "\n",
        "# Plot the histograms for original and zero-point weights\n",
        "axs.hist(weights, bins=150, alpha=0.5, label='Original weights', color='yellow', range=(-0.5, 0.5))\n",
        "axs.hist(weights_int8, bins=150, alpha=0.5, label='LLM.int8() weights', color='blue', range=(-0.5, 0.5))\n",
        "\n",
        "# Add grid\n",
        "axs.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Add legend\n",
        "axs.legend()\n",
        "\n",
        "# Add title and labels\n",
        "axs.set_title('Comparison of Original and LLM.int8() Weights', fontsize=16)\n",
        "\n",
        "axs.set_xlabel('Weights', fontsize=14)\n",
        "axs.set_ylabel('Count', fontsize=14)\n",
        "axs.yaxis.set_major_formatter(ticker.EngFormatter()) # Make y-ticks more human readable\n",
        "\n",
        "# Improve font\n",
        "plt.rc('font', size=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cWXUyGEhwI7B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}