{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/04-model-quantization/03_bert_model_quantization_with_optimum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization of a Finetuned BERT Model with HF's Optimum"
      ],
      "metadata": {
        "id": "ad3y3Qr2z-5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code in this notebook is to introduce readers to the quantization of an encoder-only language model, [distilbert-base-uncased-finetuned-banking77](https://huggingface.co/optimum/distilbert-base-uncased-finetuned-banking77) using the Hugging Face's [Optimum](https://github.com/huggingface/optimum) library. It doesn't require hardware acceleration.  "
      ],
      "metadata": {
        "id": "RdeRQ0jSm6lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing requirements in the Colab VM (only the latest HF's Optimum for the ONNX runtime and Evaluate)."
      ],
      "metadata": {
        "id": "HaeX1dd_H3WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum[onnxruntime] evaluate"
      ],
      "metadata": {
        "id": "FNHsrOLCqqKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Force the downgrade of the HF's Dataset package to release 3.6.0 because of the following [bug](https://github.com/huggingface/datasets/issues/7693) introduced in the latest version 4.0.0 (installed by default in the Colab VMs). A runtime restart would be needed when completed."
      ],
      "metadata": {
        "id": "j7dxuFQUEIOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall datasets==3.6.0"
      ],
      "metadata": {
        "id": "enLAfFClCLqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required classes."
      ],
      "metadata": {
        "id": "fK6DUXOs1V9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "V5sHCPgv1aBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the distilbert base uncased finetuned model from the HF Hub and convert it to ONNX (fp32). Then save it and the associated tokenizer to disk."
      ],
      "metadata": {
        "id": "WEPs5msnMCLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"optimum/distilbert-base-uncased-finetuned-banking77\"\n",
        "onnx_path = Path(\"onnx\")\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(model_id, export=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model.save_pretrained(onnx_path)\n",
        "tokenizer.save_pretrained(onnx_path)"
      ],
      "metadata": {
        "id": "9J4p_iA2XCGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the downloaded model works as expected. Transformers' pipelines are supported in Optimum."
      ],
      "metadata": {
        "id": "YlAYQLphXS8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "vanilla_clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "vanilla_clf(\"Could you assist me in checking my card validity?\")"
      ],
      "metadata": {
        "id": "Y-ke7ET-XOay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a5132b-8494-4264-f636-8134028e9792"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'card_not_working', 'score': 0.8198956251144409}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantize the model dynamically. First create an ORTQuantizer instance and define the quantization configuration. Then apply the quantization configuration to the model."
      ],
      "metadata": {
        "id": "wMELF2cZXsSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTQuantizer\n",
        "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "\n",
        "dynamic_quantizer = ORTQuantizer.from_pretrained(model)\n",
        "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False,\n",
        "                                              per_channel=False)\n",
        "\n",
        "model_quantized_path = dynamic_quantizer.quantize(\n",
        "    save_dir=onnx_path,\n",
        "    quantization_config=dqconfig,\n",
        ")"
      ],
      "metadata": {
        "id": "_Hw1RLL3XvzL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the size of the downloaded ONNX model and its quantized version."
      ],
      "metadata": {
        "id": "TIRVmiVv3HEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "original_model_name = \"model.onnx\"\n",
        "quantized_model_name = \"model_quantized.onnx\"\n",
        "size = os.path.getsize(onnx_path / original_model_name)/(1024*1024)\n",
        "quantized_model = os.path.getsize(onnx_path / quantized_model_name)/(1024*1024)\n",
        "\n",
        "print(f\"Original Model file size: {size:.2f} MB\")\n",
        "print(f\"Quantized Model file size: {quantized_model:.2f} MB\")"
      ],
      "metadata": {
        "id": "GxM_lX8ZX_Ct",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699f2d88-bde1-4e4e-d207-afd7ce0a5c59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model file size: 255.76 MB\n",
            "Quantized Model file size: 64.34 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that inference with the quantized model works as expected."
      ],
      "metadata": {
        "id": "pRkKmDh4YF6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(onnx_path,\n",
        "                                                file_name=quantized_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(onnx_path)\n",
        "\n",
        "q8_clf = pipeline(\"text-classification\",model=model, tokenizer=tokenizer)\n",
        "\n",
        "q8_clf(\"Could you assist me in checking my card validity?\")"
      ],
      "metadata": {
        "id": "SYcRR9_EYAS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f3227a-2f4d-4021-a690-296939ff80ca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Too many ONNX model files were found in onnx/model.onnx ,onnx/model_quantized.onnx. specify which one to load by using the `file_name` and/or the `subfolder` arguments. Loading the file model_quantized.onnx in the subfolder onnx.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'card_not_working', 'score': 0.8280301094055176}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models' performance evaluation"
      ],
      "metadata": {
        "id": "M-J7HOMtYKzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the test set of the Banking 77 dataset (available in the HF's Hub)."
      ],
      "metadata": {
        "id": "_6MUclsYAIYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import evaluator\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_id=\"PolyAI/banking77\"\n",
        "eval = evaluator(\"text-classification\")\n",
        "eval_dataset = load_dataset(dataset_id, split=\"test\")"
      ],
      "metadata": {
        "id": "hGAC5fK7AKKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the quantized model towards the downloaded test set (the HF's Evalate library is used)."
      ],
      "metadata": {
        "id": "BDFiEunhEep0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = eval.compute(\n",
        "    model_or_pipeline=q8_clf,\n",
        "    data=eval_dataset,\n",
        "    metric=\"accuracy\",\n",
        "    input_column=\"text\",\n",
        "    label_column=\"label\",\n",
        "    label_mapping=model.config.label2id,\n",
        "    strategy=\"simple\",\n",
        ")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "QmayEVJsEcBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120726f9-fa78-4fee-9bfa-5dc1d98e833b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.922077922077922, 'total_time_in_seconds': 60.16047874000003, 'samples_per_second': 51.19640110097964, 'latency_in_seconds': 0.019532622967532477}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the test scores across the original ONNX model and its quantized version."
      ],
      "metadata": {
        "id": "DrnN6HtK7-WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vanilla model: 92.5%\")\n",
        "print(f\"Quantized model: {results['accuracy']*100:.2f}%\")\n",
        "print(f\"The quantized model achieves {round(results['accuracy']/0.925,4)*100:.2f}% accuracy of the fp32 model\")"
      ],
      "metadata": {
        "id": "iO54iqZxAUnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94889c6-3a34-4c5d-8e32-20db12176e4b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla model: 92.5%\n",
            "Quantized model: 92.21%\n",
            "The quantized model achieves 99.68% accuracy of the fp32 model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to benchmark the execution times for both models."
      ],
      "metadata": {
        "id": "F7JTMiF1AaQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "import numpy as np\n",
        "\n",
        "def measure_latency(payload_prompt, pipe):\n",
        "    latencies = []\n",
        "    # Warm up\n",
        "    for _ in range(10):\n",
        "        _ = pipe(payload_prompt)\n",
        "    # Effective runs\n",
        "    for _ in range(300):\n",
        "        start_time = perf_counter()\n",
        "        _ =  pipe(payload_prompt)\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "\n",
        "    time_avg_ms = 1000 * np.mean(latencies)\n",
        "    time_std_ms = 1000 * np.std(latencies)\n",
        "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
        "\n",
        "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\\\- {time_std_ms:.2f};\", time_p95_ms"
      ],
      "metadata": {
        "id": "Z7Iq5iP9AeIm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmark the two versions of the model and compare the results."
      ],
      "metadata": {
        "id": "DCAUR0Fz8nYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Dear Sir/Madam, my name is William. I am getting in touch because I didn't get a response from you yet. What actions do I need to do to get my new card which I have requested 3 weeks ago? Please help me and answer this email as soon as possible. Have a nice rest of the day. Best Regards.\"*2\n",
        "print(f'Prompt length: {len(tokenizer(prompt)[\"input_ids\"])}')\n",
        "\n",
        "original_model_stats = measure_latency(prompt, vanilla_clf)\n",
        "quantized_model_stats = measure_latency(prompt, q8_clf)\n",
        "\n",
        "print(f\"Vanilla model: {original_model_stats[0]}\")\n",
        "print(f\"Quantized model: {quantized_model_stats[0]}\")\n",
        "print(f\"Improvement through quantization: {round(original_model_stats[1]/quantized_model_stats[1],2)}x\")"
      ],
      "metadata": {
        "id": "4Cyxwn9k8ij8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d6db0b-49ef-4cb8-bf45-c89c893c9590"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt length: 142\n",
            "Vanilla model: P95 latency (ms) - 309.96727354996665; Average latency (ms) - 202.39 +\\- 41.63;\n",
            "Quantized model: P95 latency (ms) - 228.67615234999906; Average latency (ms) - 152.53 +\\- 35.81;\n",
            "Improvement through quantization: 1.36x\n"
          ]
        }
      ]
    }
  ]
}