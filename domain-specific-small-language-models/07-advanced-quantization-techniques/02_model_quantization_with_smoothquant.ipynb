{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/07-advanced-quantization-techniques/02_model_quantization_with_smoothquant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr7_vyXBa2st"
      },
      "source": [
        "## Using SmoothQuant on OPT large models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNIFbSY0jLoB"
      },
      "source": [
        "\n",
        "The code in this notebook is to show evidence that for LLMs having more 6 or more billion parameters, systematic outliers in a model's activations lead to a degradation in accuracy after quantization, and that the application of the [SmoothQuant](https://github.com/mit-han-lab/smoothquant) technique mitigates that risk. While the code refers to the Meta AI's [OPT 6.7 B](https://huggingface.co/facebook/opt-6.7b) model, the same applies to other models too. It requires hardware acceleration to be executed.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "1ru47FwbC7Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to force the upgrade of the HF's Datasets library to the latest version. Restart the runtime at the end of this upgrade and before moving on with other cells code execution."
      ],
      "metadata": {
        "id": "ts0ajuoKC8XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -U"
      ],
      "metadata": {
        "id": "8izptv7F1We2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsrRohRGjaRo"
      },
      "outputs": [],
      "source": [
        "!pip install --force-reinstall datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbPIRBTUcVxD"
      },
      "source": [
        "Install SmoothQuant from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DlrH551CDuv"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/mit-han-lab/smoothquant.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nThhlX98chpV"
      },
      "source": [
        "Import the required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CD1M8UkYCVsb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers.models.opt.modeling_opt import OPTAttention, OPTDecoderLayer, OPTForCausalLM\n",
        "from transformers import GPT2Tokenizer\n",
        "from smoothquant.smooth import smooth_lm\n",
        "from smoothquant.fake_quant import W8A8Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbSMBcy9cmXE"
      },
      "source": [
        "Define a custom finction to quantize a model (weights and activations) in INT8 precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CigNkKliCbVS"
      },
      "outputs": [],
      "source": [
        "def quantize_model(model, weight_quant='per_tensor', act_quant='per_tensor', quantize_bmm_input=True):\n",
        "    for name, m in model.model.named_modules():\n",
        "        if isinstance(m, OPTDecoderLayer):\n",
        "            m.fc1 = W8A8Linear.from_float(m.fc1, weight_quant=weight_quant,\n",
        "                                          act_quant=act_quant)\n",
        "            m.fc2 = W8A8Linear.from_float(m.fc2, weight_quant=weight_quant,\n",
        "                                          act_quant=act_quant)\n",
        "        elif isinstance(m, OPTAttention):\n",
        "            m.q_proj = W8A8Linear.from_float(\n",
        "                m.q_proj, weight_quant=weight_quant, act_quant=act_quant,\n",
        "                quantize_output=quantize_bmm_input)\n",
        "            m.k_proj = W8A8Linear.from_float(\n",
        "                m.k_proj, weight_quant=weight_quant, act_quant=act_quant,\n",
        "                quantize_output=quantize_bmm_input)\n",
        "            m.v_proj = W8A8Linear.from_float(\n",
        "                m.v_proj, weight_quant=weight_quant, act_quant=act_quant,\n",
        "                quantize_output=quantize_bmm_input)\n",
        "            m.out_proj = W8A8Linear.from_float(m.out_proj,\n",
        "                                               weight_quant=weight_quant, act_quant=act_quant)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjc7sVIIc5V8"
      },
      "source": [
        "Implementa a class to evaluate an LLM given a test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n_B5F1w-CiUj"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, dataset, tokenizer, device):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            example = self.tokenizer(examples['text'])\n",
        "            return example\n",
        "\n",
        "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
        "        self.dataset.set_format(type='torch', columns=['input_ids'])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, model):\n",
        "        model.eval()\n",
        "        total, hit = 0, 0\n",
        "        for batch in self.dataset:\n",
        "            input_ids = batch['input_ids'].to(self.device).unsqueeze(0)\n",
        "            label = input_ids[:, -1]\n",
        "            outputs = model(input_ids)\n",
        "            last_token_logits = outputs.logits[:, -2, :]\n",
        "            pred = last_token_logits.argmax(dim=-1)\n",
        "            total += label.size(0)\n",
        "            hit += (pred == label).sum().item()\n",
        "        acc = hit / total\n",
        "        return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "uRj-nyy4DPNg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl47DI6xdIUo"
      },
      "source": [
        "Download a subset (1000 samples in this case) of the LAMBADA dataset and the Meta AI OPT 6.7B model's tokenizer from the Hugging Face's Hub and then create an instance of the Evaluator class using them. Everything goes to GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB2cUFLoCpiz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "model_id = 'facebook/opt-6.7b'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Load a dataset for the evaluation of the different versions of the target model\n",
        "dataset = load_dataset('cimec/lambada', split='validation[:1000]')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = Evaluator(dataset, tokenizer, 'cuda')\n",
        "print(\"Dataset loaded and Evaluator initialized successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "1d926998a3d14a908620b69ff4593e31",
            "d3bb7a0f2ea54a8da19eb8a4f3cadf33",
            "544e384661eb4839b9fa268bb061d8ca",
            "f1fd569ec0d54ec2ae9d6cc1657eff3c",
            "2cb4147cf3494e03a5c9519550ab79ba",
            "39722ec54b7d449ba94569cf6eadb0d3",
            "a19619822c61446b8b60d9b7d5a98a68",
            "3245cd94e730475eb11ad2771b2dcc68",
            "8ffffda0cca54d2e82788a4622c49ef6",
            "94d0d7d046a2456ba21e0aa1bf917ea1",
            "b95bdcbc2ddb4b729f260d32cff65a94"
          ]
        },
        "id": "9n0dC1p00cyV",
        "outputId": "30474041-ae99-46d2-bd42-1ee1d1d34c9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d926998a3d14a908620b69ff4593e31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and Evaluator initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_J8cAtLEGk6"
      },
      "source": [
        "## FP16 Model Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEjDztgMebk5"
      },
      "source": [
        "Download the Meta AI OPT 6.7B model in FP16 from the HF's Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tLKSBJGZEDdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3dfc9a-cf4b-49ef-e185-fc003c6a23e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 4096, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 4096)\n",
              "      (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0-31): 32 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "            (out_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc2): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model_fp16 = OPTForCausalLM.from_pretrained(model_id,\n",
        "                                            torch_dtype=torch.float16,\n",
        "                                            device_map='auto',\n",
        "                                            offload_folder='.')\n",
        "model_fp16.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV0YtmCNejz_"
      },
      "source": [
        "Evaluate the model on the 1000 samples from the LAMBADA dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcucIzD8EKiN"
      },
      "outputs": [],
      "source": [
        "acc_fp16 = evaluator.evaluate(model_fp16)\n",
        "print(f'Original model (fp16) accuracy: {acc_fp16}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruIY-iq2EPpf"
      },
      "source": [
        "## Naive W8A8 Quantized Model Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSUBP9a2eseJ"
      },
      "source": [
        "Quantize weights and activation of the vanilla model (no SmoothQuant)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw9nJmg-EM_1"
      },
      "outputs": [],
      "source": [
        "# Let’s quantize this model now\n",
        "model_w8a8 = quantize_model(model_fp16)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_w8a8)"
      ],
      "metadata": {
        "id": "z9mHjIqg4kqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m1wMMg-e0Tl"
      },
      "source": [
        "Evaluate the quantized model on the 1000 samples from the LAMBADA dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEuo8ij1EUBX"
      },
      "outputs": [],
      "source": [
        "acc_w8a8 = evaluator.evaluate(model_w8a8)\n",
        "print(f'Naive W8A8 quantized model accuracy: {acc_w8a8}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwxMG8nTEaq0"
      },
      "source": [
        "## SmoothQuant W8A8 Quantized Model Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWaNpVrDfQ3v"
      },
      "source": [
        "**To save time and free GPU memory to evaluate the model after applying SmoothQuant, a runtime restart is recommended at this time, before proceeding further.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To6V3XW-e6FT"
      },
      "source": [
        "Download the specific model's scales from the HF's Hub (mandatory to apply SmoothQuant)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv2VEOhLLRux"
      },
      "outputs": [],
      "source": [
        "!mkdir ./act_scales\n",
        "%cd act_scales\n",
        "!wget https://huggingface.co/mit-han-lab/smoothquant-scales/resolve/main/opt-6.7b.pt\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAqSspGQfy5l"
      },
      "source": [
        "Apply SmoothQuant and after quantize the vanilla model's weights and activations in INT8 format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6jzxr95gsUQ"
      },
      "outputs": [],
      "source": [
        "model_fp16 = OPTForCausalLM.from_pretrained(model_id,\n",
        "                                            torch_dtype=torch.float16,\n",
        "                                            device_map='auto',\n",
        "                                            offload_folder='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H41rVIHjEbk3"
      },
      "outputs": [],
      "source": [
        "act_scales = torch.load('./act_scales/opt-6.7b.pt')\n",
        "smooth_lm(model_fp16, act_scales, 0.5)\n",
        "model_smoothquant_w8a8 = quantize_model(model_fp16)\n",
        "print(model_smoothquant_w8a8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kr5p55sgCHy"
      },
      "source": [
        "Evaluate the smooth quantized model on the 1000 samples from the LAMBADA dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MjNqiPdiG_J"
      },
      "outputs": [],
      "source": [
        "model_smoothquant_w8a8.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_qv-WtGEdp-"
      },
      "outputs": [],
      "source": [
        "acc_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)\n",
        "print(f'SmoothQuant W8A8 quantized model accuracy: {acc_smoothquant_w8a8}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPUT6bVQgInt"
      },
      "source": [
        "The accuracy of the vanilla model and its smooth quantized version should be comparable, while there should be a significant drop (up to 40%) for the naive quantized model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d926998a3d14a908620b69ff4593e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3bb7a0f2ea54a8da19eb8a4f3cadf33",
              "IPY_MODEL_544e384661eb4839b9fa268bb061d8ca",
              "IPY_MODEL_f1fd569ec0d54ec2ae9d6cc1657eff3c"
            ],
            "layout": "IPY_MODEL_2cb4147cf3494e03a5c9519550ab79ba"
          }
        },
        "d3bb7a0f2ea54a8da19eb8a4f3cadf33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39722ec54b7d449ba94569cf6eadb0d3",
            "placeholder": "​",
            "style": "IPY_MODEL_a19619822c61446b8b60d9b7d5a98a68",
            "value": "Map: 100%"
          }
        },
        "544e384661eb4839b9fa268bb061d8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3245cd94e730475eb11ad2771b2dcc68",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ffffda0cca54d2e82788a4622c49ef6",
            "value": 1000
          }
        },
        "f1fd569ec0d54ec2ae9d6cc1657eff3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94d0d7d046a2456ba21e0aa1bf917ea1",
            "placeholder": "​",
            "style": "IPY_MODEL_b95bdcbc2ddb4b729f260d32cff65a94",
            "value": " 1000/1000 [00:00&lt;00:00, 1319.49 examples/s]"
          }
        },
        "2cb4147cf3494e03a5c9519550ab79ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39722ec54b7d449ba94569cf6eadb0d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a19619822c61446b8b60d9b7d5a98a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3245cd94e730475eb11ad2771b2dcc68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ffffda0cca54d2e82788a4622c49ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94d0d7d046a2456ba21e0aa1bf917ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b95bdcbc2ddb4b729f260d32cff65a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}