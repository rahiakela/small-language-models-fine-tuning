{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/06-generate-protein-structures/01_generating_protein_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Protein Sequences with ProtGPT2 Locally"
      ],
      "metadata": {
        "id": "U7iuQrsVxLk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code in this notebook is to generate protein sequences using the [ProtGPT2](https://huggingface.co/nferruz/ProtGPT2) model. It doesn't require hardware acceleration.  \n",
        "\n",
        "Letâ€™s see how this model works and how we can use it to generate de novo\n",
        "proteins in a zero-shot fashion."
      ],
      "metadata": {
        "id": "Deddq7EKyCnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "O8obrpKVrvyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the ProtGPT2 model from the HF Hub and set up an inference pipeline for it."
      ],
      "metadata": {
        "id": "oTtDLVJ6rxUA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0HtDGt3-2f9"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = \"nferruz/ProtGPT2\"\n",
        "protgpt2 = pipeline('text-generation', model=model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating protein sequences"
      ],
      "metadata": {
        "id": "zd2h4tYnr07u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the pipeline to start generating protein sequences (10 in this example). At the end of the generation process the protein sequences are displayed on the standard output."
      ],
      "metadata": {
        "id": "aFfd2MrsyXit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = protgpt2(\n",
        "    \"<|endoftext|>\",\n",
        "    max_length=100,\n",
        "    do_sample=True,\n",
        "    top_k=950,\n",
        "    repetition_penalty=1.2,\n",
        "    num_return_sequences=10, # generate 10 protein sequences\n",
        "    eos_token_id=0\n",
        ")\n",
        "for seq in sequences:\n",
        "  print(seq)"
      ],
      "metadata": {
        "id": "ArBLDM7c_KcW",
        "outputId": "b16f18ca-042f-41c5-8ede-5885b0d87185",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'generated_text': '<|endoftext|>\\nMSSIIDKMSDHPRALSLDPLDAVIVSDAFGEDIVYASKAFRDIYNIDPADAVGKTHADIE\\nGDGPTTPAVRELLQQALNQKAAFEADYKHETRSGLKWYRGIVKPIYDKDGNVRYFMAVEH\\nDITERKRAEAEREEVKAQLQRTQKMEALGTLAGGIAHDFNNLLGAIIGYTDLALDECDPT\\nDPISQNLKQVYKACLRARDLVQQILTFSRQSEQERKPVQIAPIVKEVLKLLRSSVPSTIE\\nIRQDISPDCGYVLADPTQIHQIMMNLCTNAYHAMEEGGGVLSMSLNDIELGEGTDPMFGD\\nLKPGTYVRLEVSDTGSGMDQTTRERIFEPYYTTKGPGEGTGLGLSVVHGIVTSYGGTIRV\\nRSEPGKGTEFHVYLPRVDEAIEPPAKETSELPRGTEHILFVDDEPALVELGRQMLERLGY\\nQVTARTSSVDALEAFRANPDRFDVVITDMTMPKMTGDQLSKEILRVRPDLPIILCTGFSA\\nLISEEKAKKIGIRGFVMKPLVMRELAETIRRVLDHSKN\\n'}\n",
            "{'generated_text': '<|endoftext|>\\nMAKNENELIDTYVLMGLHDRTIKKKLRKRKAELGKPDRTESMREVLSDPVLRNRYDRIMR\\nN\\n'}\n",
            "{'generated_text': '<|endoftext|>\\nMADIFSKLKNGDKLKAALSSTWPIVKKYHPLIILGLVAIAAYMFYQNMKDPVIQRKAEYE\\nTAKQQKAKEEAEKARLKELERQRLEQQAAQFQKDMNDAMSTEKAAKETSSKDYYNSLSGE\\nDKKKMQDELEALRKQMGGLGGYGTQTASKPVSSSGGGRMYDTSGNVVKPDSSYSGRSTTR\\nRSSSSGQTSSDYLYY\\n'}\n",
            "{'generated_text': '<|endoftext|>\\nMKIKSFYLLAALSLLSTQAASASTTVPNVDLSVFPGGSTSLSISGLATSQITKVSFGAAT\\nTTQVVSSSNGTVSATFGASSGCSTGKCTVTLSSNGGTNIAITVSAPTPPAITGATATPAS\\nGTVTLTWTPSATGGSAIAAYKIYRNGSQIATLTATNYTDTGLTNGQSYTYTVRAVNSAGA\\nSGQSNTVSATPVAAASPVVTVPAVNLSVFPGGGTNLSITGLASSQITKASFGSATTTQIV\\nTSSSGTVTATLGVSSGCTTGTCTVTLSANGGSNIAITVNAPPVPPPPPPVTPGVPTFTIT\\nSVSPNSAQAGATVTVTVTGFNFQPGAVVNFNGVAAATTFVSPTTLTAAIAASDLATSGLA\\nNITVKNQAGTASVTSSFSVTPVPTPTPTPVTPVGAVSASLASSSIAVGQTSVLTANFTPT\\nNATNKNVTWSSSSTAVATVSNTGLATAVAPGSATITATSEGKSGSATVIVTSPPGGGGGG\\nGGSAAFVPPTCTAGSCSSGGACTDTGTGTYNCALGCAAGFCTGGACSSGTTGGGGGGSGN\\nITITLSGQNPTAGQTVSLQLPVSVTNNGSSSASGVSLRVVATAPSGWGQTLTSVSLGTLG\\nPNQTTTVEIPVTVPSNAPAGTYALVFEVRTADGSTTGDSTISVQVSSGGGSGGSSGGGGG\\nGGGGGGGSSTVQVTIQSADPTAPAGSVITTTLIVANLGSSPATG'}\n",
            "{'generated_text': '<|endoftext|>\\nMPVRGYPRWLALAAAALLVAVLAGCGVPGDTTSGTDPPGASAPRSTWRSWSWPTQPAPRP\\nTAPPKPTQRITRSGPALDTRVDGRTVGLAVTAQSLTVDGKAPTAAVTWTATTPGRAVSGD\\nGSLTAGLAPGDYTLTVTATGTDGTSATASTTFTVPATEPAGPVAVPAAGTCVQRGGAPGT\\nDGTRLVCAPAADAARWRLDDTGTLVDLAGGLRLRGPQGTPLALGGRLLPSGGLGVGGAVT\\nLADGRVAVTDTGGVRTLDPASGVVTTWLAPPSTVPLGTGVCRVVATDVPARGKTVACSDG\\nSAAWSYDGAGTLVTTADGLQVRTATGWPLVLGGRTG\\n'}\n",
            "{'generated_text': '<|endoftext|>\\nMETCRRCGRDGHRQRDCPKLRQLKCFRCQRFGHVARNCPSRVEQGRDSNAGTSRAAVSRA\\nSAGEARASSSSGVGARSATAAAESAEAASATNGGEGSRARGVKRVRFPEVSTASTRVATE\\nGGNAPGGVSLAVEQGRNSDAGSSRAVVSRASGDEASASGGGSGSRGRGVKRVRFPEVPSS\\nSASVVTEGGVAPGGVGGTVEQGQDSTAGSTREGVSRGSGDVASSSEGGSSCRGVRVKRVR\\nFPGVSTSSTSVVTEGGAAPGGSGGTVEQGQGSTAGSTREGAGRGSGDVALSSESNSSRRG\\nVRVKEVRFPGLPSTSTSVVTEGVDEAGGASLAAERGQNSNARTWRGVSRGSGEAASSSEG\\nGSSWRGIRVKGVRVPGLPGVSTSAVVVTEDADEAGGGSLAAEKGQSSNLRIWKGVARGSV\\nEASSSEGGGSSRGIRLKEVRLPGVPSSSTHAIVEGKRASGGMSSVAERGKNSHAGTRRGL\\nSRGSEDVASSSGDGSSWCGTRVKEVELPGLPSTSTSAVAEEGRAVGGSVPVAGKGKNSYG\\nGTRRGLNRGNEDAASSSSDGSSRCGTGVREVELPGVSSASTYTIAEGRSAVSGVASIVAE\\nHRKNSYARTRQAVRRGSEDAASSSGGGSFWRGVRVEEVEFPGLPSSSTASIVEDDKAACS\\nGVSSVAEEERNSHGGTRRGLSRGSENAASSSLNGSSVRG'}\n",
            "{'generated_text': '<|endoftext|>\\nMMILKVLIPKKLFIENKTEDKTKIKTEKKPKKKRKLKLPFLKKQKFTQKKEKNEEKETNI\\nEKENINSPEEPIEVKQEQPQNIKEEIPIKKEQLSPEIANQLKKVEEIKQKQEEEIKRLKS\\nEKVLLEQRIKEKENKLKSLEDKKSEKITNQQSEIPKEPIQQPIEVPKELPKLEIPKKVET\\nPIQPEEPKQVETQPKKEEIKPIEIPEKVSKDQHQKKRRIILPPAIQKRIDEEKKKEIIKQ\\nIMLLLKRIKL\\n'}\n",
            "{'generated_text': '<|endoftext|>\\nMSKSGRPKVSSDTIVAKAMELFWQHGYEGTSMRDLVDAMGINRASIYATYGSKAGLFNEA\\nVRHYIESAQPKVLDALEREPTAREAIGALLSKSAALYSSASTPRGCMIVNTGLTAGPDDP\\nDVRKMVTQRRSEIESRFSAQLDRGVAEGELPSDVDTQALARFIVATLQGLAVQARAGASR\\nSDLEALLSVALDAWPRT\\n'}\n",
            "{'generated_text': '<|endoftext|>\\nMMMLLFMTTTVLARVLLPLLLLCLLSSCCCCYADSVAMVAAATAVSRSRFSVPSVTHCVN\\nRC\\n'}\n",
            "{'generated_text': '<|endoftext|>\\nMLTHRKAKKATAEESPKYQPSRPTKPRGRDSQRSKDNDENKIDEVFKKLAVSKGDKIAEG\\nKDELSVIKFIKGTTKTLKFEGILTEETEMKKFEDLLQEMAKDSGDKVKKANLNEETVVNI\\nSGGGFQAKNAKIKVKGIDDEEAIKEEFAKTCRKLNLGDLEKLEVTKIAKNSCSIWLKGKS\\nEEEKNKLLTATKKAGVEVDGKRYTVLNDEPKGVNFKIKVDPNVDDQKIIDALNKIFQENG\\nGEAEIVKIKKRKNDDGSIVTESLRVIVPYQSLPKSIKFGYIFAGCRCEKQPFQPLRCRRC\\nLKFGHEAKNCKDEIKCAKCGEEGHSASECTNEGKECPNCKKAHRTDVKTCPSYKKQFNNK\\nEA\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate perplexity"
      ],
      "metadata": {
        "id": "A0KTOdHTrFn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to calculate the perplexity metric for the generated results."
      ],
      "metadata": {
        "id": "uxm4McgDzOHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def calculate_perplexity(model, tokenizer, text, device):\n",
        "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "    input_ids = encodings.input_ids\n",
        "    target_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "\n",
        "    neg_log_likelihood = outputs.loss\n",
        "\n",
        "    perplexity = torch.exp(neg_log_likelihood)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "nvNTt0iIJQ2E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the generated results by calculating the perplexity metric for them."
      ],
      "metadata": {
        "id": "wSRkSa5rAg3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "for seq in sequences:\n",
        "  print(calculate_perplexity(protgpt2.model, protgpt2.tokenizer, seq['generated_text'], device))"
      ],
      "metadata": {
        "id": "tCDvGIGHJSC4",
        "outputId": "ea8892b1-2687-4515-eac4-f5766ad5ecdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(26.2018)\n",
            "tensor(324.6881)\n",
            "tensor(314.5198)\n",
            "tensor(77.1016)\n",
            "tensor(213.4827)\n",
            "tensor(55.0086)\n",
            "tensor(298.3368)\n",
            "tensor(36.9863)\n",
            "tensor(229.7952)\n",
            "tensor(265.0749)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively we can calculate perplexity on a batch of generated protein sequences. Let's define a custom function for this."
      ],
      "metadata": {
        "id": "WRrVc4CZ3goN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "protgpt2.tokenizer.pad_token = protgpt2.tokenizer.eos_token\n",
        "\n",
        "def calculate_batch_perplexity(input_texts, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a batch of input texts using a pretrained language model.\n",
        "\n",
        "    Args:\n",
        "    - input_texts (List[str]): A list of input texts to evaluate.\n",
        "\n",
        "    Returns:\n",
        "    - List[float]: A list of perplexity scores, one for each input text.\n",
        "    \"\"\"\n",
        "    # Tokenize the batch of texts with padding for uniform length\n",
        "    inputs = tokenizer(\n",
        "        input_texts, return_tensors=\"pt\", padding=True, truncation=True\n",
        "    )\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "    # Pass the input batch through the model to get logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Shift the logits and input_ids to align targets correctly\n",
        "    # Logits dimensions are: (batch_size, seq_length, vocab_size)\n",
        "    shift_logits = logits[:, :-1, :]  # Ignore the last token's logits\n",
        "    shift_labels = input_ids[:, 1:]   # Skip the first token in the labels\n",
        "\n",
        "    # Compute log probabilities\n",
        "    log_probs = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
        "\n",
        "    # Gather the log probabilities for the correct tokens\n",
        "    target_log_probs = log_probs.gather(dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # Mask out positions corresponding to padding tokens\n",
        "    target_log_probs = target_log_probs * attention_mask[:, 1:].to(log_probs.dtype)\n",
        "\n",
        "    # Compute the mean negative log-likelihood for each sequence\n",
        "    negative_log_likelihood = -target_log_probs.sum(dim=-1) / attention_mask[:, 1:].sum(dim=-1)\n",
        "\n",
        "    # Compute perplexity for each sequence\n",
        "    perplexities = torch.exp(negative_log_likelihood)\n",
        "\n",
        "    # Take mean of perplexities of each batch\n",
        "    mean_perplexity_score = torch.mean(perplexities)\n",
        "\n",
        "    return {\"perplexities\": perplexities, \"mean_perplexity\": mean_perplexity_score}"
      ],
      "metadata": {
        "id": "33tjAT_5-3a5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the ```calculate_batch_perplexity``` function on the generated protein sequences.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dRGmtGy3spU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_texts = [seq['generated_text'] for seq in sequences]\n",
        "print(f\"Perplexity scores: {calculate_batch_perplexity(sequence_texts, protgpt2.model, protgpt2.tokenizer)}\")"
      ],
      "metadata": {
        "id": "cE9FtGCk_GIl",
        "outputId": "cd00896c-d827-49d3-a645-3867bbb1f574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity scores: {'perplexities': tensor([ 26.2018, 324.6875, 314.5200,  77.1016, 213.4828,  55.0086, 298.3365,\n",
            "         36.9863, 229.7955, 265.0750]), 'mean_perplexity': tensor(184.1196)}\n"
          ]
        }
      ]
    }
  ]
}