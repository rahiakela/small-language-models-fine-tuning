{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/01-tuning-for-specific-domain/02_fine_tuning_bert_for_question_answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning BERT for Question Answering"
      ],
      "metadata": {
        "id": "wRNWVQoS8A0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code in this notebook is to show how to fine tune a [DistilBert](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model for extractive question answering (extract the answer from a given context). While all the steps refer to DistilBert, the same apply to a larger pool of LLM architectures.   \n",
        "While no hardware acceleration is required to execute all the code cells, it is recommended to use a GPU to speed up the fine tuning step."
      ],
      "metadata": {
        "id": "JbZ9ZX0Q8HWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ],
      "metadata": {
        "id": "4uHFwfadgsCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing requirements in the Colab VM (HF's Datasets and Accelerate)."
      ],
      "metadata": {
        "id": "fy-K1QUR7eRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_42fPZIJABs4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omjzXzqvABs8"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmzD9CdfABs9"
      },
      "source": [
        "Load a subset (5000 samples) of the *SQuAD* dataset using the HF's *Datasets* library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq-f1GLGABs9"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "squad = load_dataset(\"squad\", split=\"train[:5000]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msmbw21JABs9"
      },
      "source": [
        "Split the dataset's `train` split into a train and test set (80%/20%) with the *train_test_split* method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "euD6tpNVABs9"
      },
      "outputs": [],
      "source": [
        "squad = squad.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxnglcBDABs9"
      },
      "source": [
        "Display a sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2LpaNG_JABs9",
        "outputId": "fdb9322f-47ec-4304-9691-6cd2bbde0f1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56ce304daab44d1400b88510',\n",
              " 'title': 'New_York_City',\n",
              " 'context': 'New York—often called New York City or the City of New York to distinguish it from the State of New York, of which it is a part—is the most populous city in the United States and the center of the New York metropolitan area, the premier gateway for legal immigration to the United States and one of the most populous urban agglomerations in the world. A global power city, New York exerts a significant impact upon commerce, finance, media, art, fashion, research, technology, education, and entertainment, its fast pace defining the term New York minute. Home to the headquarters of the United Nations, New York is an important center for international diplomacy and has been described as the cultural and financial capital of the world.',\n",
              " 'question': 'What city has been called the cultural capital of the world?',\n",
              " 'answers': {'text': ['New York'], 'answer_start': [0]}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "squad[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaASkXPmABs9"
      },
      "source": [
        "The three important fields are:\n",
        "\n",
        "- `answers`: the starting location of the answer token and the answer text.\n",
        "- `context`: background information from which the model needs to extract the answer.\n",
        "- `question`: the question a model should answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZQqzyePABs-"
      },
      "source": [
        "Load a DistilBERT tokenizer to process the `question` and `context` fields in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnEvX-GfABs-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok-3EhNZABs-"
      },
      "source": [
        "Some data preprocessing is needed:\n",
        "\n",
        "1. Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. To deal with longer sequences, truncate only the `context` by setting `truncation=\"only_second\"`.\n",
        "2. Map the start and end positions of the answer to the original `context` by setting\n",
        "   `return_offset_mapping=True`.\n",
        "3. Use the *sequence_ids* method to\n",
        "   find which part of the offset corresponds to the `question` and which corresponds to the `context`.\n",
        "\n",
        "Let's define a function that implementes the aforementioned preprocessing steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Sg7MlHunABs-"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ugzikm2ABs-"
      },
      "source": [
        "To apply the preprocessing function over the entire dataset, we can use the HF's Datasets *map* function. By setting `batched=True`, multiple elements of the dataset will be processed at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF_2ThUbABs_"
      },
      "outputs": [],
      "source": [
        "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7d2uzZsABs_"
      },
      "source": [
        "Now create a batch of examples using DefaultDataCollator. Unlike other data collators in the HF's Transformers library, it doesn't apply any additional preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D2uy4eiWABs_"
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs43LivaABs_"
      },
      "source": [
        "## Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x41WGrCqABs_"
      },
      "source": [
        "Load DistilBERT with *AutoModelForQuestionAnswering*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46tOjLvUABs_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\", device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training hyperparameters in a *TrainingArguments* instance. The only required parameter is *output_dir* which specifies where to save our model."
      ],
      "metadata": {
        "id": "bsbTmyjClZLr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "a8wah7KRABs_"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_qa_model\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass the training arguments to a *Trainer* instance along with the model, the dataset, the tokenizer, and the data collator."
      ],
      "metadata": {
        "id": "W9EpydVFli9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_squad[\"train\"],\n",
        "    eval_dataset=tokenized_squad[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "pvApMj8PlCIY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the fine tuning:"
      ],
      "metadata": {
        "id": "a7qxkWExluF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "8l3QZPoZk_Jg",
        "outputId": "77ef4fa2-d19f-4567-d9bd-abb4ab756889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 08:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.302944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.731400</td>\n",
              "      <td>1.705511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.731400</td>\n",
              "      <td>1.627654</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=750, training_loss=2.2862471110026044, metrics={'train_runtime': 489.2193, 'train_samples_per_second': 24.529, 'train_steps_per_second': 1.533, 'total_flos': 1175877900288000.0, 'train_loss': 2.2862471110026044, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-uGlCy1ABtA"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDhvl3msABtB"
      },
      "source": [
        "The finetuned model can now be used for inference. Let's provide a question and some context we'd like the model to predict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MxWta00WABtB"
      },
      "outputs": [],
      "source": [
        "question = \"How many official league titles has Juventus won?\"\n",
        "context = \"\"\"Juventus Football Club (from Latin: iuventūs), colloquially known as Juve,\n",
        "is a professional football club based in Turin, Piedmont, Italy, that competes in the Serie A,\n",
        "the top tier of the Italian football league system. Founded in 1897 by a group of Torinese students,\n",
        "the club has worn a black and white striped home kit since 1903 and has played home matches\n",
        "in different grounds around its city, the latest being the 41,507-capacity Juventus Stadium.\n",
        "Nicknamed la Vecchia Signora (the Old Lady), the club has won 36 official league titles, 14 Coppa Italia\n",
        "titles and nine Supercoppa Italiana titles, being the record holder for all these competitions;\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd-Ey7MIABtC"
      },
      "source": [
        "First, tokenize the text and return PyTorch tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8eEoimtlABtC",
        "outputId": "3601d6f7-78bb-4c38-e83c-0e5a3e9ae89e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  2129,  2116,  2880,  2223,  4486,  2038, 22760,  2180,  1029,\n",
              "           102, 22760,  2374,  2252,  1006,  2013,  3763,  1024,  1045, 27346,\n",
              "          5809,  1007,  1010, 23992,  2124,  2004, 18414,  3726,  1010,  2003,\n",
              "          1037,  2658,  2374,  2252,  2241,  1999, 13667,  1010, 18873,  1010,\n",
              "          3304,  1010,  2008, 14190,  1999,  1996,  8668,  1037,  1010,  1996,\n",
              "          2327,  7563,  1997,  1996,  3059,  2374,  2223,  2291,  1012,  2631,\n",
              "          1999,  6347,  2011,  1037,  2177,  1997, 23413, 14183,  2493,  1010,\n",
              "          1996,  2252,  2038,  6247,  1037,  2304,  1998,  2317, 17983,  2188,\n",
              "          8934,  2144,  5778,  1998,  2038,  2209,  2188,  3503,  1999,  2367,\n",
              "          5286,  2105,  2049,  2103,  1010,  1996,  6745,  2108,  1996,  4601,\n",
              "          1010,  2753,  2581,  1011,  3977, 22760,  3346,  1012,  9919,  2474,\n",
              "          2310, 25955,  2050,  3696,  6525,  1006,  1996,  2214,  3203,  1007,\n",
              "          1010,  1996,  2252,  2038,  2180,  4029,  2880,  2223,  4486,  1010,\n",
              "          2403,  8872,  4502, 13052,  4486,  1998,  3157,  3565,  3597, 13944,\n",
              "         28059,  4486,  1010,  2108,  1996,  2501,  9111,  2005,  2035,  2122,\n",
              "          6479,  1025,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "inputs.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8AO8_jVABtC"
      },
      "source": [
        "Then let's pass our inputs to the model and return the `logits`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5XrNXoGOABtC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh_bDsFbABtD"
      },
      "source": [
        "Get the highest probability from the model output for the start and end positions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4VFXAr5jABtD"
      },
      "outputs": [],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl0iK7vEABtD"
      },
      "source": [
        "Decode the predicted tokens to get the answer in natural language:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6s_zLLuXABtD",
        "outputId": "4f8920c9-71fc-478c-9c9c-9ee9afcfac5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'36'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}