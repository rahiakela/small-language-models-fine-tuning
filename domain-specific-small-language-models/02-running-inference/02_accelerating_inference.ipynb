{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/02-running-inference/02_accelerating_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerating inference for GPT-Neo with DeepSpeed"
      ],
      "metadata": {
        "id": "x6wXI87UM6Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this notebook is to introduce readers to the [DeepSpeed](https://github.com/microsoft/DeepSpeed) library to accelerate inference for the [GPT-Neo model](https://github.com/EleutherAI/gpt-neo) for text generation tasks. It can be executed in the Colab free tier with hardware acceleration (GPU).  "
      ],
      "metadata": {
        "id": "DXnYUlS2LTLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing dependencies in the Colab VM (DeepSpeed and HF's Accelerate only)."
      ],
      "metadata": {
        "id": "YDRRbZPpOLM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txg6MF3QdT8D"
      },
      "outputs": [],
      "source": [
        "!pip install -q deepspeed accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before loading the model, let's define a custom function to be used for benchmarking (latency measurement)."
      ],
      "metadata": {
        "id": "PwgbZqd6ijqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter\n",
        "import numpy as np\n",
        "\n",
        "def measure_latency(model, tokenizer, payload, device, generation_args={}):\n",
        "    input_ids = tokenizer(payload, return_tensors=\"pt\").input_ids.to(device)\n",
        "    latencies = []\n",
        "    # Do GPU warm up before benchmarking\n",
        "    for _ in range(2):\n",
        "        _ =  model.generate(input_ids, **generation_args)\n",
        "    # Runs used for measuring the latency\n",
        "    for _ in range(20):\n",
        "        start_time = perf_counter()\n",
        "        _ = model.generate(input_ids, **generation_args)\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "\n",
        "    time_avg_ms = 1000 * np.mean(latencies)\n",
        "    time_std_ms = 1000 * np.std(latencies)\n",
        "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
        "\n",
        "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_p95_ms\n"
      ],
      "metadata": {
        "id": "qWNFW20Dg8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the base GPT-Neo 2.7B model in half precision and the related tokenizer from the HF's Hub."
      ],
      "metadata": {
        "id": "AnTa3jQEdtYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"EleutherAI/gpt-neo-2.7B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                          torch_dtype=torch.float16,\n",
        "                                          device_map=\"auto\")"
      ],
      "metadata": {
        "id": "CpMiuRhtdjUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"model is loaded on device {model.device.type}\")"
      ],
      "metadata": {
        "id": "NSQ_KJLB0zxl",
        "outputId": "0ddca528-9b45-4b4e-d1df-9e1fe0dc4215",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model is loaded on device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do inference with the downloaded model to verify that everything is working as expected."
      ],
      "metadata": {
        "id": "MeZNahEFPGNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"The story so far: in the beginning, the universe was created.\"\n",
        "\n",
        "input_ids = tokenizer(example, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "logits = model.generate(input_ids,\n",
        "                        do_sample=True,\n",
        "                        num_beams=1,\n",
        "                        min_length=128,\n",
        "                        max_new_tokens=128,\n",
        "                        pad_token_id=50256)\n",
        "\n",
        "print(f\"prediction: \\n \\n {tokenizer.decode(logits[0].tolist()[len(input_ids[0]):])}\")"
      ],
      "metadata": {
        "id": "K47Okvzld-yh",
        "outputId": "f0ec0df2-ce3b-424e-c22a-72bb64e5b7c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: \n",
            " \n",
            "  The Big Bang Theory states that at a very early stage in a protons, and electrons were created out of the vaccuum of space. The universe expands exponentially, creating enormous amounts of matter and energy, and eventually, stars begin to form. Eventually, when the universe is about 13.7 billion years old, planets form. Then, life evolves on these planets, developing multicellular life and complex life forms, such as humans.\n",
            "\n",
            "In the second law of thermodynamics, everything eventually cools down. Eventually, the universe (with the vast majority of species) goes into a rapid cooling down to about a millionth of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform benchmark for the vanilla model. The previously defined ```measure_latency``` function is used.\n",
        "\n"
      ],
      "metadata": {
        "id": "h0jVn7MSPi_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = dict(do_sample=True,\n",
        "                      max_length=300,\n",
        "                      pad_token_id=50256,\n",
        "                      use_cache=True\n",
        ")\n",
        "vanilla_results = measure_latency(model, tokenizer, example,\n",
        "                                  model.device, generation_args)\n",
        "\n",
        "print(f\"Vanilla model: {vanilla_results[0]}\")"
      ],
      "metadata": {
        "id": "BvEk6ZEChcXu",
        "outputId": "dcac8f78-885c-4ca9-ec25-89e731bdc764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla model: P95 latency (ms) - 12297.61378600016; Average latency (ms) - 10779.98 +\\- 721.81;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimize model with DeepSpeed"
      ],
      "metadata": {
        "id": "Q8rvpsjy4dw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now optimize the base GPT-Neo 2.7B model for inference on GPU with DeepSpeed. The decision about which of the original model's layers have to be replaced is left to DeepSpeed itself here."
      ],
      "metadata": {
        "id": "8UfJvEb1eTXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '9999'\n",
        "os.environ['RANK'] = \"0\"\n",
        "os.environ['LOCAL_RANK'] = \"0\"\n",
        "os.environ['WORLD_SIZE'] = \"1\""
      ],
      "metadata": {
        "id": "tpb4h2IsUNxw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepspeed\n",
        "\n",
        "ds_model = deepspeed.init_inference(\n",
        "    model=model,\n",
        "    mp_size=1,\n",
        "    dtype=torch.float16,\n",
        "    replace_method=\"auto\",\n",
        "    replace_with_kernel_inject=True,\n",
        ")\n",
        "print(f\"model is loaded on device {ds_model.module.device}\")"
      ],
      "metadata": {
        "id": "hfXLPFHtecMz",
        "outputId": "0a50794f-e775-4dd9-ebcd-2ab9e629717b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model is loaded on device cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the optimized model's architecture to this cell output to verify that some of the original model's layers have been replaced with DeepSpeed optimized kernel implementations."
      ],
      "metadata": {
        "id": "cTFt748TRmMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_model"
      ],
      "metadata": {
        "id": "XhYIvUXOg2E5",
        "outputId": "3e0b2ce6-267d-40fb-f62c-eed04e3a3a10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InferenceEngine(\n",
              "  (module): GPTNeoForCausalLM(\n",
              "    (transformer): GPTNeoModel(\n",
              "      (wte): Embedding(50257, 2560)\n",
              "      (wpe): Embedding(2048, 2560)\n",
              "      (drop): Dropout(p=0.0, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-31): 32 x DeepSpeedGPTInference(\n",
              "          (attention): DeepSpeedSelfAttention(\n",
              "            (qkv_func): QKVGemmOp()\n",
              "            (score_context_func): SoftmaxContextOp()\n",
              "            (linear_func): LinearOp()\n",
              "            (vector_matmul_func): VectorMatMulOp()\n",
              "          )\n",
              "          (mlp): DeepSpeedMLP(\n",
              "            (mlp_gemm_func): MLPGemmOp(\n",
              "              (pre_rms_norm): PreRMSNormOp()\n",
              "            )\n",
              "            (vector_matmul_func): VectorMatMulOp()\n",
              "            (fused_gemm_gelu): GELUGemmOp()\n",
              "            (residual_add_func): ResidualAddOp(\n",
              "              (vector_add): VectorAddOp()\n",
              "            )\n",
              "          )\n",
              "          (layer_norm): LayerNormOp()\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=2560, out_features=50257, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do inference with the optimized model to verify that everything is working as expected."
      ],
      "metadata": {
        "id": "RWBvqWiLRfGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(example, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "logits = ds_model.generate(input_ids,\n",
        "                           do_sample=True,\n",
        "                           num_beams=1,\n",
        "                           min_length=128,\n",
        "                           max_new_tokens=128,\n",
        "                           pad_token_id=50256,\n",
        "                           use_cache=False\n",
        "                          )\n",
        "print(tokenizer.decode(logits[0].tolist()))"
      ],
      "metadata": {
        "id": "qhDqFBEken8u",
        "outputId": "7cffd19e-c41d-480b-f59f-6e9fd2451f0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The story so far: in the beginning, the universe was created. It was a hot universe called the multiverse. There were many universes where it appeared that matter and energy were everywhere; there was nothing but energy, but in the multiverse all energy was made out of matter and was everywhere. This way, nothing that we know of today would exist or at least wouldn't be allowed to exist.\n",
            "\n",
            "Back then, matter was energy that existed as particles, and the energy of matter was negative, and negative energy was bad.\n",
            "\n",
            "Things were like this for some time. Then suddenly, there was an explosion, we're not exactly sure what happened, perhaps the explosion created something, an intelligent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform now benchmark for the DeepSpeed optimized model. The previously defined ```measure_latency``` function is used."
      ],
      "metadata": {
        "id": "1pY3LIeuSMx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = dict(do_sample=True,\n",
        "                       max_length=300,\n",
        "                       pad_token_id=50256,\n",
        "                       use_cache=True)\n",
        "ds_results = measure_latency(ds_model, tokenizer, example,\n",
        "                             ds_model.module.device, generation_args)\n",
        "\n",
        "print(f\"DeepSpeed model: {ds_results[0]}\")"
      ],
      "metadata": {
        "id": "kRGcsLTDh-bT",
        "outputId": "9ce9f66d-4afc-4319-d6b2-479d7e7579b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSpeed model: P95 latency (ms) - 8276.489515550087; Average latency (ms) - 8207.76 +\\- 71.77;\n"
          ]
        }
      ]
    }
  ]
}