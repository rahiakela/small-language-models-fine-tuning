{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/02-running-inference/01_running_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-Neo inference with the HF's Transformers Library"
      ],
      "metadata": {
        "id": "pCU24Cr8XsWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this notebook is to introduce readers to the inference (text generation) with the [GPT-Neo model](https://github.com/EleutherAI/gpt-neo) using the Hugging Face's [Transformers library](https://github.com/huggingface/transformers). It can be executed in the Colab free tier with hardware acceleration (GPU).  "
      ],
      "metadata": {
        "id": "ovU8AL_mAz3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the missing requirements in the Colab VM (HF's Accelerate only)."
      ],
      "metadata": {
        "id": "QZPtUdQxYoJm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cgXtRrrfjzF"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the GPT-Neo 2.7B model and the associated tokenizer from the HF's Hub. The model is loaded in full precision and is then loaded into the GPU."
      ],
      "metadata": {
        "id": "YN89RT34YyTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_id = \"EleutherAI/gpt-neo-2.7B\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
        "model = GPTNeoForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "NA52Ly1dfy05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```log\n",
        "GPTNeoForCausalLM(\n",
        "  (transformer): GPTNeoModel(\n",
        "    (wte): Embedding(50257, 2560)\n",
        "    (wpe): Embedding(2048, 2560)\n",
        "    (drop): Dropout(p=0.0, inplace=False)\n",
        "    (h): ModuleList(\n",
        "      (0-31): 32 x GPTNeoBlock(\n",
        "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
        "        (attn): GPTNeoAttention(\n",
        "          (attention): GPTNeoSelfAttention(\n",
        "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
        "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
        "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
        "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
        "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
        "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
        "          )\n",
        "        )\n",
        "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
        "        (mlp): GPTNeoMLP(\n",
        "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
        "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
        "          (act): NewGELUActivation()\n",
        "          (dropout): Dropout(p=0.0, inplace=False)\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
        "  )\n",
        "  (lm_head): Linear(in_features=2560, out_features=50257, bias=False)\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "gYBpaioNEZk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify where the model layers have been loaded (all in the GPU memory or also RAM and/or disk)."
      ],
      "metadata": {
        "id": "ljgwRcLZZCn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.hf_device_map"
      ],
      "metadata": {
        "id": "zm6qY99tJw-L",
        "outputId": "cbd88c94-be58-44a7-ad31-baf93e5027d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform standard inference (text completion)."
      ],
      "metadata": {
        "id": "-wcXcAoYZU4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The story so far: in the beginning, the universe was created.\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "generated_ids = model.generate(input_ids,\n",
        "                               do_sample=True,\n",
        "                               temperature=0.9,\n",
        "                               max_length=200,\n",
        "                               pad_token_id=50256)\n",
        "generated_text = tokenizer.decode(generated_ids[0])\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "1ZjuJkgMf3-e",
        "outputId": "077d2c97-7f82-4af2-9a8a-da08a6a0b216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The story so far: in the beginning, the universe was created. It had size, shape and form.\n",
            "\n",
            "Then one day, a huge explosion occurred that left the universe in the state we know it now.\n",
            "\n",
            "The beginning of the universe – in its current form\n",
            "\n",
            "So how did the universe start?\n",
            "\n",
            "Well, let’s take a closer look.\n",
            "\n",
            "We’ve been told that before the universe exploded, it was created.\n",
            "\n",
            "What we don’t know\n",
            "\n",
            "But where did the universe come from?\n",
            "\n",
            "We don’t know – we just know that there had to have been something before all this.\n",
            "\n",
            "How big was the universe at the time of the big bang?\n",
            "\n",
            "We don’t know. We know that it had size, shape and form – but nothing else.\n",
            "\n",
            "We see pictures of the early universe with these three characteristics – size, shape and form – but we don\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-shot learning"
      ],
      "metadata": {
        "id": "h8TZF43TFvfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do few-shot text classification (the model can generalize learning from few new and unseen examples."
      ],
      "metadata": {
        "id": "xImTdNsKZf0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Sentence: This movie is very nice.\n",
        "Sentiment: positive\n",
        "\n",
        "#####\n",
        "\n",
        "Sentence: I hated this movie, it sucks.\n",
        "Sentiment: negative\n",
        "\n",
        "#####\n",
        "\n",
        "Sentence: This movie was actually pretty funny.\n",
        "Sentiment: positive\n",
        "\n",
        "#####\n",
        "\n",
        "Sentence: This movie could have been better.\n",
        "Sentiment: neutral\n",
        "\"\"\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "generated_ids = model.generate(input_ids,\n",
        "                               do_sample=True,\n",
        "                               temperature=0.9,\n",
        "                               max_length=200,\n",
        "                               pad_token_id=50256)\n",
        "generated_text = tokenizer.decode(generated_ids[0])\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "eNwj1bPlu0KZ",
        "outputId": "007ec5d8-f0ae-49ed-c2f2-a802fc193459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence: This movie is very nice.\n",
            "Sentiment: positive\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: I hated this movie, it sucks.\n",
            "Sentiment: negative\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie was actually pretty funny.\n",
            "Sentiment: positive\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie could have been better.\n",
            "Sentiment: neutral\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: I liked this movie, it was actually kinda fun.\n",
            "Sentiment: neutral\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: I liked the movie.\n",
            "Sentiment: neutral.\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie did not really make me laugh.\n",
            "Sentiment: neutral\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie is alright.\n",
            "Sentiment: neutral\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: I really enjoyed this movie.\n",
            "Sentiment: neutral\n",
            "\n",
            "#####\n",
            "\n",
            "Sentence: This movie was actually pretty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code generation"
      ],
      "metadata": {
        "id": "Vws8q09Nab5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Instruction: Generate a Python function that lets you reverse a list of integers.\n",
        "\n",
        "Answer: \"\"\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "generated_ids = model.generate(input_ids,\n",
        "                               do_sample=True,\n",
        "                               temperature=0.9,\n",
        "                               max_length=200,\n",
        "                               pad_token_id=50256\n",
        "                               )\n",
        "generated_text = tokenizer.decode(generated_ids[0])\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "xjcA7JkIvibe",
        "outputId": "ae2a6ca1-1608-44cb-e64e-6b2e259b3fec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instruction: Generate a Python function that lets you reverse a list of integers.\n",
            "\n",
            "Answer:  To reverse a list, you can use the function *reverse*:\n",
            "import copy\n",
            "\n",
            "def reverse():    \n",
            "    numbers = [123, 321, 456, 888, 321, 888, 888, 888]   # the list \n",
            "    numbers[::-1] = [555, 444, 222, 555]\n",
            "    return numbers\n",
            "\n",
            "print(reverse())\n",
            "\n",
            "Output: \n",
            "[999, 666, 465, 496, 6666, 666, 999, 222, 456, 555, 722, 556, 222, 555, 722, 555, 444, 888, 888, 888]\n",
            "\n",
            "<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Prompting"
      ],
      "metadata": {
        "id": "35p6opJrF5zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do batch text completion."
      ],
      "metadata": {
        "id": "1bEY9T56aibv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Once there was a man \", \"The weather today will be \", \"A great soccer player must \"]\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoding = tokenizer(texts, padding=True, return_tensors='pt').to(device)\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(**encoding,\n",
        "                                   do_sample=True,\n",
        "                                   temperature=0.9,\n",
        "                                   max_length=50,\n",
        "                                   pad_token_id=50256)\n",
        "generated_texts = tokenizer.batch_decode(\n",
        "    generated_ids, skip_special_tokens=True)\n",
        "\n",
        "for text in generated_texts:\n",
        "  print(\"---------\")\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "VYLTgEhjwkes",
        "outputId": "9c1aa51a-89de-430e-d220-8a52aec0232e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "Once there was a man  \n",
            "Who was given to drink  \n",
            "And as he was lying  \n",
            "In bed he said to a friend,  \n",
            "“I think I shall drown myself.” “No,\n",
            "---------\n",
            "The weather today will be \n",
            "hot with a high of \n",
            "86 and a low of 72. \n",
            "The high will last all day long and \n",
            "the low will start after sunset. \n",
            "Temperatures will be in the low\n",
            "---------\n",
            "A great soccer player must \n",
            "always have confidence on the field,\n",
            "and that’s what we’re trying to \n",
            "figure out right now.\n",
            ">> Our next speaker, \n",
            "she is a professional athlete.\n",
            ">>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating LLM"
      ],
      "metadata": {
        "id": "kJYHoE-TFiE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness\n",
        "cd lm-evaluation-harness\n",
        "pip install -e ."
      ],
      "metadata": {
        "id": "Y8Kj5JHAHXdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lm_eval \\\n",
        "  --model hf-auto \\\n",
        "  --model_args pretrained=EleutherAI/gpt-neo-2.7B,dtype=\"float16\" \\\n",
        "  --tasks wikitext \\\n",
        "  --device cuda:0"
      ],
      "metadata": {
        "id": "qLolrW8IIHFl",
        "outputId": "745f3b30-5bd6-45d8-cac0-d7427597e0dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-23 05:57:25.102717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1758607045.133734    9967 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1758607045.143366    9967 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1758607045.166206    9967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758607045.166239    9967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758607045.166248    9967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1758607045.166254    9967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "INFO:lm_eval.__main__:Selected Tasks: ['wikitext']\n",
            "INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "INFO:lm_eval.evaluator:Initializing hf-auto model, with arguments: {'pretrained': 'EleutherAI/gpt-neo-2.7B', 'dtype': 'float16'}\n",
            "INFO:lm_eval.models.huggingface:Using device 'cuda:0'\n",
            "INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
            "    sys.exit(cli_evaluate())\n",
            "             ^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
            "    results = evaluator.simple_evaluate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/utils.py\", line 456, in _wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
            "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
            "    return cls(**args, **args2)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 219, in __init__\n",
            "    self._create_model(\n",
            "  File \"/content/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 631, in _create_model\n",
            "    self._model = self.AUTO_MODEL_CLASS.from_pretrained(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 5176, in from_pretrained\n",
            "    ) = cls._load_pretrained_model(\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 5639, in _load_pretrained_model\n",
            "    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 946, in load_shard_file\n",
            "    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 813, in _load_state_dict_into_meta_model\n",
            "    param = param[...]\n",
            "            ~~~~~^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 2418 has 10.35 GiB memory in use. Process 106614 has 4.38 GiB memory in use. Of the allocated memory 4.15 GiB is allocated by PyTorch, and 116.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KV Caching"
      ],
      "metadata": {
        "id": "h6Y3JZvnHPOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmarking the model on text completion: comparing the cases where the KV cache is used to those where it isn't."
      ],
      "metadata": {
        "id": "refKUvsCawPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "prompt = \"The story so far: in the beginning, the universe was created.\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "for use_cache in (True, False):\n",
        "  times = []\n",
        "  for _ in range(20):\n",
        "    start = time.time()\n",
        "    generated_ids = model.generate(input_ids,\n",
        "                                  do_sample=True,\n",
        "                                  temperature=0.9,\n",
        "                                  max_length=200,\n",
        "                                  pad_token_id=50256,\n",
        "                                  use_cache=use_cache)\n",
        "    times.append(time.time() - start)\n",
        "  print(f\"{'Using' if use_cache else 'No'} KV cache: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
      ],
      "metadata": {
        "id": "CKVJVqwSTXgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimating the generation time"
      ],
      "metadata": {
        "id": "hErcSGvzG9KU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmarking the model's total generation time."
      ],
      "metadata": {
        "id": "sbDehxEOfA4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "prompt = \"The story so far: in the beginning, the universe was created.\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "max_length = 300\n",
        "times = []\n",
        "inference_runs = 21\n",
        "for _ in range(inference_runs):\n",
        "  start = time.time()\n",
        "  generated_ids = model.generate(input_ids,\n",
        "                                do_sample=True,\n",
        "                                temperature=0.9,\n",
        "                                max_length=max_length,\n",
        "                                pad_token_id=50256,\n",
        "                                )\n",
        "  times.append(time.time() - start)\n",
        "print(f\"Average Total Generation time: {round(np.mean(times[1:]), 3)} +- {round(np.std(times[1:]), 3)} seconds\")"
      ],
      "metadata": {
        "id": "wfuH8Djp0QgZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}