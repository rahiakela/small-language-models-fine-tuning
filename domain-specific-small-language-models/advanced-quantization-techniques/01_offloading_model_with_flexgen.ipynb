{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/advanced-quantization-techniques/01_offloading_model_with_flexgen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using FlexGen to Offload OPT Models' Weights to RAM and Disk"
      ],
      "metadata": {
        "id": "RHR7quvS5Ld1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code in this notebook is to perform inference with a Meta AI's OPT models, by offloading part of models' weights from VRAM to RAM and/or disk, using the [FlexGen](https://github.com/FMInference/FlexLLMGen/) generation engine programmatically. While the code refers to the [OPT 1.3 B](https://huggingface.co/facebook/opt-1.3b) model, the same applies to any other model from the same family. It requires hardware acceleration to be executed.  "
      ],
      "metadata": {
        "id": "RzxsVG0AVHKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the FlexGen from source."
      ],
      "metadata": {
        "id": "cXnjUWSo65zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FMInference/FlexLLMGen.git\n",
        "%cd FlexLLMGen\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "PpVXlrLZFtG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate FlexGen"
      ],
      "metadata": {
        "id": "0QHp1hsJX-SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m flexllmgen.flex_opt --model facebook/opt-1.3b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvt4Y2lmX_wf",
        "outputId": "08ad8c72-b384-41fb-d2a5-1e8016ce1ab6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 81% 315/389 [13:28<02:25,  1.97s/it]\u001b[A\n",
            " 82% 319/389 [13:41<02:53,  2.48s/it]\u001b[A\n",
            " 83% 321/389 [13:55<04:01,  3.55s/it]\u001b[A\n",
            " 84% 325/389 [13:57<02:28,  2.33s/it]\u001b[A\n",
            " 84% 327/389 [14:00<02:09,  2.09s/it]\u001b[A\n",
            " 85% 329/389 [14:05<02:12,  2.21s/it]\u001b[A\n",
            " 85% 331/389 [14:06<01:41,  1.74s/it]\u001b[A\n",
            " 86% 335/389 [14:19<02:09,  2.39s/it]\u001b[A\n",
            " 87% 337/389 [14:28<02:30,  2.89s/it]\u001b[A\n",
            " 88% 341/389 [14:30<01:34,  1.96s/it]\u001b[A\n",
            " 88% 343/389 [14:33<01:25,  1.86s/it]\u001b[A\n",
            " 89% 345/389 [14:36<01:18,  1.79s/it]\u001b[A\n",
            " 89% 347/389 [14:38<01:06,  1.59s/it]\u001b[A\n",
            " 90% 351/389 [14:49<01:17,  2.05s/it]\u001b[A\n",
            " 91% 353/389 [15:01<01:46,  2.95s/it]\u001b[A\n",
            " 92% 357/389 [15:02<00:59,  1.85s/it]\u001b[A\n",
            " 92% 359/389 [15:05<00:54,  1.81s/it]\u001b[A\n",
            " 93% 361/389 [15:11<00:59,  2.11s/it]\u001b[A\n",
            " 93% 363/389 [15:13<00:45,  1.76s/it]\u001b[A\n",
            " 94% 364/389 [15:13<00:38,  1.52s/it]\u001b[A\n",
            " 94% 367/389 [15:25<00:57,  2.60s/it]\u001b[A\n",
            " 95% 369/389 [15:38<01:11,  3.60s/it]\u001b[A\n",
            " 96% 373/389 [15:40<00:35,  2.19s/it]\u001b[A\n",
            " 96% 375/389 [15:42<00:27,  1.97s/it]\u001b[A\n",
            " 97% 377/389 [15:46<00:23,  1.96s/it]\u001b[A\n",
            " 97% 379/389 [15:48<00:16,  1.68s/it]\u001b[A\n",
            " 98% 383/389 [15:58<00:12,  2.06s/it]\u001b[A\n",
            " 99% 385/389 [16:09<00:11,  2.85s/it]\u001b[A\n",
            "100% 389/389 [17:19<00:00,  8.74s/it]\u001b[A\n",
            "Convert format: 100% 1/1 [17:29<00:00, 1049.39s/it]\n",
            "warmup - generate\n",
            "benchmark - generate\n",
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1125: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
            "  return isinstance(obj, torch.Tensor)\n",
            "Outputs:\n",
            "----------------------------------------------------------------------\n",
            "0: Paris is the capital city of France. It is the most populous city in France, with an estimated population of 6,848,000 in 2016. It is the second most populous city\n",
            "----------------------------------------------------------------------\n",
            "3: Paris is the capital city of France. It is the most populous city in France, with an estimated population of 6,848,000 in 2016. It is the second most populous city\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "TorchDevice: cuda:0\n",
            "  cur_mem: 2.6505 GB,  peak_mem: 3.2478 GB\n",
            "TorchDevice: cpu\n",
            "  cur_mem: 0.0000 GB,  peak_mem: 0.0000 GB\n",
            "model size: 2.443 GB\tcache size: 0.398 GB\thidden size (p): 0.008 GB\n",
            "peak gpu mem: 3.248 GB\tprojected: False\n",
            "prefill latency: 0.339 s\tprefill throughput: 6046.475 token/s\n",
            "decode latency: 0.988 s\tdecode throughput: 125.569 token/s\n",
            "total latency: 1.326 s\ttotal throughput: 96.515 token/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m flexllmgen.flex_opt --model facebook/opt-6.7b --percent 50"
      ],
      "metadata": {
        "id": "FI_gf7C6mpQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using FlexGen and the Transformers library programmatically"
      ],
      "metadata": {
        "id": "IjHOD3HNnwx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required FlexGen classes."
      ],
      "metadata": {
        "id": "U5I5V7gA72Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flexllmgen.flex_opt import (Policy, OptLM, ExecutionEnv, CompressionConfig, str2bool)"
      ],
      "metadata": {
        "id": "ZpJyirZsoA3B"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the OPT 1.3 B tokenizer form the Hugging Face's Hub."
      ],
      "metadata": {
        "id": "Ecdzz6_h8AD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"facebook/opt-1.3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
        "tokenizer.add_bos_token = False\n",
        "stop = tokenizer(\"\\n\").input_ids[0]"
      ],
      "metadata": {
        "id": "9KRpa3tJ6knM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the FlexGen execution environment."
      ],
      "metadata": {
        "id": "Rtmal7Oc8LcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "offload_dir = './flexgen_offload'\n",
        "env = ExecutionEnv.create(offload_dir)"
      ],
      "metadata": {
        "id": "LpT-1eLU7cWq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare a list of prompts for batch inference."
      ],
      "metadata": {
        "id": "Abl4xDU98jPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Question: Where were the 2004 Olympics held?\\n\"\n",
        "    \"Answer: Athens, Greece\\n\"\n",
        "    \"Question: What is the longest river on the earth?\\n\"\n",
        "    \"Answer:\",\n",
        "\n",
        "    \"Extract the airport codes from this text.\\n\"\n",
        "    \"Text: \\\"I want a flight from New York to San Francisco.\\\"\\n\"\n",
        "    \"Airport codes: JFK, SFO.\\n\"\n",
        "    \"Text: \\\"I want you to book a flight from Phoenix to Las Vegas.\\\"\\n\"\n",
        "    \"Airport codes:\",\n",
        "]"
      ],
      "metadata": {
        "id": "Fwm6oye_8LUs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup an offloading policy."
      ],
      "metadata": {
        "id": "W9HOysuq8mqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = Policy(len(prompts), 1,\n",
        "                70, 30, 70, 30, 100, 0,\n",
        "                overlap=True, sep_layer=True, pin_weight=True,\n",
        "                cpu_cache_compute=True, attn_sparsity=1.0,\n",
        "                compress_weight=True,\n",
        "                comp_weight_config=CompressionConfig(\n",
        "                    num_bits=4, group_size=64,\n",
        "                    group_dim=0, symmetric=False),\n",
        "                compress_cache=False, # Set compress_cache to False\n",
        "                comp_cache_config=CompressionConfig(\n",
        "                    num_bits=4, group_size=64,\n",
        "                    group_dim=2, symmetric=False)\n",
        "                )"
      ],
      "metadata": {
        "id": "cNMUhRWC8FZr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model to be executed through the FlexGen inference engine and following the preliminary defined offloading policies. This step also downloads the model's checkpoints from the Hugging Face's Hub and manages the conversion process."
      ],
      "metadata": {
        "id": "gvOFzEh_8v_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '~/opt_weights'\n",
        "model = OptLM(model_id, env, path, policy)"
      ],
      "metadata": {
        "id": "YPPXnIrh6zgU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text for the given set of prompts and then display the generated result for each one."
      ],
      "metadata": {
        "id": "t5gUNQNN9Bhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generate...\")\n",
        "inputs = tokenizer(prompts, padding=\"max_length\", max_length=128)\n",
        "output_ids = model.generate(\n",
        "    inputs.input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    max_new_tokens=32,\n",
        "    stop=stop)\n",
        "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "print(\"Outputs:\\n\" + 70 * '-')\n",
        "for i in [0, len(outputs)-1]:\n",
        "    print(f\"{i}: {outputs[i]}\")\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "-eFB8TIF-JBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4481bd8-a83c-4d91-8fd1-5a01c0c262f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs:\n",
            "----------------------------------------------------------------------\n",
            "0: Question: Where were the 2004 Olympics held?\n",
            "Answer: Athens, Greece\n",
            "Question: What is the longest river on the earth?\n",
            "Answer: The Nile\n",
            "Question: What is the number of Grecian tigers?\n",
            "Answer: 10,000\n",
            "Question: What is the capital of Macedonia?\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "1: Extract the airport codes from this text.\n",
            "Text: \"I want a flight from New York to San Francisco.\"\n",
            "Airport codes: JFK, SFO.\n",
            "Text: \"I want you to book a flight from Phoenix to Las Vegas.\"\n",
            "Airport codes: PHX, LVG.\n",
            "\n",
            "Text: I want to book a flight from New York to San Francisco.\n",
            "Airport codes: JFK, SFO\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shutdown the FlexGen execution environment when done."
      ],
      "metadata": {
        "id": "6W9F4pEI9LDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shutting down...\")\n",
        "env.close_copy_threads()"
      ],
      "metadata": {
        "id": "EqCEdaED-QhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd8bcee-3708-4303-eeb2-916e2c83ff4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down...\n"
          ]
        }
      ]
    }
  ]
}