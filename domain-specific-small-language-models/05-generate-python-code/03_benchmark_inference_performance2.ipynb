{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/05-generate-python-code/03_benchmark_inference_performance2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro2mCsr9wNoZ"
      },
      "source": [
        "## Benchmarking Python Code Generation with Vanilla and 8-bit Quantized StarCoder2 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code in this notebook is to benchmark inference performance (latency and throughtput) when generating Python code using a vanilla [StarCoder2](https://huggingface.co/Salesforce/codegen-350M-mono) 2B model, and after 8-bit quantization of the same model. It reuqires hardware acceleration.  "
      ],
      "metadata": {
        "id": "U2M1klT1Edcm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxm-Oc-FxRGx"
      },
      "source": [
        "Install the missing requirements in the ColabVM (only HF's Optimum for the ONNX runtime and Bitsandbytes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBniEqbYvTsI"
      },
      "outputs": [],
      "source": [
        "!pip install optimum[onnxruntime-gpu]==1.21.2\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpSD_2ERL_Q1"
      },
      "source": [
        "Upgrade the Numpy and HF's Transformers packages to the latest version. A restart of the VM is needed after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0qslPrQC5QU"
      },
      "outputs": [],
      "source": [
        "!pip install -U numpy transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qgo8j3yAEC_"
      },
      "source": [
        "### Vanilla Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3uieL3kxc-C"
      },
      "source": [
        "Download the StarCoder2-3B model (in bfloat16) and its tokenizer from the HF's Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oQXJ5wJJuMUd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"bigcode/starcoder2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9JpoSzoHaOR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2ee6a9-3f69-4508-95d6-378459366c3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2Attention(\n",
              "          (q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.bfloat16)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlUHkMBOxtGL"
      },
      "source": [
        "Set a text prompt (a Python function header) to be used across benchmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Euq2A50ANLW"
      },
      "outputs": [],
      "source": [
        "prompt = \"def print_hello_world():\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QIHUGmFx1EB"
      },
      "source": [
        "The code in the following cell is just to verify that model and tokenizer have been downloaded properly. You can skip its execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3_AXjHFvaTO8",
        "outputId": "1b759ade-6604-4172-e699-4558a01f17a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqw9ViDFNGcN",
        "outputId": "a3a4be63-07ea-4f34-d99d-9d22afaa4547"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def print_hello_world():\n",
            "    print(\"Hello World\")\n",
            "\n",
            "def print_hello_world_with_name(name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"def fibonacci(n):\"\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs)"
      ],
      "metadata": {
        "id": "LbJaAkTaNqZ4",
        "outputId": "32855924-baf0-4aa7-ca48-1ee1d6d0abcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVV7pxY1NtZT",
        "outputId": "ef44bcc4-b924-4195-c1a7-d1be36cb8304"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def fibonacci(n):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td1Nl5uUx_6v"
      },
      "source": [
        "Setup a Transformers' pipeline for inference with the vanilla model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HTBIqZ9MmF6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a517e3ef-8063-4aa9-9d3e-b93ab4079112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            do_sample=True,\n",
        "            use_cache=True,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            max_length=14\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3x3oWkbyHFw"
      },
      "source": [
        "Test the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p4NxE8a1m6xE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe224b9-cf86-49e8-bcb3-e13e6b7650c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def fibonacci(n):\n",
            "    if n == 0:\n",
            "       \n"
          ]
        }
      ],
      "source": [
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxNVcKjGgGS"
      },
      "source": [
        "Save the checkpoints locally, to be reused when quantizing it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DIIsBPsmudM3"
      },
      "outputs": [],
      "source": [
        "checkpoint_save_dir = 'local-pt-checkpoint'\n",
        "tokenizer.save_pretrained(checkpoint_save_dir)\n",
        "model.save_pretrained(checkpoint_save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q70kfNsKCeYx"
      },
      "source": [
        "Define some utils for benchmarking (more details about them in chapter 6 of the book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pL77v2MCEhNx"
      },
      "outputs": [],
      "source": [
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass\n",
        "from time import perf_counter\n",
        "\n",
        "@contextmanager\n",
        "def track_infer_time(time_buffer):\n",
        "    start_time = perf_counter()\n",
        "    yield\n",
        "    end_time = perf_counter()\n",
        "\n",
        "    time_buffer.append(end_time - start_time)\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkInferenceResult:\n",
        "    model_inference_time: [int]\n",
        "    optimized_model_path: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il5ctRs0yU7Y"
      },
      "source": [
        "Define a custom funtion to be reused across benchmarks with the different versions of the model under evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kROG4REwFO2d"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "\n",
        "def benchmark_inference(providers_dict, pipe, prompt, results):\n",
        "  for device, label in PROVIDERS:\n",
        "      for _ in trange(10, desc=\"Warming up\"):\n",
        "          pipe(prompt)\n",
        "\n",
        "      time_buffer = []\n",
        "      for _ in trange(100, desc=f\"Tracking inference time ({label})\"):\n",
        "        with track_infer_time(time_buffer):\n",
        "            pipe(prompt)\n",
        "\n",
        "      results[label] = BenchmarkInferenceResult(\n",
        "          time_buffer,\n",
        "          None\n",
        "      )\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9-RNdkvyfQ1"
      },
      "source": [
        "Execute the benchmarks for the StarCoder2 vanilla model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dL9m9HtcFsP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e06204c-c774-4bbe-91d7-d98c48fd61a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  10%|█         | 1/10 [00:00<00:05,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  20%|██        | 2/10 [00:01<00:04,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  30%|███       | 3/10 [00:01<00:03,  1.88it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  40%|████      | 4/10 [00:02<00:03,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  50%|█████     | 5/10 [00:02<00:02,  1.92it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  60%|██████    | 6/10 [00:03<00:01,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  70%|███████   | 7/10 [00:03<00:01,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  80%|████████  | 8/10 [00:03<00:00,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  90%|█████████ | 9/10 [00:04<00:00,  2.25it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up: 100%|██████████| 10/10 [00:04<00:00,  2.09it/s]\n",
            "Tracking inference time (PyTorch GPU):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   1%|          | 1/100 [00:00<00:42,  2.32it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   2%|▏         | 2/100 [00:00<00:42,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   3%|▎         | 3/100 [00:01<00:41,  2.33it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   4%|▍         | 4/100 [00:01<00:40,  2.35it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   5%|▌         | 5/100 [00:02<00:40,  2.34it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   6%|▌         | 6/100 [00:02<00:40,  2.34it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   7%|▋         | 7/100 [00:03<00:40,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   8%|▊         | 8/100 [00:03<00:39,  2.33it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   9%|▉         | 9/100 [00:03<00:38,  2.34it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  10%|█         | 10/100 [00:04<00:38,  2.34it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  11%|█         | 11/100 [00:04<00:38,  2.34it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  12%|█▏        | 12/100 [00:05<00:37,  2.32it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  13%|█▎        | 13/100 [00:05<00:37,  2.32it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  14%|█▍        | 14/100 [00:06<00:37,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  15%|█▌        | 15/100 [00:06<00:36,  2.32it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  16%|█▌        | 16/100 [00:06<00:36,  2.33it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  17%|█▋        | 17/100 [00:07<00:35,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  18%|█▊        | 18/100 [00:07<00:35,  2.33it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  19%|█▉        | 19/100 [00:08<00:36,  2.20it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  20%|██        | 20/100 [00:08<00:37,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  21%|██        | 21/100 [00:09<00:37,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  22%|██▏       | 22/100 [00:09<00:38,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  23%|██▎       | 23/100 [00:10<00:37,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  24%|██▍       | 24/100 [00:10<00:35,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  25%|██▌       | 25/100 [00:11<00:34,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  26%|██▌       | 26/100 [00:11<00:33,  2.23it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  27%|██▋       | 27/100 [00:11<00:32,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  28%|██▊       | 28/100 [00:12<00:31,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  29%|██▉       | 29/100 [00:12<00:31,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  30%|███       | 30/100 [00:13<00:30,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  31%|███       | 31/100 [00:13<00:29,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  32%|███▏      | 32/100 [00:14<00:29,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  33%|███▎      | 33/100 [00:14<00:29,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  34%|███▍      | 34/100 [00:14<00:28,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  35%|███▌      | 35/100 [00:15<00:28,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  36%|███▌      | 36/100 [00:15<00:27,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  37%|███▋      | 37/100 [00:16<00:27,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  38%|███▊      | 38/100 [00:16<00:27,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  39%|███▉      | 39/100 [00:17<00:26,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  40%|████      | 40/100 [00:17<00:26,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  41%|████      | 41/100 [00:18<00:25,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  42%|████▏     | 42/100 [00:18<00:25,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  43%|████▎     | 43/100 [00:18<00:24,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  44%|████▍     | 44/100 [00:19<00:24,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  45%|████▌     | 45/100 [00:19<00:23,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  46%|████▌     | 46/100 [00:20<00:25,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  47%|████▋     | 47/100 [00:20<00:25,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  48%|████▊     | 48/100 [00:21<00:24,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  49%|████▉     | 49/100 [00:21<00:25,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  50%|█████     | 50/100 [00:22<00:24,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  51%|█████     | 51/100 [00:22<00:23,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  52%|█████▏    | 52/100 [00:23<00:22,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  53%|█████▎    | 53/100 [00:23<00:21,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  54%|█████▍    | 54/100 [00:24<00:20,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  55%|█████▌    | 55/100 [00:24<00:19,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  56%|█████▌    | 56/100 [00:24<00:19,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  57%|█████▋    | 57/100 [00:25<00:18,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  58%|█████▊    | 58/100 [00:25<00:18,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  59%|█████▉    | 59/100 [00:26<00:17,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  60%|██████    | 60/100 [00:26<00:17,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  61%|██████    | 61/100 [00:27<00:16,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  62%|██████▏   | 62/100 [00:27<00:16,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  63%|██████▎   | 63/100 [00:27<00:16,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  64%|██████▍   | 64/100 [00:28<00:15,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  65%|██████▌   | 65/100 [00:28<00:15,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  66%|██████▌   | 66/100 [00:29<00:14,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  67%|██████▋   | 67/100 [00:29<00:14,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  68%|██████▊   | 68/100 [00:30<00:13,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  69%|██████▉   | 69/100 [00:30<00:13,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  70%|███████   | 70/100 [00:31<00:13,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  71%|███████   | 71/100 [00:31<00:12,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  72%|███████▏  | 72/100 [00:31<00:12,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  73%|███████▎  | 73/100 [00:32<00:12,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  74%|███████▍  | 74/100 [00:32<00:12,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  75%|███████▌  | 75/100 [00:33<00:12,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  76%|███████▌  | 76/100 [00:34<00:12,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  77%|███████▋  | 77/100 [00:34<00:11,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  78%|███████▊  | 78/100 [00:34<00:10,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  79%|███████▉  | 79/100 [00:35<00:09,  2.17it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  80%|████████  | 80/100 [00:35<00:09,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  81%|████████  | 81/100 [00:36<00:08,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  82%|████████▏ | 82/100 [00:36<00:08,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  83%|████████▎ | 83/100 [00:37<00:07,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  84%|████████▍ | 84/100 [00:37<00:07,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  85%|████████▌ | 85/100 [00:37<00:06,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  86%|████████▌ | 86/100 [00:38<00:06,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  87%|████████▋ | 87/100 [00:38<00:05,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  88%|████████▊ | 88/100 [00:39<00:05,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  89%|████████▉ | 89/100 [00:39<00:04,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  90%|█████████ | 90/100 [00:40<00:04,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  91%|█████████ | 91/100 [00:40<00:03,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  92%|█████████▏| 92/100 [00:41<00:03,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  93%|█████████▎| 93/100 [00:41<00:03,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  94%|█████████▍| 94/100 [00:41<00:02,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  95%|█████████▌| 95/100 [00:42<00:02,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  96%|█████████▌| 96/100 [00:42<00:01,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  97%|█████████▋| 97/100 [00:43<00:01,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  98%|█████████▊| 98/100 [00:43<00:00,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  99%|█████████▉| 99/100 [00:44<00:00,  2.17it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU): 100%|██████████| 100/100 [00:44<00:00,  2.24it/s]\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "PROVIDERS = {\n",
        "    (\"gpu\", \"PyTorch GPU\"),\n",
        "}\n",
        "results = benchmark_inference(PROVIDERS, pipe, prompt, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKlAIq_oz81N"
      },
      "source": [
        "### 8-bit Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KESvr_wOzy7i"
      },
      "source": [
        "To prevent potential out of memory issues, let's do some VRAM and RAM cleanup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TQUAblExJWvm"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "model.cpu()\n",
        "del model\n",
        "del pipe\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nijBTdmv0V41"
      },
      "source": [
        "Let's do 8-bit quantization of the original model using Bitsandbytes library and save it to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YfM7NeIb2vqR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "8c5fd1c5f4f54f508397dcdbb6d567ec",
            "3f1bd9bdddb14bbbbb97100f88c65b6f",
            "ee6567c55f394b66a5e8553858793040",
            "a368ca9de61644b9b516c093c329ca5f",
            "4a7846b911c146c4a691331087133f45",
            "e48545f9e9a1464b99ac2bdbd688dbf8",
            "46d1ae5fa72545998e6cbac5ca869189",
            "908e088d3ea34c71802236ed8a9b59a5",
            "913dc9a6117b4b8394306f8bb8b9b25c",
            "6b18128b83f1418e8e24ff67f176dc58",
            "315c08d5b8f44bc8b66ddd4ddd50a76c"
          ]
        },
        "outputId": "e450f3ec-6ca7-4ec0-d152-aeee67b72614"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c5fd1c5f4f54f508397dcdbb6d567ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2Attention(\n",
              "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear8bitLt(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear8bitLt(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_save_dir)\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(checkpoint_save_dir,\n",
        "                                        quantization_config=quantization_config)\n",
        "quantized_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW11NW8NfpUY"
      },
      "source": [
        "The code in the following cell is just to verify that model and tokenizer have been downloaded properly. You can skip its execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "evHyv8igwX64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2061c12-df2c-4bf2-8d45-b87f4f3e6ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def fibonacci(n):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = quantized_model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "klEhjGNPI4xk"
      },
      "outputs": [],
      "source": [
        "quantized_model.save_pretrained('local-8bit-checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kJDOQ80HKlWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d5ff4d-3fb7-4e03-cbf9-0b9fee58efe0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2Attention(\n",
              "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear8bitLt(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear8bitLt(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "checkpoint_8bit_save_dir = 'local-8bit-checkpoint'\n",
        "\n",
        "# Load the quantized model from the specified directory\n",
        "quantized_model_loaded = AutoModelForCausalLM.from_pretrained(checkpoint_8bit_save_dir)\n",
        "quantized_model_loaded.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUosoY7N0o3C"
      },
      "source": [
        "Setup the pipeline for inference with the quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6GQypqF7Qco_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c09b696d-4c43-4cc7-8f8a-f43e745707ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "            model=quantized_model_loaded,\n",
        "            tokenizer=tokenizer,\n",
        "            do_sample=True,\n",
        "            use_cache=True,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            max_length=14,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0GqH1E17dz"
      },
      "source": [
        "Verify that the pipeline works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ehBfVq_CQq28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfbc84a-b94f-4aad-f35c-a22b37d910c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'def fibonacci(n):\\n    if n == 0:\\n       '}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "result = pipe(prompt)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGUi3AaYDC-I"
      },
      "source": [
        "Repeat the benchmark on the quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "l4cTBDxVIu4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90160c43-51a4-40eb-f1c4-f5823c444ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  10%|█         | 1/10 [00:01<00:10,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  20%|██        | 2/10 [00:02<00:09,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  30%|███       | 3/10 [00:03<00:08,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  40%|████      | 4/10 [00:05<00:07,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  50%|█████     | 5/10 [00:06<00:06,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  60%|██████    | 6/10 [00:07<00:04,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  70%|███████   | 7/10 [00:08<00:03,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  80%|████████  | 8/10 [00:09<00:02,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  90%|█████████ | 9/10 [00:10<00:01,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up: 100%|██████████| 10/10 [00:12<00:00,  1.20s/it]\n",
            "Tracking inference time (Quant GPU):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   1%|          | 1/100 [00:01<01:50,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   2%|▏         | 2/100 [00:02<01:50,  1.13s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   3%|▎         | 3/100 [00:03<01:54,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   4%|▍         | 4/100 [00:04<02:05,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   5%|▌         | 5/100 [00:06<02:00,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   6%|▌         | 6/100 [00:07<01:55,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   7%|▋         | 7/100 [00:08<01:51,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   8%|▊         | 8/100 [00:09<01:49,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   9%|▉         | 9/100 [00:10<01:47,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  10%|█         | 10/100 [00:11<01:45,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  11%|█         | 11/100 [00:13<01:43,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  12%|█▏        | 12/100 [00:14<01:41,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  13%|█▎        | 13/100 [00:15<01:43,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  14%|█▍        | 14/100 [00:17<01:50,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  15%|█▌        | 15/100 [00:18<01:45,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  16%|█▌        | 16/100 [00:19<01:42,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  17%|█▋        | 17/100 [00:20<01:39,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  18%|█▊        | 18/100 [00:21<01:37,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  19%|█▉        | 19/100 [00:22<01:35,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  20%|██        | 20/100 [00:23<01:33,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  21%|██        | 21/100 [00:25<01:31,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  22%|██▏       | 22/100 [00:26<01:29,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  23%|██▎       | 23/100 [00:27<01:32,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  24%|██▍       | 24/100 [00:29<01:37,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  25%|██▌       | 25/100 [00:30<01:33,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  26%|██▌       | 26/100 [00:31<01:29,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  27%|██▋       | 27/100 [00:32<01:27,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  28%|██▊       | 28/100 [00:33<01:24,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  29%|██▉       | 29/100 [00:34<01:23,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  30%|███       | 30/100 [00:35<01:21,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  31%|███       | 31/100 [00:37<01:20,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  32%|███▏      | 32/100 [00:38<01:18,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  33%|███▎      | 33/100 [00:39<01:19,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  34%|███▍      | 34/100 [00:40<01:24,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  35%|███▌      | 35/100 [00:42<01:20,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  36%|███▌      | 36/100 [00:43<01:17,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  37%|███▋      | 37/100 [00:44<01:15,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  38%|███▊      | 38/100 [00:45<01:12,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  39%|███▉      | 39/100 [00:46<01:10,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  40%|████      | 40/100 [00:47<01:09,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  41%|████      | 41/100 [00:48<01:08,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  42%|████▏     | 42/100 [00:50<01:06,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  43%|████▎     | 43/100 [00:51<01:07,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  44%|████▍     | 44/100 [00:52<01:11,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  45%|████▌     | 45/100 [00:54<01:08,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  46%|████▌     | 46/100 [00:55<01:05,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  47%|████▋     | 47/100 [00:56<01:03,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  48%|████▊     | 48/100 [00:57<01:01,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  49%|████▉     | 49/100 [00:58<00:59,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  50%|█████     | 50/100 [00:59<00:57,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  51%|█████     | 51/100 [01:00<00:56,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  52%|█████▏    | 52/100 [01:02<00:55,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  53%|█████▎    | 53/100 [01:03<00:55,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  54%|█████▍    | 54/100 [01:04<00:58,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  55%|█████▌    | 55/100 [01:05<00:56,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  56%|█████▌    | 56/100 [01:07<00:53,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  57%|█████▋    | 57/100 [01:08<00:51,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  58%|█████▊    | 58/100 [01:09<00:49,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  59%|█████▉    | 59/100 [01:10<00:48,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  60%|██████    | 60/100 [01:11<00:46,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  61%|██████    | 61/100 [01:12<00:45,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  62%|██████▏   | 62/100 [01:13<00:43,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  63%|██████▎   | 63/100 [01:15<00:44,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  64%|██████▍   | 64/100 [01:16<00:46,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  65%|██████▌   | 65/100 [01:17<00:43,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  66%|██████▌   | 66/100 [01:19<00:41,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  67%|██████▋   | 67/100 [01:20<00:39,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  68%|██████▊   | 68/100 [01:21<00:37,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  69%|██████▉   | 69/100 [01:22<00:36,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  70%|███████   | 70/100 [01:23<00:34,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  71%|███████   | 71/100 [01:24<00:33,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  72%|███████▏  | 72/100 [01:25<00:32,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  73%|███████▎  | 73/100 [01:27<00:32,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  74%|███████▍  | 74/100 [01:28<00:33,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  75%|███████▌  | 75/100 [01:29<00:31,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  76%|███████▌  | 76/100 [01:31<00:29,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  77%|███████▋  | 77/100 [01:32<00:27,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  78%|███████▊  | 78/100 [01:33<00:25,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  79%|███████▉  | 79/100 [01:34<00:24,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  80%|████████  | 80/100 [01:35<00:23,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  81%|████████  | 81/100 [01:36<00:21,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  82%|████████▏ | 82/100 [01:37<00:20,  1.15s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  83%|████████▎ | 83/100 [01:39<00:20,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  84%|████████▍ | 84/100 [01:40<00:20,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  85%|████████▌ | 85/100 [01:41<00:19,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  86%|████████▌ | 86/100 [01:43<00:18,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  87%|████████▋ | 87/100 [01:44<00:16,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  88%|████████▊ | 88/100 [01:45<00:14,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  89%|████████▉ | 89/100 [01:46<00:13,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  90%|█████████ | 90/100 [01:47<00:11,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  91%|█████████ | 91/100 [01:49<00:10,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  92%|█████████▏| 92/100 [01:50<00:09,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  93%|█████████▎| 93/100 [01:51<00:08,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  94%|█████████▍| 94/100 [01:53<00:07,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  95%|█████████▌| 95/100 [01:54<00:06,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  96%|█████████▌| 96/100 [01:55<00:04,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  97%|█████████▋| 97/100 [01:56<00:03,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  98%|█████████▊| 98/100 [01:57<00:02,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  99%|█████████▉| 99/100 [01:58<00:01,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU): 100%|██████████| 100/100 [01:59<00:00,  1.20s/it]\n"
          ]
        }
      ],
      "source": [
        "PROVIDERS = {\n",
        "    (\"ort\", \"Quant GPU\"),\n",
        "}\n",
        "results = benchmark_inference(PROVIDERS, pipe, prompt, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56NkSjpODGv5"
      },
      "source": [
        "### Results of the Benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ak1DWKV2Y5p"
      },
      "source": [
        "Visually compare the average inference times across benchmarks for the 2 different versions of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2QmZNhc4DMhq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "c7d15e4f-62b6-4d28-e20c-fa985f461998"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"0af14dd5-8830-4bdd-8164-f94a206239a9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0af14dd5-8830-4bdd-8164-f94a206239a9\")) {                    Plotly.newPlot(                        \"0af14dd5-8830-4bdd-8164-f94a206239a9\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Provider=%{x}\\u003cbr\\u003eAvg Inference time (ms)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"texttemplate\":\"%{y:.2s}\",\"x\":[\"PyTorch GPU\",\"Quant GPU\"],\"xaxis\":\"x\",\"y\":[444.1866643499952,1198.109270270029],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Provider\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Avg Inference time (ms)\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Average inference time (ms) for each provider\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0af14dd5-8830-4bdd-8164-f94a206239a9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "# Compute average inference time\n",
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "\n",
        "fig = px.bar(x=time_results.keys(), y=time_results.values(),\n",
        "             title=\"Average inference time (ms) for each provider\",\n",
        "             labels={'x':'Provider', 'y':'Avg Inference time (ms)'},\n",
        "             text_auto='.2s')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgE7aBCb2l32"
      },
      "source": [
        "Calculate latency and throughput metrics for the 3 benchmark sets and put them into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fOsjKpmlK_PQ"
      },
      "outputs": [],
      "source": [
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "time_results_std = {k: np.std(v.model_inference_time) * 1000 for k, v in results.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Uao1a7ALLAP6"
      },
      "outputs": [],
      "source": [
        "perf_results = {}\n",
        "for k, v in results.items():\n",
        "  latency_list = v.model_inference_time\n",
        "  latency_50 = np.percentile(latency_list, 50) * 1e3\n",
        "  latency_75 = np.percentile(latency_list, 75) * 1e3\n",
        "  latency_90 = np.percentile(latency_list, 90) * 1e3\n",
        "  latency_95 = np.percentile(latency_list, 95) * 1e3\n",
        "  latency_99 = np.percentile(latency_list, 99) * 1e3\n",
        "\n",
        "  average_latency = np.mean(v.model_inference_time) * 1e3\n",
        "  throughput = 1 * (1000 / average_latency)\n",
        "\n",
        "  perf_results[k] = (\n",
        "        average_latency,\n",
        "        latency_50,\n",
        "        latency_75,\n",
        "        latency_90,\n",
        "        latency_95,\n",
        "        latency_99,\n",
        "        throughput,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "P5AMItfgMQMi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "0f496c4d-d7e1-4349-9be4-d941c51f96cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      PyTorch GPU    Quant GPU\n",
              "Average_latency (ms)   444.186664  1198.109270\n",
              "Latency_P50            429.924856  1146.817116\n",
              "Latency_P75            443.844957  1170.920498\n",
              "Latency_P90            505.177847  1411.364848\n",
              "Latency_P95            518.523323  1489.098394\n",
              "Latency_P99            547.661634  1493.464796\n",
              "Throughput               2.251306     0.834648"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91ef1b64-d8ad-4dfd-a9d3-87efc0addcd1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PyTorch GPU</th>\n",
              "      <th>Quant GPU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Average_latency (ms)</th>\n",
              "      <td>444.186664</td>\n",
              "      <td>1198.109270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P50</th>\n",
              "      <td>429.924856</td>\n",
              "      <td>1146.817116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P75</th>\n",
              "      <td>443.844957</td>\n",
              "      <td>1170.920498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P90</th>\n",
              "      <td>505.177847</td>\n",
              "      <td>1411.364848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P95</th>\n",
              "      <td>518.523323</td>\n",
              "      <td>1489.098394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P99</th>\n",
              "      <td>547.661634</td>\n",
              "      <td>1493.464796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Throughput</th>\n",
              "      <td>2.251306</td>\n",
              "      <td>0.834648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91ef1b64-d8ad-4dfd-a9d3-87efc0addcd1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-91ef1b64-d8ad-4dfd-a9d3-87efc0addcd1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-91ef1b64-d8ad-4dfd-a9d3-87efc0addcd1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-59ee5c6c-e66b-4ca7-81c6-50c58fba41e6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-59ee5c6c-e66b-4ca7-81c6-50c58fba41e6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-59ee5c6c-e66b-4ca7-81c6-50c58fba41e6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4d507558-3f9a-4075-920c-a6f60cf22308\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('perf_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4d507558-3f9a-4075-920c-a6f60cf22308 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('perf_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "perf_df",
              "summary": "{\n  \"name\": \"perf_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"PyTorch GPU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 186.49870254572912,\n        \"min\": 2.2513057690810223,\n        \"max\": 547.661634069723,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          444.1866643499952,\n          429.92485550007586,\n          547.661634069723\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quant GPU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 519.9094481508955,\n        \"min\": 0.8346484121390871,\n        \"max\": 1493.464795760001,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1198.109270270029,\n          1146.8171159999656,\n          1493.464795760001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "index_labels = ['Average_latency (ms)', 'Latency_P50', 'Latency_P75',\n",
        "                'Latency_P90', 'Latency_P95', 'Latency_P99', 'Throughput']\n",
        "perf_df = pd.DataFrame(data=perf_results, index=index_labels)\n",
        "perf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOFMzIya283h"
      },
      "source": [
        "Visually compare inference durations across benchmarks for the 2 different versions of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "22IhNSXGPcfC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "d010fbe6-27cf-4e2d-d971-331209358a9d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"93984040-a056-4d27-9153-66ded67f9d68\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"93984040-a056-4d27-9153-66ded67f9d68\")) {                    Plotly.newPlot(                        \"93984040-a056-4d27-9153-66ded67f9d68\",                        [{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"hovertemplate\":\"Provider=%{x}\\u003cbr\\u003eInference durations (ms)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\"},\"name\":\"\",\"notched\":false,\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[413.498810999954,434.5383770000808,419.83290100006343,433.1017280001106,425.15110800013645,422.10444900001676,426.3975329999994,423.02829499976724,437.0304720000604,420.8703169997534,509.27723699987837,487.7025189998676,481.46623799993904,547.2654709997187,418.4947129997454,426.1155899998812,421.1748399998214,433.5788479997973,417.54549600000246,421.6321099997913,428.30597700003636,426.7551440002535,438.6658290000014,425.293827999667,437.90315700016436,418.3526889996756,451.48082300011083,425.33451400004196,423.56129900008455,438.9338319997478,423.0275290001373,425.94924100012577,423.1360210001185,423.0616739996549,444.8741380001593,426.2204690003273,445.84101300006296,517.3108720000528,498.5894120000012,494.5507809998162,541.5598879999379,424.60853200009296,419.350516999657,429.19330599988825,418.3383370000229,448.25570000011794,425.80998600033126,428.7754040001346,428.770627999711,426.62590299960357,442.76996899998267,430.26110800019524,435.538403999999,427.8962439998395,434.15996799967616,429.35027400017134,425.7299719997718,440.8908059999703,430.8226860002833,428.8458930000161,431.97394799972244,432.58779899997535,447.46006300010777,427.67498900002465,545.1251740000771,494.12702800009356,508.0868139998529,542.504492000262,424.7062409999671,432.6261000001068,423.43649500026004,445.4344129999299,426.4346499999192,442.01408899971284,431.0898720000296,428.9771430003384,433.4167619999789,428.57755800014274,446.30926300033025,426.52676299985615,445.1617999998234,429.5886029999565,429.13023699975383,431.92280099992786,428.792618999978,444.504235000295,431.08299200002875,431.434363000335,431.9833770000514,427.085143000113,443.62519700007397,505.10050000002593,507.7728119999847,505.8739740002238,586.8817780001336,418.2228380000197,442.76995399968655,426.9606860002568,428.34165700014637,428.8323929999933,1117.5616410000657,1135.6006370001523,1247.908370999994,1489.0054369998325,1211.4243000000897,1140.2378009997847,1148.0975179997586,1149.0132480003012,1156.4856009999858,1149.491795999893,1136.0035120001157,1154.2194769999696,1263.7952179998138,1493.3998559999964,1162.8132339997137,1141.8393870003456,1167.9445439999654,1158.8183600001685,1144.2511650002416,1140.9527540004092,1136.6259369997351,1134.2003380000278,1308.8821639998969,1493.2925230000365,1149.7202330001528,1141.2908560000687,1147.2706099998504,1137.333604000105,1156.6279799999393,1144.3983979997938,1154.6124420001433,1143.6892679998891,1276.2051720001182,1488.6460769998848,1148.3268819997647,1140.613282999766,1147.1243949999916,1125.6671559999631,1138.4276620001401,1143.388529999811,1141.9356050000715,1132.6963310002611,1287.4077070000567,1499.8938320004527,1159.9541749997115,1132.8326460002245,1153.9155479999863,1141.2539359998846,1129.3201029998272,1137.4070259998916,1148.8228720004372,1146.1084140000821,1267.3014859997238,1490.8645829996203,1178.3378949999133,1168.4480320000148,1149.2172700000083,1146.2234929999795,1143.692179000027,1135.5987010001627,1136.1269519993584,1141.8006869998862,1282.6741380004023,1493.283978999898,1149.3478990005315,1137.3328390000097,1142.4244180007008,1146.5098369999396,1146.1578389998976,1137.8660569998829,1141.0039929996856,1157.058920000054,1333.7367100002666,1482.6336469996022,1135.7776009999725,1146.178836999752,1148.1701650000105,1111.381050000091,1138.454597999953,1147.4535369998193,1136.7594280000048,1137.68769500075,1257.5960240001223,1481.5918779995627,1290.9381669996947,1458.3437960000083,1153.6931660002665,1152.5619639996876,1140.108023000721,1142.8310890005378,1137.731709000036,1140.2882140000656,1379.8237370001516,1406.1449649998394,1141.7014630005724,1140.7493780006916,1140.968857000189,1182.1882479998749,1134.4047350003166,1143.0015870000716],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Provider\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Inference durations (ms)\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('93984040-a056-4d27-9153-66ded67f9d68');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "results_df = pd.DataFrame(columns=['Provider', 'Inference_time'])\n",
        "for k, v in results.items():\n",
        "  for i in range(len(v.model_inference_time)):\n",
        "    results_df.loc[len(results_df.index)] = [k, v.model_inference_time[i] * 1e3]\n",
        "\n",
        "fig = px.box(results_df, x=\"Provider\", y=\"Inference_time\",\n",
        "             points=\"all\",\n",
        "             labels={'Provider':'Provider', 'Inference_time':'Inference durations (ms)'})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-bit Quantization"
      ],
      "metadata": {
        "id": "DvULKGdJPAgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do 4-bit quantization of the original model using Bitsandbytes library and save it to disk."
      ],
      "metadata": {
        "id": "8jD0qv7TL_ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_save_dir)\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(checkpoint_save_dir,\n",
        "                                        quantization_config=quantization_config)\n",
        "quantized_model.eval()"
      ],
      "metadata": {
        "id": "OM-FGClDL7kQ",
        "outputId": "0f31c319-115d-4ec2-cdc7-6a97ab795072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "9266d938932a4065b7ed2393359292fa",
            "28db246cb2404ee1b79629f7fc9fb34f",
            "f672ad93d8054e03a1d0a516e84f3e2e",
            "7081b8ca0ad7480d8f7da84faa0a55fa",
            "177316a9b043405c93e0e9ec79159c62",
            "b2f8d9e3bb55408f8eac93227409d622",
            "979be8a7162543b991d865d190707be8",
            "d4c351f313c749b8b381ad0dfa7b02b5",
            "3834f7c658c84146b9d480c0fbdbc996",
            "57fe93fc3bd440e5acc04aa3f11adcb3",
            "54652caf5e5a457897b0292848341e2b"
          ]
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9266d938932a4065b7ed2393359292fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2Attention(\n",
              "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=True)\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear4bit(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear4bit(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in the following cell is just to verify that model and tokenizer have been downloaded properly. You can skip its execution."
      ],
      "metadata": {
        "id": "l48Rq91sMLUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = quantized_model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "KQzPGX4xMLw9",
        "outputId": "91e301ab-7bbb-421a-dd55-e9c1f138a7ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def fibonacci(n):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.save_pretrained('local-4bit-checkpoint')"
      ],
      "metadata": {
        "id": "X7voHbK8MQga"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_4bit_save_dir = 'local-4bit-checkpoint'\n",
        "\n",
        "# Load the quantized model from the specified directory\n",
        "quantized_model_loaded = AutoModelForCausalLM.from_pretrained(checkpoint_4bit_save_dir)\n",
        "quantized_model_loaded.eval()"
      ],
      "metadata": {
        "id": "V2O0C97AMQ7B",
        "outputId": "5df2c569-fe7f-4d04-d59c-18af48d46b11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2Attention(\n",
              "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=True)\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear4bit(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear4bit(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup the pipeline for inference with the quantized model."
      ],
      "metadata": {
        "id": "6ES6h5dPMb5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "            model=quantized_model_loaded,\n",
        "            tokenizer=tokenizer,\n",
        "            do_sample=True,\n",
        "            use_cache=True,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            max_length=14,\n",
        ")"
      ],
      "metadata": {
        "id": "T2FXM_HPMcWV",
        "outputId": "483b0068-836e-495f-babb-adbe5cb886ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that the pipeline works as expected."
      ],
      "metadata": {
        "id": "7uvOlj5DMiud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe(prompt)\n",
        "result"
      ],
      "metadata": {
        "id": "zGKHJY9lMf60",
        "outputId": "8378ea38-c0c7-40e9-da2a-df4d5bfc662e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'def fibonacci(n):\\n    if n == 0:\\n       '}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the benchmark on the quantized model."
      ],
      "metadata": {
        "id": "mXs5CQ47Mpra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROVIDERS = {\n",
        "    (\"ort\", \"Quant GPU\"),\n",
        "}\n",
        "results = benchmark_inference(PROVIDERS, pipe, prompt, results)"
      ],
      "metadata": {
        "id": "ifZ5MCsFMq9t",
        "outputId": "a95e9f33-8be8-456d-d904-c6d5d22968d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  10%|█         | 1/10 [00:01<00:17,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  20%|██        | 2/10 [00:03<00:12,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  30%|███       | 3/10 [00:04<00:08,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  40%|████      | 4/10 [00:05<00:06,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  50%|█████     | 5/10 [00:05<00:05,  1.03s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  60%|██████    | 6/10 [00:07<00:04,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  70%|███████   | 7/10 [00:08<00:03,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  80%|████████  | 8/10 [00:09<00:02,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  90%|█████████ | 9/10 [00:10<00:01,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n",
            "Tracking inference time (Quant GPU):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   1%|          | 1/100 [00:00<01:36,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   2%|▏         | 2/100 [00:01<01:31,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   3%|▎         | 3/100 [00:02<01:30,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   4%|▍         | 4/100 [00:04<01:47,  1.12s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   5%|▌         | 5/100 [00:06<02:10,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   6%|▌         | 6/100 [00:07<02:20,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   7%|▋         | 7/100 [00:08<02:02,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   8%|▊         | 8/100 [00:09<01:51,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   9%|▉         | 9/100 [00:11<02:09,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  10%|█         | 10/100 [00:12<01:59,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  11%|█         | 11/100 [00:13<01:53,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  12%|█▏        | 12/100 [00:14<01:42,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  13%|█▎        | 13/100 [00:15<01:27,  1.00s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  14%|█▍        | 14/100 [00:16<01:15,  1.13it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  15%|█▌        | 15/100 [00:16<01:08,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  16%|█▌        | 16/100 [00:17<01:02,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  17%|█▋        | 17/100 [00:17<00:58,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  18%|█▊        | 18/100 [00:18<00:55,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  19%|█▉        | 19/100 [00:19<00:53,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  20%|██        | 20/100 [00:19<00:52,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  21%|██        | 21/100 [00:20<00:50,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  22%|██▏       | 22/100 [00:20<00:49,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  23%|██▎       | 23/100 [00:21<00:48,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  24%|██▍       | 24/100 [00:22<00:47,  1.59it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  25%|██▌       | 25/100 [00:23<00:50,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  26%|██▌       | 26/100 [00:23<00:51,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  27%|██▋       | 27/100 [00:24<00:51,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  28%|██▊       | 28/100 [00:25<00:48,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  29%|██▉       | 29/100 [00:25<00:47,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  30%|███       | 30/100 [00:26<00:44,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  31%|███       | 31/100 [00:26<00:43,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  32%|███▏      | 32/100 [00:27<00:42,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  33%|███▎      | 33/100 [00:28<00:41,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  34%|███▍      | 34/100 [00:28<00:40,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  35%|███▌      | 35/100 [00:29<00:39,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  36%|███▌      | 36/100 [00:29<00:39,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  37%|███▋      | 37/100 [00:30<00:38,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  38%|███▊      | 38/100 [00:31<00:37,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  39%|███▉      | 39/100 [00:31<00:37,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  40%|████      | 40/100 [00:32<00:36,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  41%|████      | 41/100 [00:33<00:36,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  42%|████▏     | 42/100 [00:33<00:35,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  43%|████▎     | 43/100 [00:34<00:34,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  44%|████▍     | 44/100 [00:34<00:36,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  45%|████▌     | 45/100 [00:35<00:37,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  46%|████▌     | 46/100 [00:36<00:37,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  47%|████▋     | 47/100 [00:37<00:35,  1.48it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  48%|████▊     | 48/100 [00:37<00:34,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  49%|████▉     | 49/100 [00:38<00:32,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  50%|█████     | 50/100 [00:38<00:31,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  51%|█████     | 51/100 [00:39<00:30,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  52%|█████▏    | 52/100 [00:40<00:29,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  53%|█████▎    | 53/100 [00:40<00:28,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  54%|█████▍    | 54/100 [00:41<00:28,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  55%|█████▌    | 55/100 [00:41<00:27,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  56%|█████▌    | 56/100 [00:42<00:26,  1.64it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  57%|█████▋    | 57/100 [00:43<00:26,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  58%|█████▊    | 58/100 [00:43<00:25,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  59%|█████▉    | 59/100 [00:44<00:24,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  60%|██████    | 60/100 [00:44<00:24,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  61%|██████    | 61/100 [00:45<00:23,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  62%|██████▏   | 62/100 [00:46<00:22,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  63%|██████▎   | 63/100 [00:46<00:23,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  64%|██████▍   | 64/100 [00:47<00:23,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  65%|██████▌   | 65/100 [00:48<00:24,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  66%|██████▌   | 66/100 [00:48<00:22,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  67%|██████▋   | 67/100 [00:49<00:21,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  68%|██████▊   | 68/100 [00:50<00:20,  1.58it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  69%|██████▉   | 69/100 [00:50<00:19,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  70%|███████   | 70/100 [00:51<00:18,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  71%|███████   | 71/100 [00:51<00:17,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  72%|███████▏  | 72/100 [00:52<00:16,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  73%|███████▎  | 73/100 [00:53<00:16,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  74%|███████▍  | 74/100 [00:53<00:15,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  75%|███████▌  | 75/100 [00:54<00:15,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  76%|███████▌  | 76/100 [00:54<00:14,  1.66it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  77%|███████▋  | 77/100 [00:55<00:13,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  78%|███████▊  | 78/100 [00:56<00:13,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  79%|███████▉  | 79/100 [00:56<00:12,  1.67it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  80%|████████  | 80/100 [00:57<00:11,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  81%|████████  | 81/100 [00:57<00:11,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  82%|████████▏ | 82/100 [00:58<00:11,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  83%|████████▎ | 83/100 [00:59<00:11,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  84%|████████▍ | 84/100 [01:00<00:11,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  85%|████████▌ | 85/100 [01:00<00:10,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  86%|████████▌ | 86/100 [01:01<00:09,  1.53it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  87%|████████▋ | 87/100 [01:01<00:08,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  88%|████████▊ | 88/100 [01:02<00:07,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  89%|████████▉ | 89/100 [01:03<00:06,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  90%|█████████ | 90/100 [01:05<00:09,  1.02it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  91%|█████████ | 91/100 [01:06<00:09,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  92%|█████████▏| 92/100 [01:08<00:10,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  93%|█████████▎| 93/100 [01:09<00:09,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  94%|█████████▍| 94/100 [01:11<00:09,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  95%|█████████▌| 95/100 [01:14<00:09,  1.85s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  96%|█████████▌| 96/100 [01:15<00:06,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  97%|█████████▋| 97/100 [01:15<00:03,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  98%|█████████▊| 98/100 [01:16<00:02,  1.09s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  99%|█████████▉| 99/100 [01:16<00:00,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU): 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results of the Benchmarks"
      ],
      "metadata": {
        "id": "igxjf2_kNLdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually compare the average inference times across benchmarks for the 2 different versions of the model."
      ],
      "metadata": {
        "id": "rJ8QTSoGNL71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "# Compute average inference time\n",
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "\n",
        "fig = px.bar(x=time_results.keys(), y=time_results.values(),\n",
        "             title=\"Average inference time (ms) for each provider\",\n",
        "             labels={'x':'Provider', 'y':'Avg Inference time (ms)'},\n",
        "             text_auto='.2s')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "CtFE3wbWNPmw",
        "outputId": "60eae5a3-b285-42bc-f04a-28183f7b9677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"24da7d27-d987-4e93-83f2-8ee44eecbd13\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"24da7d27-d987-4e93-83f2-8ee44eecbd13\")) {                    Plotly.newPlot(                        \"24da7d27-d987-4e93-83f2-8ee44eecbd13\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Provider=%{x}\\u003cbr\\u003eAvg Inference time (ms)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"texttemplate\":\"%{y:.2s}\",\"x\":[\"PyTorch GPU\",\"Quant GPU\"],\"xaxis\":\"x\",\"y\":[446.2493440899834,773.8063646799674],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Provider\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Avg Inference time (ms)\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Average inference time (ms) for each provider\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('24da7d27-d987-4e93-83f2-8ee44eecbd13');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate latency and throughput metrics for the 3 benchmark sets and put them into a Pandas DataFrame."
      ],
      "metadata": {
        "id": "4LwkDHfoNUk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "time_results_std = {k: np.std(v.model_inference_time) * 1000 for k, v in results.items()}"
      ],
      "metadata": {
        "id": "1hcNEjYZNVFX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perf_results = {}\n",
        "for k, v in results.items():\n",
        "  latency_list = v.model_inference_time\n",
        "  latency_50 = np.percentile(latency_list, 50) * 1e3\n",
        "  latency_75 = np.percentile(latency_list, 75) * 1e3\n",
        "  latency_90 = np.percentile(latency_list, 90) * 1e3\n",
        "  latency_95 = np.percentile(latency_list, 95) * 1e3\n",
        "  latency_99 = np.percentile(latency_list, 99) * 1e3\n",
        "\n",
        "  average_latency = np.mean(v.model_inference_time) * 1e3\n",
        "  throughput = 1 * (1000 / average_latency)\n",
        "\n",
        "  perf_results[k] = (\n",
        "        average_latency,\n",
        "        latency_50,\n",
        "        latency_75,\n",
        "        latency_90,\n",
        "        latency_95,\n",
        "        latency_99,\n",
        "        throughput,\n",
        "    )"
      ],
      "metadata": {
        "id": "qR6GpT7DNao-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "index_labels = ['Average_latency (ms)', 'Latency_P50', 'Latency_P75',\n",
        "                'Latency_P90', 'Latency_P95', 'Latency_P99', 'Throughput']\n",
        "perf_df = pd.DataFrame(data=perf_results, index=index_labels)\n",
        "perf_df"
      ],
      "metadata": {
        "id": "0gNKzNvwNc2i",
        "outputId": "be5761e7-8de8-4c20-f444-256dd68391b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      PyTorch GPU    Quant GPU\n",
              "Average_latency (ms)   446.249344   773.806365\n",
              "Latency_P50            436.388582   611.183692\n",
              "Latency_P75            448.030915   734.996651\n",
              "Latency_P90            497.910668  1155.086453\n",
              "Latency_P95            512.421678  1758.680258\n",
              "Latency_P99            531.599582  2177.271080\n",
              "Throughput               2.240900     1.292313"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e5e263e1-d6e9-4130-97f0-aff36d6d4baf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PyTorch GPU</th>\n",
              "      <th>Quant GPU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Average_latency (ms)</th>\n",
              "      <td>446.249344</td>\n",
              "      <td>773.806365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P50</th>\n",
              "      <td>436.388582</td>\n",
              "      <td>611.183692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P75</th>\n",
              "      <td>448.030915</td>\n",
              "      <td>734.996651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P90</th>\n",
              "      <td>497.910668</td>\n",
              "      <td>1155.086453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P95</th>\n",
              "      <td>512.421678</td>\n",
              "      <td>1758.680258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P99</th>\n",
              "      <td>531.599582</td>\n",
              "      <td>2177.271080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Throughput</th>\n",
              "      <td>2.240900</td>\n",
              "      <td>1.292313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5e263e1-d6e9-4130-97f0-aff36d6d4baf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e5e263e1-d6e9-4130-97f0-aff36d6d4baf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e5e263e1-d6e9-4130-97f0-aff36d6d4baf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-44e72335-e2b9-479b-bc1a-a5fb03e645e8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44e72335-e2b9-479b-bc1a-a5fb03e645e8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-44e72335-e2b9-479b-bc1a-a5fb03e645e8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_456aeac0-aaa6-4570-9a61-8a8f3aa72c25\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('perf_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_456aeac0-aaa6-4570-9a61-8a8f3aa72c25 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('perf_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "perf_df",
              "summary": "{\n  \"name\": \"perf_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"PyTorch GPU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 183.81412796223563,\n        \"min\": 2.2408996522768136,\n        \"max\": 531.5995823897995,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          446.2493440899834,\n          436.3885819998359,\n          531.5995823897995\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quant GPU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 735.88500728757,\n        \"min\": 1.2923129682625218,\n        \"max\": 2177.2710802002453,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          773.8063646799674,\n          611.1836924992531,\n          2177.2710802002453\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually compare inference durations across benchmarks for the 2 different versions of the model."
      ],
      "metadata": {
        "id": "9FomlQ2RNYCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(columns=['Provider', 'Inference_time'])\n",
        "for k, v in results.items():\n",
        "  for i in range(len(v.model_inference_time)):\n",
        "    results_df.loc[len(results_df.index)] = [k, v.model_inference_time[i] * 1e3]\n",
        "\n",
        "fig = px.box(results_df, x=\"Provider\", y=\"Inference_time\",\n",
        "             points=\"all\",\n",
        "             labels={'Provider':'Provider', 'Inference_time':'Inference durations (ms)'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "JNn3C8SeNfeh",
        "outputId": "f7655610-8f83-4584-b882-3ec19f59e357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"3e45fd12-cb13-4e01-a5df-d8a2c8c48a25\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3e45fd12-cb13-4e01-a5df-d8a2c8c48a25\")) {                    Plotly.newPlot(                        \"3e45fd12-cb13-4e01-a5df-d8a2c8c48a25\",                        [{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"hovertemplate\":\"Provider=%{x}\\u003cbr\\u003eInference durations (ms)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\"},\"name\":\"\",\"notched\":false,\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[430.348595000396,436.8996499997593,420.4068589997405,421.2314639999022,427.5383980002516,426.7594470002223,442.25871799972083,424.20843799936847,419.33888099993055,429.6360600001208,426.0354620000726,439.3890890005423,429.97256299986475,437.32382000052894,423.99313500027347,425.65333699985786,438.15655699927447,421.0847690001174,511.5424569994502,494.8293389998071,497.69060000016907,528.2331530006559,454.68586900005903,425.11428099987825,424.7495620002155,432.9962620004153,426.55819700030406,448.5808649997125,424.5216689996596,435.8404240001619,426.4676570001029,429.65463700056716,443.35857100031717,431.2670409999555,449.790820999624,425.2687399994102,427.68563899971923,456.42691900047794,428.31808000028104,449.4248900000457,426.3870169997972,448.0401710006845,425.18507599925215,431.9846990001679,431.68246200002613,529.42596199955,499.8912789997121,486.73325100025977,559.2460640000354,433.0128589999731,448.5760219995427,426.6697610000847,429.2242069996064,432.21379599981447,434.4383690004179,448.02782900023885,429.2536430002656,441.11337899994396,429.4156079995446,437.03368899969064,431.3792700004342,439.3603919997986,444.25930800025526,430.98168400047143,446.548356000676,428.8228489995163,428.2434579999972,437.24747499982186,440.4561179999291,451.95727899954363,442.977171000166,442.5939449993166,511.5894950004076,494.0316239999447,528.3555810001417,531.3203249997969,439.3989310001416,433.79697899945313,441.9190159997015,431.14097000034235,445.31789100074093,433.99420799960353,434.6094379998249,447.6150279997455,438.2390259997919,451.70014900031674,433.07454800014966,442.81977099944925,425.73072699997283,434.25627400029043,441.1198459993102,435.8775139999125,456.7815909995261,432.50923699997657,448.47988999936206,423.44884400063165,437.49338399993576,443.57226899956004,510.40939500035165,510.7090950004931,976.5296490004403,905.3376920001028,934.8875270006829,1409.8880799992912,1820.4299039989564,1720.9168560002581,938.1515200002468,997.178603998691,1886.6976750014146,1111.2974690004194,1146.1475469986908,931.1405789994751,614.0707940012362,605.5228149998584,617.8931039994495,614.4850880009471,611.371834998863,617.8513409995503,604.0167080009269,653.1137000001763,610.9163830005855,610.9955499996431,617.2261249994335,624.429687999509,775.2417949996016,741.9961559990043,731.6936179995537,596.1429049984872,643.4038410006906,587.1597130008013,619.2552690008597,595.6963179996819,610.1275500004704,613.2107049998012,594.3353080001543,627.6555399999779,601.9884850011294,602.4719090000872,619.2422720014292,596.3926839995111,619.3641790014226,596.2289419985609,623.6158960000466,755.6135670001822,710.849983001026,732.6634830005787,644.8760359999142,613.9480129986623,595.3445750001265,590.1091300002008,610.8976719988277,597.3853779996716,610.099365001588,606.4782379999087,605.0662330017076,608.7391579985706,595.9690550007508,600.8310699999129,605.8213360010996,599.2559169990272,604.3369180006266,593.1729399999313,757.5847690004593,696.5815880012087,755.2062770009798,591.5925249992142,602.520888998697,599.9306960002286,614.9392879997322,589.8337679991528,605.1517629985028,585.2400710009533,588.1776419992093,606.0717710006429,602.8160659989226,603.1780869998329,587.5487269986479,591.7695309999544,602.950028000123,588.0111360002047,632.1665179984848,716.86716899967,699.0540079987113,808.740250999108,597.7758879998873,605.7192810003471,604.4974550004554,586.283534001268,599.7148669994203,1820.4110010010481,1235.5366040010267,1755.4312720003509,1423.5179900006187,2174.2143310002575,2479.8892509988946,1060.3297919988108,593.9143700015848,585.5202609982371,598.2353939998575,576.5352239995991],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Provider\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Inference durations (ms)\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3e45fd12-cb13-4e01-a5df-d8a2c8c48a25');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c5fd1c5f4f54f508397dcdbb6d567ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f1bd9bdddb14bbbbb97100f88c65b6f",
              "IPY_MODEL_ee6567c55f394b66a5e8553858793040",
              "IPY_MODEL_a368ca9de61644b9b516c093c329ca5f"
            ],
            "layout": "IPY_MODEL_4a7846b911c146c4a691331087133f45"
          }
        },
        "3f1bd9bdddb14bbbbb97100f88c65b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48545f9e9a1464b99ac2bdbd688dbf8",
            "placeholder": "​",
            "style": "IPY_MODEL_46d1ae5fa72545998e6cbac5ca869189",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ee6567c55f394b66a5e8553858793040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_908e088d3ea34c71802236ed8a9b59a5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_913dc9a6117b4b8394306f8bb8b9b25c",
            "value": 2
          }
        },
        "a368ca9de61644b9b516c093c329ca5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b18128b83f1418e8e24ff67f176dc58",
            "placeholder": "​",
            "style": "IPY_MODEL_315c08d5b8f44bc8b66ddd4ddd50a76c",
            "value": " 2/2 [00:35&lt;00:00, 15.52s/it]"
          }
        },
        "4a7846b911c146c4a691331087133f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48545f9e9a1464b99ac2bdbd688dbf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46d1ae5fa72545998e6cbac5ca869189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "908e088d3ea34c71802236ed8a9b59a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913dc9a6117b4b8394306f8bb8b9b25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b18128b83f1418e8e24ff67f176dc58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315c08d5b8f44bc8b66ddd4ddd50a76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9266d938932a4065b7ed2393359292fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28db246cb2404ee1b79629f7fc9fb34f",
              "IPY_MODEL_f672ad93d8054e03a1d0a516e84f3e2e",
              "IPY_MODEL_7081b8ca0ad7480d8f7da84faa0a55fa"
            ],
            "layout": "IPY_MODEL_177316a9b043405c93e0e9ec79159c62"
          }
        },
        "28db246cb2404ee1b79629f7fc9fb34f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f8d9e3bb55408f8eac93227409d622",
            "placeholder": "​",
            "style": "IPY_MODEL_979be8a7162543b991d865d190707be8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f672ad93d8054e03a1d0a516e84f3e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4c351f313c749b8b381ad0dfa7b02b5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3834f7c658c84146b9d480c0fbdbc996",
            "value": 2
          }
        },
        "7081b8ca0ad7480d8f7da84faa0a55fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57fe93fc3bd440e5acc04aa3f11adcb3",
            "placeholder": "​",
            "style": "IPY_MODEL_54652caf5e5a457897b0292848341e2b",
            "value": " 2/2 [00:37&lt;00:00, 16.37s/it]"
          }
        },
        "177316a9b043405c93e0e9ec79159c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f8d9e3bb55408f8eac93227409d622": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979be8a7162543b991d865d190707be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4c351f313c749b8b381ad0dfa7b02b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3834f7c658c84146b9d480c0fbdbc996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57fe93fc3bd440e5acc04aa3f11adcb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54652caf5e5a457897b0292848341e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}