{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/domain-specific-small-language-models/05-generate-python-code/03_benchmark_inference_performance2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro2mCsr9wNoZ"
      },
      "source": [
        "## Benchmarking Python Code Generation with Vanilla and 8-bit Quantized StarCoder2 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code in this notebook is to benchmark inference performance (latency and throughtput) when generating Python code using a vanilla [StarCoder2](https://huggingface.co/Salesforce/codegen-350M-mono) 2B model, and after 8-bit quantization of the same model. It reuqires hardware acceleration.  "
      ],
      "metadata": {
        "id": "U2M1klT1Edcm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxm-Oc-FxRGx"
      },
      "source": [
        "Install the missing requirements in the ColabVM (only HF's Optimum for the ONNX runtime and Bitsandbytes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBniEqbYvTsI"
      },
      "outputs": [],
      "source": [
        "!pip install optimum[onnxruntime-gpu]==1.21.2\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpSD_2ERL_Q1"
      },
      "source": [
        "Upgrade the Numpy and HF's Transformers packages to the latest version. A restart of the VM is needed after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0qslPrQC5QU"
      },
      "outputs": [],
      "source": [
        "!pip install -U numpy transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qgo8j3yAEC_"
      },
      "source": [
        "### Vanilla Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3uieL3kxc-C"
      },
      "source": [
        "Download the StarCoder2-3B model (in bfloat16) and its tokenizer from the HF's Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQXJ5wJJuMUd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"bigcode/starcoder2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9JpoSzoHaOR4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545,
          "referenced_widgets": [
            "35e1968a522241e6a9a3bcc1e7e8a12a",
            "1b4cea0b2a1243eea82c4539e8990e66",
            "e535800278814319a50b11aba9a053a9",
            "4677e4d40ff54bada4f7f1dcd28768a8",
            "6395808bdce24f399e92c81818e3b9fa",
            "12289f46364a4f909acc67eebad4c4f5",
            "54062f2a4209428e85e2fb07f90dabb4",
            "3e079d28c22f4d6eaa6fc88c8c76bf97",
            "fda1d8bd1bb34e8c90b2439e49e321e5",
            "5fc3cde0400e451da0aa05b53e3380d3",
            "d2df94c0bb7647399d8252fd907c8fc5",
            "780e0a8cdb43446f98c172fcc76dd20f",
            "b2ad15b2637744b7a58adc639ff97bd2",
            "5eab0aecba9e490b80b3bc2be1da4192",
            "db1eae9c4c1d4592b482bb98973c367c",
            "e028c94e4b1344c4a58db94add2585d3",
            "afa3f4f4880d423987ef512b399904c8",
            "690615310ccb421eaa47e0c9363bff73",
            "b0b02288244d495dace7d4e320b3bdbc",
            "bc42701c1b0c41bbbb16e60a89e3fcf3",
            "4c8df86fc85c4f338b417923f3e7dd01",
            "5e0764c31fe444bc8dca5cdf96e0d8a6"
          ]
        },
        "outputId": "96510ade-ec03-46db-ce28-c06d654c8dc5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35e1968a522241e6a9a3bcc1e7e8a12a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/12.1G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "780e0a8cdb43446f98c172fcc76dd20f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2Attention(\n",
              "          (q_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear(in_features=3072, out_features=3072, bias=True)\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype=torch.bfloat16)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlUHkMBOxtGL"
      },
      "source": [
        "Set a text prompt (a Python function header) to be used across benchmarks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Euq2A50ANLW"
      },
      "outputs": [],
      "source": [
        "prompt = \"def print_hello_world():\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QIHUGmFx1EB"
      },
      "source": [
        "The code in the following cell is just to verify that model and tokenizer have been downloaded properly. You can skip its execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_AXjHFvaTO8"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqw9ViDFNGcN",
        "outputId": "772302ca-3c72-4f19-bbc2-77b9798d10ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def print_hello_world():\n",
            "    print(\"Hello World\")\n",
            "\n",
            "def print_hello_world_with_name(name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"def fibonacci(n):\"\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs)"
      ],
      "metadata": {
        "id": "LbJaAkTaNqZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVV7pxY1NtZT",
        "outputId": "b066331a-4289-4232-95de-48d3b1821dee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def fibonacci(n):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td1Nl5uUx_6v"
      },
      "source": [
        "Setup a Transformers' pipeline for inference with the vanilla model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HTBIqZ9MmF6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e091b0-8c6b-4358-ae8e-3635a5dbccda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            do_sample=True,\n",
        "            use_cache=True,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            max_length=14\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3x3oWkbyHFw"
      },
      "source": [
        "Test the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p4NxE8a1m6xE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7aee889-4151-4774-8739-02d78231722f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def fibonacci(n):\n",
            "    if n == 0:\n",
            "       \n"
          ]
        }
      ],
      "source": [
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvxNVcKjGgGS"
      },
      "source": [
        "Save the checkpoints locally, to be reused when quantizing it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DIIsBPsmudM3"
      },
      "outputs": [],
      "source": [
        "checkpoint_save_dir = 'local-pt-checkpoint'\n",
        "tokenizer.save_pretrained(checkpoint_save_dir)\n",
        "model.save_pretrained(checkpoint_save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q70kfNsKCeYx"
      },
      "source": [
        "Define some utils for benchmarking (more details about them in chapter 6 of the book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pL77v2MCEhNx"
      },
      "outputs": [],
      "source": [
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass\n",
        "from time import perf_counter\n",
        "\n",
        "@contextmanager\n",
        "def track_infer_time(time_buffer):\n",
        "    start_time = perf_counter()\n",
        "    yield\n",
        "    end_time = perf_counter()\n",
        "\n",
        "    time_buffer.append(end_time - start_time)\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkInferenceResult:\n",
        "    model_inference_time: [int]\n",
        "    optimized_model_path: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il5ctRs0yU7Y"
      },
      "source": [
        "Define a custom funtion to be reused across benchmarks with the different versions of the model under evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kROG4REwFO2d"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "\n",
        "def benchmark_inference(providers_dict, pipe, prompt, results):\n",
        "  for device, label in PROVIDERS:\n",
        "      for _ in trange(10, desc=\"Warming up\"):\n",
        "          pipe(prompt)\n",
        "\n",
        "      time_buffer = []\n",
        "      for _ in trange(100, desc=f\"Tracking inference time ({label})\"):\n",
        "        with track_infer_time(time_buffer):\n",
        "            pipe(prompt)\n",
        "\n",
        "      results[label] = BenchmarkInferenceResult(\n",
        "          time_buffer,\n",
        "          None\n",
        "      )\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9-RNdkvyfQ1"
      },
      "source": [
        "Execute the benchmarks for the StarCoder2 vanilla model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dL9m9HtcFsP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af725cb-7d29-4ce2-e1f6-45cf1e4c905d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  10%|█         | 1/10 [00:00<00:05,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  20%|██        | 2/10 [00:01<00:03,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  30%|███       | 3/10 [00:01<00:03,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  40%|████      | 4/10 [00:01<00:02,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  50%|█████     | 5/10 [00:02<00:02,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  60%|██████    | 6/10 [00:02<00:01,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  70%|███████   | 7/10 [00:03<00:01,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  80%|████████  | 8/10 [00:03<00:00,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  90%|█████████ | 9/10 [00:04<00:00,  2.28it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up: 100%|██████████| 10/10 [00:04<00:00,  2.23it/s]\n",
            "Tracking inference time (PyTorch GPU):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   1%|          | 1/100 [00:00<00:43,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   2%|▏         | 2/100 [00:00<00:43,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   3%|▎         | 3/100 [00:01<00:42,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   4%|▍         | 4/100 [00:01<00:42,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   5%|▌         | 5/100 [00:02<00:41,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   6%|▌         | 6/100 [00:02<00:41,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   7%|▋         | 7/100 [00:03<00:40,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   8%|▊         | 8/100 [00:03<00:42,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):   9%|▉         | 9/100 [00:04<00:43,  2.11it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  10%|█         | 10/100 [00:04<00:42,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  11%|█         | 11/100 [00:05<00:44,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  12%|█▏        | 12/100 [00:05<00:43,  2.02it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  13%|█▎        | 13/100 [00:06<00:41,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  14%|█▍        | 14/100 [00:06<00:40,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  15%|█▌        | 15/100 [00:06<00:39,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  16%|█▌        | 16/100 [00:07<00:38,  2.20it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  17%|█▋        | 17/100 [00:07<00:37,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  18%|█▊        | 18/100 [00:08<00:36,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  19%|█▉        | 19/100 [00:08<00:35,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  20%|██        | 20/100 [00:09<00:35,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  21%|██        | 21/100 [00:09<00:35,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  22%|██▏       | 22/100 [00:10<00:34,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  23%|██▎       | 23/100 [00:10<00:34,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  24%|██▍       | 24/100 [00:10<00:33,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  25%|██▌       | 25/100 [00:11<00:32,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  26%|██▌       | 26/100 [00:11<00:32,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  27%|██▋       | 27/100 [00:12<00:32,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  28%|██▊       | 28/100 [00:12<00:32,  2.23it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  29%|██▉       | 29/100 [00:13<00:31,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  30%|███       | 30/100 [00:13<00:31,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  31%|███       | 31/100 [00:14<00:30,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  32%|███▏      | 32/100 [00:14<00:29,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  33%|███▎      | 33/100 [00:14<00:29,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  34%|███▍      | 34/100 [00:15<00:29,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  35%|███▌      | 35/100 [00:15<00:30,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  36%|███▌      | 36/100 [00:16<00:30,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  37%|███▋      | 37/100 [00:16<00:30,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  38%|███▊      | 38/100 [00:17<00:31,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  39%|███▉      | 39/100 [00:17<00:29,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  40%|████      | 40/100 [00:18<00:28,  2.14it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  41%|████      | 41/100 [00:18<00:26,  2.19it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  42%|████▏     | 42/100 [00:19<00:26,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  43%|████▎     | 43/100 [00:19<00:25,  2.21it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  44%|████▍     | 44/100 [00:20<00:24,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  45%|████▌     | 45/100 [00:20<00:24,  2.23it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  46%|████▌     | 46/100 [00:20<00:24,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  47%|████▋     | 47/100 [00:21<00:23,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  48%|████▊     | 48/100 [00:21<00:22,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  49%|████▉     | 49/100 [00:22<00:22,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  50%|█████     | 50/100 [00:22<00:22,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  51%|█████     | 51/100 [00:23<00:21,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  52%|█████▏    | 52/100 [00:23<00:21,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  53%|█████▎    | 53/100 [00:24<00:20,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  54%|█████▍    | 54/100 [00:24<00:20,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  55%|█████▌    | 55/100 [00:24<00:19,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  56%|█████▌    | 56/100 [00:25<00:19,  2.31it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  57%|█████▋    | 57/100 [00:25<00:18,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  58%|█████▊    | 58/100 [00:26<00:18,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  59%|█████▉    | 59/100 [00:26<00:18,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  60%|██████    | 60/100 [00:27<00:17,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  61%|██████    | 61/100 [00:27<00:17,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  62%|██████▏   | 62/100 [00:28<00:17,  2.17it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  63%|██████▎   | 63/100 [00:28<00:17,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  64%|██████▍   | 64/100 [00:29<00:17,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  65%|██████▌   | 65/100 [00:29<00:17,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  66%|██████▌   | 66/100 [00:30<00:16,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  67%|██████▋   | 67/100 [00:30<00:15,  2.12it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  68%|██████▊   | 68/100 [00:30<00:14,  2.15it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  69%|██████▉   | 69/100 [00:31<00:14,  2.20it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  70%|███████   | 70/100 [00:31<00:13,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  71%|███████   | 71/100 [00:32<00:12,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  72%|███████▏  | 72/100 [00:32<00:12,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  73%|███████▎  | 73/100 [00:33<00:12,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  74%|███████▍  | 74/100 [00:33<00:11,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  75%|███████▌  | 75/100 [00:34<00:11,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  76%|███████▌  | 76/100 [00:34<00:10,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  77%|███████▋  | 77/100 [00:34<00:10,  2.26it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  78%|███████▊  | 78/100 [00:35<00:09,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  79%|███████▉  | 79/100 [00:35<00:09,  2.30it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  80%|████████  | 80/100 [00:36<00:08,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  81%|████████  | 81/100 [00:36<00:08,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  82%|████████▏ | 82/100 [00:37<00:08,  2.23it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  83%|████████▎ | 83/100 [00:37<00:07,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  84%|████████▍ | 84/100 [00:37<00:07,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  85%|████████▌ | 85/100 [00:38<00:06,  2.27it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  86%|████████▌ | 86/100 [00:38<00:06,  2.29it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  87%|████████▋ | 87/100 [00:39<00:05,  2.28it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  88%|████████▊ | 88/100 [00:39<00:05,  2.16it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  89%|████████▉ | 89/100 [00:40<00:05,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  90%|█████████ | 90/100 [00:40<00:04,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  91%|█████████ | 91/100 [00:41<00:04,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  92%|█████████▏| 92/100 [00:41<00:03,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  93%|█████████▎| 93/100 [00:42<00:03,  2.13it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  94%|█████████▍| 94/100 [00:42<00:02,  2.18it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  95%|█████████▌| 95/100 [00:43<00:02,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  96%|█████████▌| 96/100 [00:43<00:01,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  97%|█████████▋| 97/100 [00:43<00:01,  2.24it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  98%|█████████▊| 98/100 [00:44<00:00,  2.22it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU):  99%|█████████▉| 99/100 [00:44<00:00,  2.25it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (PyTorch GPU): 100%|██████████| 100/100 [00:45<00:00,  2.21it/s]\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "PROVIDERS = {\n",
        "    (\"gpu\", \"PyTorch GPU\"),\n",
        "}\n",
        "results = benchmark_inference(PROVIDERS, pipe, prompt, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKlAIq_oz81N"
      },
      "source": [
        "### 8-bit Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KESvr_wOzy7i"
      },
      "source": [
        "To prevent potential out of memory issues, let's do some VRAM and RAM cleanup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TQUAblExJWvm"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "model.cpu()\n",
        "del model\n",
        "del pipe\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nijBTdmv0V41"
      },
      "source": [
        "Let's do 8-bit quantization of the original model using Bitsandbytes library and save it to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YfM7NeIb2vqR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "026fc746ebe24a168483f061dd7cfd3b",
            "6177b79fdd6d412698efa9895ca1546f",
            "21a201b85d9a487aa78aba35fa5c23a8",
            "1c14011478484d918146bd7b3db702f5",
            "ce3e84480c7e4b6db5f72b47f090b68b",
            "cd8331cf5d8b498fb52721b452028b53",
            "30e1b250d3244fb0abdc47d0dbe22ae0",
            "8a0a80a8b27f472a8472b4e260d390e9",
            "a9fe28fdd0a0428ba06cc164844db242",
            "46e7163f339747af9ae9b4b870a7e466",
            "5b9cb205ed2f4bebbb0f6ec94ea16487"
          ]
        },
        "outputId": "c6c1150b-8002-4bf5-b971-175f7776da7b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "026fc746ebe24a168483f061dd7cfd3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2Attention(\n",
              "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear8bitLt(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear8bitLt(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_save_dir)\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(checkpoint_save_dir,\n",
        "                                        quantization_config=quantization_config)\n",
        "quantized_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW11NW8NfpUY"
      },
      "source": [
        "The code in the following cell is just to verify that model and tokenizer have been downloaded properly. You can skip its execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "evHyv8igwX64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4524ae5-1c2e-4386-c7f1-e7d889e8089a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def fibonacci(n):\n",
            "    if n == 0:\n",
            "        return 0\n",
            "    elif n == 1:\n",
            "        return\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = quantized_model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "klEhjGNPI4xk"
      },
      "outputs": [],
      "source": [
        "quantized_model.save_pretrained('local-8bit-checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kJDOQ80HKlWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cefc5a-bddd-491d-c9b9-0523b7cbd13f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Starcoder2ForCausalLM(\n",
              "  (model): Starcoder2Model(\n",
              "    (embed_tokens): Embedding(49152, 3072)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x Starcoder2DecoderLayer(\n",
              "        (self_attn): Starcoder2Attention(\n",
              "          (q_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "          (k_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (v_proj): Linear8bitLt(in_features=3072, out_features=256, bias=True)\n",
              "          (o_proj): Linear8bitLt(in_features=3072, out_features=3072, bias=True)\n",
              "        )\n",
              "        (mlp): Starcoder2MLP(\n",
              "          (c_fc): Linear8bitLt(in_features=3072, out_features=12288, bias=True)\n",
              "          (c_proj): Linear8bitLt(in_features=12288, out_features=3072, bias=True)\n",
              "          (act): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
              "    (rotary_emb): Starcoder2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3072, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "checkpoint_8bit_save_dir = 'local-8bit-checkpoint'\n",
        "\n",
        "# Load the quantized model from the specified directory\n",
        "quantized_model_loaded = AutoModelForCausalLM.from_pretrained(checkpoint_8bit_save_dir)\n",
        "quantized_model_loaded.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUosoY7N0o3C"
      },
      "source": [
        "Setup the pipeline for inference with the quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6GQypqF7Qco_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdec1a9-23f7-4440-aeb5-374c67afbd3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "            model=quantized_model_loaded,\n",
        "            tokenizer=tokenizer,\n",
        "            do_sample=True,\n",
        "            use_cache=True,\n",
        "            temperature=0.2,\n",
        "            top_p=0.95,\n",
        "            max_length=14,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW0GqH1E17dz"
      },
      "source": [
        "Verify that the pipeline works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ehBfVq_CQq28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eab7374-f533-4304-a9a6-4acc4454ed43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'def fibonacci(n):\\n    if n == 0:\\n       '}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "result = pipe(prompt)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGUi3AaYDC-I"
      },
      "source": [
        "Repeat the benchmark on the quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "l4cTBDxVIu4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c55fff-7123-4e2b-d254-848eccf1c03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up:   0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  10%|█         | 1/10 [00:01<00:10,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  20%|██        | 2/10 [00:02<00:09,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  30%|███       | 3/10 [00:03<00:08,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  40%|████      | 4/10 [00:04<00:07,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  50%|█████     | 5/10 [00:05<00:05,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  60%|██████    | 6/10 [00:07<00:04,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  70%|███████   | 7/10 [00:08<00:03,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  80%|████████  | 8/10 [00:09<00:02,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up:  90%|█████████ | 9/10 [00:11<00:01,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Warming up: 100%|██████████| 10/10 [00:12<00:00,  1.23s/it]\n",
            "Tracking inference time (Quant GPU):   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   1%|          | 1/100 [00:01<01:55,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   2%|▏         | 2/100 [00:02<01:56,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   3%|▎         | 3/100 [00:03<01:53,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   4%|▍         | 4/100 [00:04<01:52,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   5%|▌         | 5/100 [00:05<01:51,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   6%|▌         | 6/100 [00:07<01:50,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   7%|▋         | 7/100 [00:08<01:59,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   8%|▊         | 8/100 [00:09<02:00,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):   9%|▉         | 9/100 [00:11<01:55,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  10%|█         | 10/100 [00:12<01:51,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  11%|█         | 11/100 [00:13<01:48,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  12%|█▏        | 12/100 [00:14<01:46,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  13%|█▎        | 13/100 [00:15<01:44,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  14%|█▍        | 14/100 [00:16<01:42,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  15%|█▌        | 15/100 [00:18<01:40,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  16%|█▌        | 16/100 [00:19<01:43,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  17%|█▋        | 17/100 [00:21<01:50,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  18%|█▊        | 18/100 [00:22<01:45,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  19%|█▉        | 19/100 [00:23<01:41,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  20%|██        | 20/100 [00:24<01:38,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  21%|██        | 21/100 [00:25<01:35,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  22%|██▏       | 22/100 [00:26<01:33,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  23%|██▎       | 23/100 [00:28<01:32,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  24%|██▍       | 24/100 [00:29<01:30,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  25%|██▌       | 25/100 [00:30<01:28,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  26%|██▌       | 26/100 [00:31<01:31,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  27%|██▋       | 27/100 [00:33<01:36,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  28%|██▊       | 28/100 [00:34<01:32,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  29%|██▉       | 29/100 [00:35<01:28,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  30%|███       | 30/100 [00:36<01:25,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  31%|███       | 31/100 [00:38<01:23,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  32%|███▏      | 32/100 [00:39<01:21,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  33%|███▎      | 33/100 [00:40<01:19,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  34%|███▍      | 34/100 [00:41<01:17,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  35%|███▌      | 35/100 [00:42<01:16,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  36%|███▌      | 36/100 [00:44<01:19,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  37%|███▋      | 37/100 [00:45<01:22,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  38%|███▊      | 38/100 [00:46<01:19,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  39%|███▉      | 39/100 [00:47<01:17,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  40%|████      | 40/100 [00:49<01:14,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  41%|████      | 41/100 [00:50<01:11,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  42%|████▏     | 42/100 [00:51<01:09,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  43%|████▎     | 43/100 [00:52<01:07,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  44%|████▍     | 44/100 [00:53<01:06,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  45%|████▌     | 45/100 [00:55<01:05,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  46%|████▌     | 46/100 [00:56<01:08,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  47%|████▋     | 47/100 [00:57<01:09,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  48%|████▊     | 48/100 [00:59<01:06,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  49%|████▉     | 49/100 [01:00<01:03,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  50%|█████     | 50/100 [01:01<01:01,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  51%|█████     | 51/100 [01:02<00:59,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  52%|█████▏    | 52/100 [01:03<00:57,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  53%|█████▎    | 53/100 [01:04<00:55,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  54%|█████▍    | 54/100 [01:06<00:54,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  55%|█████▌    | 55/100 [01:07<00:53,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  56%|█████▌    | 56/100 [01:08<00:56,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  57%|█████▋    | 57/100 [01:10<00:55,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  58%|█████▊    | 58/100 [01:11<00:52,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  59%|█████▉    | 59/100 [01:12<00:50,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  60%|██████    | 60/100 [01:13<00:48,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  61%|██████    | 61/100 [01:14<00:46,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  62%|██████▏   | 62/100 [01:15<00:45,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  63%|██████▎   | 63/100 [01:17<00:44,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  64%|██████▍   | 64/100 [01:18<00:42,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  65%|██████▌   | 65/100 [01:19<00:42,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  66%|██████▌   | 66/100 [01:21<00:44,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  67%|██████▋   | 67/100 [01:22<00:42,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  68%|██████▊   | 68/100 [01:23<00:40,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  69%|██████▉   | 69/100 [01:24<00:38,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  70%|███████   | 70/100 [01:25<00:36,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  71%|███████   | 71/100 [01:27<00:34,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  72%|███████▏  | 72/100 [01:28<00:33,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  73%|███████▎  | 73/100 [01:29<00:32,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  74%|███████▍  | 74/100 [01:30<00:30,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  75%|███████▌  | 75/100 [01:32<00:31,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  76%|███████▌  | 76/100 [01:33<00:31,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  77%|███████▋  | 77/100 [01:34<00:29,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  78%|███████▊  | 78/100 [01:35<00:27,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  79%|███████▉  | 79/100 [01:37<00:25,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  80%|████████  | 80/100 [01:38<00:24,  1.21s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  81%|████████  | 81/100 [01:39<00:22,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  82%|████████▏ | 82/100 [01:40<00:21,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  83%|████████▎ | 83/100 [01:41<00:20,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  84%|████████▍ | 84/100 [01:42<00:18,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  85%|████████▌ | 85/100 [01:44<00:18,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  86%|████████▌ | 86/100 [01:45<00:18,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  87%|████████▋ | 87/100 [01:46<00:16,  1.28s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  88%|████████▊ | 88/100 [01:48<00:15,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  89%|████████▉ | 89/100 [01:49<00:13,  1.25s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  90%|█████████ | 90/100 [01:50<00:12,  1.23s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  91%|█████████ | 91/100 [01:51<00:10,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  92%|█████████▏| 92/100 [01:52<00:09,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  93%|█████████▎| 93/100 [01:54<00:08,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  94%|█████████▍| 94/100 [01:55<00:07,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  95%|█████████▌| 95/100 [01:56<00:06,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  96%|█████████▌| 96/100 [01:58<00:05,  1.30s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  97%|█████████▋| 97/100 [01:59<00:03,  1.26s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  98%|█████████▊| 98/100 [02:00<00:02,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU):  99%|█████████▉| 99/100 [02:01<00:01,  1.22s/it]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Tracking inference time (Quant GPU): 100%|██████████| 100/100 [02:02<00:00,  1.23s/it]\n"
          ]
        }
      ],
      "source": [
        "PROVIDERS = {\n",
        "    (\"ort\", \"Quant GPU\"),\n",
        "}\n",
        "results = benchmark_inference(PROVIDERS, pipe, prompt, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56NkSjpODGv5"
      },
      "source": [
        "### Results of the Benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ak1DWKV2Y5p"
      },
      "source": [
        "Visually compare the average inference times across benchmarks for the 2 different versions of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2QmZNhc4DMhq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "67bf8c35-0732-4c01-db26-4b1fa12cb21e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"4ba8260d-fab6-4136-81b8-ab1adca28f01\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4ba8260d-fab6-4136-81b8-ab1adca28f01\")) {                    Plotly.newPlot(                        \"4ba8260d-fab6-4136-81b8-ab1adca28f01\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Provider=%{x}\\u003cbr\\u003eAvg Inference time (ms)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"texttemplate\":\"%{y:.2s}\",\"x\":[\"PyTorch GPU\",\"Quant GPU\"],\"xaxis\":\"x\",\"y\":[451.9898047400193,1227.6480078600343],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Provider\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Avg Inference time (ms)\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Average inference time (ms) for each provider\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4ba8260d-fab6-4136-81b8-ab1adca28f01');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "# Compute average inference time\n",
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "\n",
        "fig = px.bar(x=time_results.keys(), y=time_results.values(),\n",
        "             title=\"Average inference time (ms) for each provider\",\n",
        "             labels={'x':'Provider', 'y':'Avg Inference time (ms)'},\n",
        "             text_auto='.2s')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgE7aBCb2l32"
      },
      "source": [
        "Calculate latency and throughput metrics for the 3 benchmark sets and put them into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fOsjKpmlK_PQ"
      },
      "outputs": [],
      "source": [
        "time_results = {k: np.mean(v.model_inference_time) * 1e3 for k, v in results.items()}\n",
        "time_results_std = {k: np.std(v.model_inference_time) * 1000 for k, v in results.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Uao1a7ALLAP6"
      },
      "outputs": [],
      "source": [
        "perf_results = {}\n",
        "for k, v in results.items():\n",
        "  latency_list = v.model_inference_time\n",
        "  latency_50 = np.percentile(latency_list, 50) * 1e3\n",
        "  latency_75 = np.percentile(latency_list, 75) * 1e3\n",
        "  latency_90 = np.percentile(latency_list, 90) * 1e3\n",
        "  latency_95 = np.percentile(latency_list, 95) * 1e3\n",
        "  latency_99 = np.percentile(latency_list, 99) * 1e3\n",
        "\n",
        "  average_latency = np.mean(v.model_inference_time) * 1e3\n",
        "  throughput = 1 * (1000 / average_latency)\n",
        "\n",
        "  perf_results[k] = (\n",
        "        average_latency,\n",
        "        latency_50,\n",
        "        latency_75,\n",
        "        latency_90,\n",
        "        latency_95,\n",
        "        latency_99,\n",
        "        throughput,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "P5AMItfgMQMi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "8bd397d4-b0f9-4746-ba73-8ce6420927ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      PyTorch GPU    Quant GPU\n",
              "Average_latency (ms)   451.989805  1227.648008\n",
              "Latency_P50            441.700732  1176.418144\n",
              "Latency_P75            453.452057  1210.200198\n",
              "Latency_P90            506.745469  1418.151640\n",
              "Latency_P95            523.371302  1507.512108\n",
              "Latency_P99            546.437064  1530.378054\n",
              "Throughput               2.212439     0.814566"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-055aa8fe-4c52-4e76-96cf-09c9eb156cbb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PyTorch GPU</th>\n",
              "      <th>Quant GPU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Average_latency (ms)</th>\n",
              "      <td>451.989805</td>\n",
              "      <td>1227.648008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P50</th>\n",
              "      <td>441.700732</td>\n",
              "      <td>1176.418144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P75</th>\n",
              "      <td>453.452057</td>\n",
              "      <td>1210.200198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P90</th>\n",
              "      <td>506.745469</td>\n",
              "      <td>1418.151640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P95</th>\n",
              "      <td>523.371302</td>\n",
              "      <td>1507.512108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latency_P99</th>\n",
              "      <td>546.437064</td>\n",
              "      <td>1530.378054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Throughput</th>\n",
              "      <td>2.212439</td>\n",
              "      <td>0.814566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-055aa8fe-4c52-4e76-96cf-09c9eb156cbb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-055aa8fe-4c52-4e76-96cf-09c9eb156cbb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-055aa8fe-4c52-4e76-96cf-09c9eb156cbb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-facc7900-748f-411d-8d5e-db47246ffc82\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-facc7900-748f-411d-8d5e-db47246ffc82')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-facc7900-748f-411d-8d5e-db47246ffc82 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_7c448475-955e-4401-864b-3a8090f2f2fe\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('perf_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7c448475-955e-4401-864b-3a8090f2f2fe button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('perf_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "perf_df",
              "summary": "{\n  \"name\": \"perf_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"PyTorch GPU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 187.67353002521884,\n        \"min\": 2.212439284056842,\n        \"max\": 546.4370637296362,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          451.9898047400193,\n          441.7007315000774,\n          546.4370637296362\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quant GPU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 528.4130790615548,\n        \"min\": 0.814565733498108,\n        \"max\": 1530.3780543299945,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1227.6480078600343,\n          1176.4181435000864,\n          1530.3780543299945\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "index_labels = ['Average_latency (ms)', 'Latency_P50', 'Latency_P75',\n",
        "                'Latency_P90', 'Latency_P95', 'Latency_P99', 'Throughput']\n",
        "perf_df = pd.DataFrame(data=perf_results, index=index_labels)\n",
        "perf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOFMzIya283h"
      },
      "source": [
        "Visually compare inference durations across benchmarks for the 2 different versions of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "22IhNSXGPcfC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "58481d9a-ffa0-48ec-81e4-ce617ea0ba75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d9a0b5d1-2218-4223-bd8e-7ca13b666065\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d9a0b5d1-2218-4223-bd8e-7ca13b666065\")) {                    Plotly.newPlot(                        \"d9a0b5d1-2218-4223-bd8e-7ca13b666065\",                        [{\"alignmentgroup\":\"True\",\"boxpoints\":\"all\",\"hovertemplate\":\"Provider=%{x}\\u003cbr\\u003eInference durations (ms)=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\"},\"name\":\"\",\"notched\":false,\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"PyTorch GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\",\"Quant GPU\"],\"x0\":\" \",\"xaxis\":\"x\",\"y\":[434.432169000047,445.95204899997043,441.8545319999794,449.98546700026054,426.4759719999347,437.74855799983925,424.93747800017445,508.81492100006653,506.6598870002963,483.2250069998736,534.8846930000946,494.06822499986447,450.0817449998067,448.106638000354,440.2664139997796,431.89415899996675,444.1808110000238,435.87779200015575,437.43831499978114,450.1353279997602,444.00035500029844,431.92722399999184,445.65463500020996,437.9182170000604,433.1174750000173,450.0840910000079,455.1492089999556,449.79429600016374,442.54438600000867,437.1214760003568,427.58783099998254,443.0667999999969,430.4249179999715,447.7526470000157,523.0002909997893,507.5157070000387,505.5997899999056,563.4513750001133,425.5888299999242,423.719266000262,431.6404610003701,434.5708830001058,453.17909000004875,432.9198189998351,451.4242789996388,454.7803840000597,430.290258000241,428.6168089997773,426.71220600004744,462.52349399992454,434.65708199983055,453.11922700011564,422.65522300021985,435.68570200022805,432.5672949998989,426.2769169999956,449.6330259999013,430.01540299974295,454.2709570000625,418.6696579999989,437.5369300000784,518.9991990000635,491.23604199985493,539.230322000094,530.4205160000492,447.2101090000251,436.17587200014896,444.9889880002047,429.8960330002046,443.50787100029265,430.0770449999618,437.9272900000615,447.43941300021106,433.1547470001169,459.2336040000191,441.5469310001754,435.38472899990666,427.5560549999682,427.1245090003504,451.33029700036786,440.0075440003093,464.0430499998729,434.873787000015,432.6467930000035,437.42184499978976,425.6031830000211,445.04533299959803,517.7410999999665,503.1380909999825,483.7462499999674,546.2652019996312,456.7609059999995,429.534436999802,430.62078799994197,429.8901309998655,449.7486929999468,433.7710119998519,459.0340320000905,428.8264080000772,424.0382349998981,1168.4901989992795,1193.5856350000904,1153.5338779995072,1170.2692449998722,1176.6454799999337,1179.1819829995802,1519.464560000415,1345.3042930004813,1180.6170129993916,1175.2139810005247,1171.1782940001285,1171.3731879999614,1174.8954990007405,1197.5562190000346,1165.5807639999693,1343.6127490003855,1540.2092820004327,1196.6144620000705,1173.2550790002279,1170.5512919998,1176.1908070002391,1181.0942399997657,1172.443311999814,1167.6014700005908,1160.310992999257,1374.2824849996396,1520.2056179996362,1182.8565300002083,1162.9023450004752,1168.710762000046,1170.4767500004891,1164.5001159995445,1171.208688000661,1158.366959999512,1162.3840299998847,1382.870747999732,1483.5493020000285,1225.6266990007134,1208.8903330004541,1168.6543300002086,1148.1391240004086,1184.136614999261,1168.1463299992174,1179.5997980007087,1165.404106999631,1472.365176000494,1385.443645000123,1191.0622799996418,1168.6887650002973,1188.1540909998876,1168.715654999687,1171.0867970004983,1167.2853810005108,1180.1358720003918,1200.4694699999163,1519.3743820000236,1301.662149000549,1150.17748199989,1160.3197839995119,1181.2432360002276,1169.0806679998786,1163.7714650005364,1214.1297910002322,1156.6440309998143,1329.4114970003648,1530.27874899999,1215.523003999806,1166.5345840001464,1167.5016300005154,1171.5150899999571,1172.8942119998464,1172.5653370003783,1184.2049550004958,1174.1068289993564,1369.4873740005278,1506.8877779995091,1170.6747359994552,1150.311424999927,1193.2190460001948,1169.6417859993744,1175.212979999742,1178.450729999895,1164.0724610006146,1169.1968189998079,1422.7279670003554,1417.643159000363,1219.2177979995904,1254.027054999824,1208.7795690003986,1180.669528999715,1180.7721609993678,1163.4770809996553,1185.1467540000158,1151.1999390004348,1486.4458350002678,1359.3566610006746,1165.4784249994918,1201.550137000595,1169.512255999507,1181.6097310002078],\"y0\":\" \",\"yaxis\":\"y\",\"type\":\"box\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Provider\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Inference durations (ms)\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"boxmode\":\"group\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d9a0b5d1-2218-4223-bd8e-7ca13b666065');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "results_df = pd.DataFrame(columns=['Provider', 'Inference_time'])\n",
        "for k, v in results.items():\n",
        "  for i in range(len(v.model_inference_time)):\n",
        "    results_df.loc[len(results_df.index)] = [k, v.model_inference_time[i] * 1e3]\n",
        "\n",
        "fig = px.box(results_df, x=\"Provider\", y=\"Inference_time\",\n",
        "             points=\"all\",\n",
        "             labels={'Provider':'Provider', 'Inference_time':'Inference durations (ms)'})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-bit Quantization"
      ],
      "metadata": {
        "id": "DvULKGdJPAgu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fVauEgaUPDa5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35e1968a522241e6a9a3bcc1e7e8a12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b4cea0b2a1243eea82c4539e8990e66",
              "IPY_MODEL_e535800278814319a50b11aba9a053a9",
              "IPY_MODEL_4677e4d40ff54bada4f7f1dcd28768a8"
            ],
            "layout": "IPY_MODEL_6395808bdce24f399e92c81818e3b9fa"
          }
        },
        "1b4cea0b2a1243eea82c4539e8990e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12289f46364a4f909acc67eebad4c4f5",
            "placeholder": "​",
            "style": "IPY_MODEL_54062f2a4209428e85e2fb07f90dabb4",
            "value": "config.json: 100%"
          }
        },
        "e535800278814319a50b11aba9a053a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e079d28c22f4d6eaa6fc88c8c76bf97",
            "max": 700,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fda1d8bd1bb34e8c90b2439e49e321e5",
            "value": 700
          }
        },
        "4677e4d40ff54bada4f7f1dcd28768a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc3cde0400e451da0aa05b53e3380d3",
            "placeholder": "​",
            "style": "IPY_MODEL_d2df94c0bb7647399d8252fd907c8fc5",
            "value": " 700/700 [00:00&lt;00:00, 11.4kB/s]"
          }
        },
        "6395808bdce24f399e92c81818e3b9fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12289f46364a4f909acc67eebad4c4f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54062f2a4209428e85e2fb07f90dabb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e079d28c22f4d6eaa6fc88c8c76bf97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fda1d8bd1bb34e8c90b2439e49e321e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5fc3cde0400e451da0aa05b53e3380d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2df94c0bb7647399d8252fd907c8fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "780e0a8cdb43446f98c172fcc76dd20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2ad15b2637744b7a58adc639ff97bd2",
              "IPY_MODEL_5eab0aecba9e490b80b3bc2be1da4192",
              "IPY_MODEL_db1eae9c4c1d4592b482bb98973c367c"
            ],
            "layout": "IPY_MODEL_e028c94e4b1344c4a58db94add2585d3"
          }
        },
        "b2ad15b2637744b7a58adc639ff97bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afa3f4f4880d423987ef512b399904c8",
            "placeholder": "​",
            "style": "IPY_MODEL_690615310ccb421eaa47e0c9363bff73",
            "value": "model.safetensors: 100%"
          }
        },
        "5eab0aecba9e490b80b3bc2be1da4192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0b02288244d495dace7d4e320b3bdbc",
            "max": 12121539344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc42701c1b0c41bbbb16e60a89e3fcf3",
            "value": 12121539344
          }
        },
        "db1eae9c4c1d4592b482bb98973c367c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c8df86fc85c4f338b417923f3e7dd01",
            "placeholder": "​",
            "style": "IPY_MODEL_5e0764c31fe444bc8dca5cdf96e0d8a6",
            "value": " 12.1G/12.1G [12:58&lt;00:00, 65.4MB/s]"
          }
        },
        "e028c94e4b1344c4a58db94add2585d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa3f4f4880d423987ef512b399904c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690615310ccb421eaa47e0c9363bff73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0b02288244d495dace7d4e320b3bdbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc42701c1b0c41bbbb16e60a89e3fcf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c8df86fc85c4f338b417923f3e7dd01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0764c31fe444bc8dca5cdf96e0d8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "026fc746ebe24a168483f061dd7cfd3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6177b79fdd6d412698efa9895ca1546f",
              "IPY_MODEL_21a201b85d9a487aa78aba35fa5c23a8",
              "IPY_MODEL_1c14011478484d918146bd7b3db702f5"
            ],
            "layout": "IPY_MODEL_ce3e84480c7e4b6db5f72b47f090b68b"
          }
        },
        "6177b79fdd6d412698efa9895ca1546f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd8331cf5d8b498fb52721b452028b53",
            "placeholder": "​",
            "style": "IPY_MODEL_30e1b250d3244fb0abdc47d0dbe22ae0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "21a201b85d9a487aa78aba35fa5c23a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a0a80a8b27f472a8472b4e260d390e9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9fe28fdd0a0428ba06cc164844db242",
            "value": 2
          }
        },
        "1c14011478484d918146bd7b3db702f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e7163f339747af9ae9b4b870a7e466",
            "placeholder": "​",
            "style": "IPY_MODEL_5b9cb205ed2f4bebbb0f6ec94ea16487",
            "value": " 2/2 [00:35&lt;00:00, 15.48s/it]"
          }
        },
        "ce3e84480c7e4b6db5f72b47f090b68b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd8331cf5d8b498fb52721b452028b53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30e1b250d3244fb0abdc47d0dbe22ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a0a80a8b27f472a8472b4e260d390e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9fe28fdd0a0428ba06cc164844db242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46e7163f339747af9ae9b4b870a7e466": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9cb205ed2f4bebbb0f6ec94ea16487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}