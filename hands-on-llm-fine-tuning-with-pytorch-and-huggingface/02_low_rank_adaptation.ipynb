{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/hands-on-llm-fine-tuning-with-pytorch-and-huggingface/02_low_rank_adaptation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dd522b0"
      },
      "source": [
        "## Chapter 3: Low-Rank Adaptation (LoRA)"
      ],
      "id": "5dd522b0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdd3396a"
      },
      "source": [
        "## Spoilers\n",
        "\n",
        "In this chapter, we will:\n",
        "\n",
        "- Understand what a low-rank adapter is and why it’s useful\n",
        "- Prepare the quantized model for training\n",
        "- Use `peft` to create and attach adapters to a base model\n",
        "- Discuss configuration options for targeting layers for training"
      ],
      "id": "bdd3396a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "001b25f1"
      },
      "source": [
        "## Setup"
      ],
      "id": "001b25f1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "acc2f40c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# If you're running on Colab\n",
        "!pip install datasets bitsandbytes trl"
      ],
      "id": "acc2f40c"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ca2f769f"
      },
      "outputs": [],
      "source": [
        "# If you're running on runpod.io's Jupyter Template\n",
        "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
      ],
      "id": "ca2f769f"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "556455b2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "from numpy.linalg import matrix_rank\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "id": "556455b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8836cf4f"
      },
      "source": [
        "## The Goal"
      ],
      "id": "8836cf4f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We attach adapters to the huge linear layers in an LLM to drastically reduce the number of trainable parameters. We can easily shrink the number of trainable parameters down to less than 1% of their original number. By reducing both computation (fewer gradients to compute) and memory footprint (fewer parameters tracked by the optimizer), we achieve significant efficiency gains. Keep in mind, however, that low-rank adapters are unlikely to match the performance of full-model tuning, and their effectiveness may vary depending on the base model and the task."
      ],
      "metadata": {
        "id": "-_pagCR3fAO5"
      },
      "id": "-_pagCR3fAO5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31ecd6d8"
      },
      "source": [
        "The idea behind low-rank adaptation (LoRA) is to train smaller matrices to approximate updates to large\n",
        "matrices instead of updating them directly. This approach is especially useful for fine-tuning large models, as it minimizes computational and memory overhead.\n",
        "\n",
        "To better understand its power, you need to be familiar with the basics of matrix multiplication and,\n",
        "preferably, the idea of decomposing a large matrix into two smaller ones (e.g. singular value decomposition,\n",
        "SVD).\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/matmul.png?raw=True)\n",
        "<center>Figure 3.1 - Matrix multiplication</center>\n",
        "\n",
        "\n",
        "Notice that the two small matrices have nine elements altogether, while the resulting matrix has twenty. Even\n",
        "in a tiny example such as this, the total number of elements was reduced in 55% (from twenty to nine). The\n",
        "greater the ratio between the \"remaining\" and \"facing\" dimensions, the larger the reduction.\n",
        "\n",
        "The resulting matrix, although 4 by 5, has redundancy that allows it to be represented by smaller matrices. The\n",
        "redundancy is measured by the matrix’s rank, which counts the number of independent rows or columns. In\n",
        "our example, the rank is determined by the \"facing\" dimensions of the small matrices, which is one."
      ],
      "id": "31ecd6d8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low-Rank Adaptation in a Nutshell"
      ],
      "metadata": {
        "id": "_LRmpfcZx5Wm"
      },
      "id": "_LRmpfcZx5Wm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "461eaab6"
      },
      "source": [
        "The rank shows you the real dimensions of the matrix—or basically, how much redundancy is in\n",
        "there:\n",
        "\n",
        "- A high-ranking value (that is, one close to the actual number of rows or columns) means there’s little to no\n",
        "redundancy.\n",
        "- A low-rank matrix implies there’s a lot of redundancy, and the large matrix can be easily represented by\n",
        "two smaller ones.\n",
        "\n",
        "Now, you know why it’s called a low-rank adapter. We’re using two small matrices to produce a low-rank big\n",
        "matrix (representing the updates) that matches the size of the original layer.\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/two_matrices.png?raw=True)\n",
        "<center>Figure 3.2 - Multiplying two low-rank matrices</center>"
      ],
      "id": "461eaab6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start with a base layer as if it were one of the linear layers in the attention module. We’re making it a big,\n",
        "square, one million-weight matrix:"
      ],
      "metadata": {
        "id": "thr9OWL51Fnp"
      },
      "id": "thr9OWL51Fnp"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e05c5b83",
        "outputId": "bc9c9206-4acd-42ce-b0d5-c7eb4febbeb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024, 1024]), 1048576)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "base_layer = nn.Linear(1024, 1024, bias=False)\n",
        "base_layer.weight.shape, base_layer.weight.numel()"
      ],
      "id": "e05c5b83"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55651493"
      },
      "source": [
        "We’d need one million updates to apply to these one million weights.\n",
        "\n",
        "So, let’s create two small matrices:\n",
        "\n",
        "- The first one, let’s call it layer A, has to match the input features of the base layer.\n",
        "- The second one, let’s call it layer B, has to match the output features of the base layer.\n",
        "- Since they need to be multiplied, their inner (\"facing\") dimensions must match:\n",
        "  - This is the rank of the matrix that will result from their multiplication, and you get to choose this value.\n",
        "\n",
        "\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/lowrank_matrices.png?raw=True)\n",
        "<center>Figure 3.3 - Frozen weights and low-rank matrices</center>"
      ],
      "id": "55651493"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If our rank of choice is eight, our layers look like this:"
      ],
      "metadata": {
        "id": "MTawqmCJ1vVq"
      },
      "id": "MTawqmCJ1vVq"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0305a2fc"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(11)\n",
        "\n",
        "r = 8\n",
        "layer_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
        "layer_B = nn.Linear(r, base_layer.out_features, bias=False)"
      ],
      "id": "0305a2fc"
    },
    {
      "cell_type": "code",
      "source": [
        "layer_A.weight.shape, layer_B.weight.shape"
      ],
      "metadata": {
        "id": "I9oV3PfE1-it",
        "outputId": "1dc515fa-fbc2-460b-9d25-05c712ac2d7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "I9oV3PfE1-it",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 1024]), torch.Size([1024, 8]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_A, layer_B"
      ],
      "metadata": {
        "id": "1DnugXzV2Bcw",
        "outputId": "a7ed1123-206a-44ec-c27e-0f161a357b84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1DnugXzV2Bcw",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Linear(in_features=1024, out_features=8, bias=False),\n",
              " Linear(in_features=8, out_features=1024, bias=False))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e716c5f3",
        "outputId": "46c24f49-9acc-4679-ef76-bca363429048"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8192, 8192)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "layer_A.weight.numel(), layer_B.weight.numel()"
      ],
      "id": "e716c5f3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonetheless, if we multiply them, we get a full-sized 1M-parameter matrix as a result:"
      ],
      "metadata": {
        "id": "HiBnENuL2RdQ"
      },
      "id": "HiBnENuL2RdQ"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da2e401c",
        "outputId": "b49339b2-63df-47bf-cb7d-fb19cee2881b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1024, 1024]), 1048576)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "composite = layer_B.weight @ layer_A.weight\n",
        "composite.shape, composite.numel()"
      ],
      "id": "da2e401c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we call Numpy’s matrix_rank() function, however, it will expose its true dimensions (its rank):"
      ],
      "metadata": {
        "id": "Y6kZbMFb2k2V"
      },
      "id": "Y6kZbMFb2k2V"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25358bc",
        "outputId": "c2d4a854-9427-447e-9798-85ca22a988a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(8)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "matrix_rank(composite.detach().numpy())"
      ],
      "id": "b25358bc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what you described, as it would happen inside a layer’s\n",
        "forward() method."
      ],
      "metadata": {
        "id": "wVJKN-rL4Kmk"
      },
      "id": "wVJKN-rL4Kmk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952840a7"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\text{output} = X @ (W + B @ A)^T\n",
        "$$\n",
        "<center>Equation 3.1 - Adding the resulting product to the weights</center>"
      ],
      "id": "952840a7"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ea66ae2",
        "outputId": "25410700-e4d6-4bff-adc5-4652f03b96bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4918, -0.1138, -0.8948,  ..., -0.8239,  0.1980,  0.7781]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "torch.manual_seed(19)\n",
        "\n",
        "batch = torch.randn(1, 1024)\n",
        "\n",
        "batch @ (base_layer.weight.data + layer_B.weight @ layer_A.weight).T"
      ],
      "id": "1ea66ae2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4575839d"
      },
      "source": [
        "But, thanks to the distributive property of matrix multiplication, we can split this into two passes: one forward\n",
        "pass through the base layer, and another through the resulting low-rank matrix.\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{output} = \\underbrace{X @ W^T}_{O_W} + \\underbrace{X @ (B @ A)^T}_{O_{AB}}\n",
        "$$\n",
        "<center>Equation 3.2 - Using two forward passes</center>\n",
        "\n",
        "This is very convenient and easy to implement since we can keep the original flow (on the left of the following\n",
        "figure) and compute an additional output using our two small matrices. In the end, we simply add them\n",
        "together:\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch3/forward.png?raw=True)\n",
        "<center>Figure 3.4 - Using two forward passes</center>"
      ],
      "id": "4575839d"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8e9b9c3",
        "outputId": "800e2f27-0376-488c-a82c-b23b8a110b88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.4561, -0.1368, -0.4342,  ..., -0.5319,  0.0037,  0.2741]]),\n",
              " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
              "        grad_fn=<MmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "regular_output = batch @ base_layer.weight.data.T\n",
        "additional_output = batch @ (layer_B.weight @ layer_A.weight).T\n",
        "regular_output, additional_output"
      ],
      "id": "a8e9b9c3"
    },
    {
      "cell_type": "code",
      "source": [
        "regular_output.shape, additional_output.shape"
      ],
      "metadata": {
        "id": "lU-dqc4666Oo",
        "outputId": "d312d703-0f03-4738-9f45-8f0f079433f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lU-dqc4666Oo",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1024]), torch.Size([1, 1024]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "547f73e0"
      },
      "source": [
        "As it turns out, the additional output can be split into two chained operations—that is, passing the input batch\n",
        "through layer A, and then its output through layer B.\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\text{additional} = X @ (B @ A)^T = \\underbrace{\\underbrace{(X @ A^T)}_{O_A} @ B^T}_{O_{AB}}\n",
        "$$\n",
        "<center>Equation 3.3 - Chaining the adapter’s forward passes</center>\n",
        "\n",
        "This equivalence is easy to verify in code."
      ],
      "id": "547f73e0"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4e9cd6e",
        "outputId": "57200355-a5d1-4e6c-fd2e-1855d99d18c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
              "       grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "out_A = (batch @ layer_A.weight.T)\n",
        "additional_output = out_A @ layer_B.weight.T\n",
        "additional_output"
      ],
      "id": "f4e9cd6e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, if we stick with the layers themselves, it’s as straightforward as this:"
      ],
      "metadata": {
        "id": "u4fd0XOh7fJY"
      },
      "id": "u4fd0XOh7fJY"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "267a2e77",
        "outputId": "b4eb81e9-2826-4a32-c5e4-5b0cac4ec483"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.4561, -0.1368, -0.4342,  ..., -0.5319,  0.0037,  0.2741]],\n",
              "        grad_fn=<MmBackward0>),\n",
              " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
              "        grad_fn=<MmBackward0>),\n",
              " tensor([[-0.4918, -0.1138, -0.8948,  ..., -0.8239,  0.1980,  0.7781]],\n",
              "        grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "regular_output = base_layer(batch)\n",
        "out_A = layer_A(batch)\n",
        "additional_output = layer_B(out_A)\n",
        "output = regular_output + additional_output\n",
        "regular_output, additional_output, output"
      ],
      "id": "267a2e77"
    },
    {
      "cell_type": "code",
      "source": [
        "regular_output.shape, additional_output.shape, output.shape"
      ],
      "metadata": {
        "id": "xTFWtiZ67sHe",
        "outputId": "736ed93a-f074-45a3-b21e-788f2495f06e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xTFWtiZ67sHe",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1024]), torch.Size([1, 1024]), torch.Size([1, 1024]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final output matches the first output we had obtained!\n",
        "\n",
        "There’s just one final, small detail: the additional output can be adjusted using a multiplier.\n",
        "\n",
        "The higher the\n",
        "multiplier, the greater the impact of the adapters. By convention, the multiplier is not a single number but a\n",
        "fraction, as shown in the equation below."
      ],
      "metadata": {
        "id": "FJP_lR4K9Pa3"
      },
      "id": "FJP_lR4K9Pa3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ea74713"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "\\text{output} = X @ W^T + \\frac{\\alpha}{r}\\left[X @ (B @ A)^T\\right]\n",
        "$$\n",
        "<center>Equation 3.4 - LoRA’s alpha</center>\n",
        "\n",
        "The denominator is fixed and it’s the rank we chose, but we can tweak the numerator, which is called LoRA’s\n",
        "alpha.\n",
        "\n",
        "In practice, alpha’s often set to twice the rank, effectively doubling the additional output."
      ],
      "id": "9ea74713"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k5zPYncOejmF",
        "outputId": "491f38b6-f606-4806-f75d-1ba7bd59c27c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5275, -0.0908, -1.3555,  ..., -1.1160,  0.3924,  1.2822]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "alpha = 2*r\n",
        "output = regular_output + (alpha / r) * additional_output\n",
        "output"
      ],
      "id": "k5zPYncOejmF"
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "id": "avdkCdAn91DG",
        "outputId": "da37ef7a-1f15-4703-c8d9-8fa98c6f6c43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "avdkCdAn91DG",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55483043"
      },
      "source": [
        "## The Road So Far"
      ],
      "id": "55483043"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9b713bac",
        "outputId": "162d5e81-39c7-4d04-d354-338916897f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "97bb4d8ce37a4ee9a2abf5bef5e8b7bc",
            "3182b97ab0cb44eba8117d20f2ece923",
            "cdbdbae83c964ef8b3a654209fc410c0",
            "7723631cb5a64fd783390140f16d4011",
            "64b5e519320e4af59150ebe7e3dd5af3",
            "0115bb773c2241fa8e2b2cadd96889df",
            "57b478e5b0104a90b45456015f30e77d",
            "c0d13cd490a141b99fab0daffbfd7bfa",
            "285a3a0de119473494d1a9c295d024bb",
            "022b675e3ecb42cfbf12ebeaaf6bb1a9",
            "00e2d6f84c6b4355bc70b4919b1f2cb4",
            "c4f1e1f7a885463fbf07931af3cfa026",
            "691580236976472a9469b72dd833ceac",
            "5b72c21145794498b43365d831267e43",
            "f5db76abd50e49d79fc670bbfab32aaa",
            "5b26f64e774d4df89d0e864529c3e77b",
            "9a44552df0704b76a931ac41a0a6f7d5",
            "525a94dc1d4d48349f9ab234c6ff0468",
            "a462604e5c1744a18e368fd616667903",
            "5d7f9490a88e41a9af688955ef320cd5",
            "8fe5ddeb468e411eadc0c0fc1014ab90",
            "b494d6709f654afb89023591ca853b53",
            "03415c41a50547929591d403267c6c36",
            "23955ab06baa448c87123eb4ff5963b5",
            "8df3c4c2072b461a8acc98e1c277bedd",
            "c25f032081104e5fb9fe7f37a14e130e",
            "5bfad2d137a847e0b0d06cdda3344b98",
            "94a840cddf234586b35ebd27ec96992e",
            "14f97e9d26b6439f8efb8e4f86b9ce47",
            "68b53a35545c4cf89b6236471fdf7f5c",
            "eb1c40f8a6744c4899555af9a0584d69",
            "6b7eacbe5e2a4c9c933f0c56840cd2c8",
            "6ea586980a114d96b06e0e6de5c3c2a9",
            "68cc5d5c1a4c4ddc92914ed00ce6f8c2",
            "88710d99585f4b3ea90c81bac3000670",
            "66219dbfa8004ac9b67c5215d6733a0d",
            "ccd17b8c996f42bdbf3f1f2b58d8f2b7",
            "e5f2b354ee6a4e848aabf6dee0a166d8",
            "0c054d6b87ca42efb23ff6c0ffb3fd00",
            "7c6a09831f8e473288e27da21f6e6176",
            "206a2383769e4d6f9ccd750a72848492",
            "f249493f1a464961b917fde6ae74358c",
            "569f93ac04b545cf86a8818b2451120a",
            "f327bde1365345d3973404b8a3f435d9"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97bb4d8ce37a4ee9a2abf5bef5e8b7bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4f1e1f7a885463fbf07931af3cfa026"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03415c41a50547929591d403267c6c36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68cc5d5c1a4c4ddc92914ed00ce6f8c2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=compute_dtype\n",
        ")\n",
        "\n",
        "# Here is our 4-bit quantized model\n",
        "model_q4 = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-350m\",\n",
        "    device_map='cuda:0',\n",
        "    torch_dtype=compute_dtype,\n",
        "    quantization_config=nf4_config\n",
        ")"
      ],
      "id": "9b713bac"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85a9072b"
      },
      "source": [
        "## Parameter Types and Gradients"
      ],
      "id": "85a9072b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "****\n",
        "**Summary of \"Parameter Types and Gradients\"**\n",
        "- quantization only freezes the linear layers that have been quantized\n",
        "- after quantization, a model can be prepared using the `prepare_model_for_kbit_training()` function\n",
        "  - it freezes **all** layers\n",
        "  - it casts every non-quantized 16-bit layer to FP32 to improve training\n",
        "  - it enables gradient checkpointing\n",
        "- you'll be able to unfreeze layers of your choice later on using the LoRA configuration\n",
        "****"
      ],
      "metadata": {
        "id": "H7HYW3OJ0v5p"
      },
      "id": "H7HYW3OJ0v5p"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49a9abe7",
        "outputId": "57775e4e-34ab-4c2d-89b0-6735b0184789",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('decoder.embed_tokens.weight', torch.float32),\n",
              " ('decoder.embed_positions.weight', torch.float32),\n",
              " ('decoder.layers.0.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.0.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.0.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.0.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.1.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.1.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.1.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.1.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.2.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.2.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.2.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.2.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.3.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.3.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.3.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.3.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.4.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.4.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.4.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.4.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.5.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.5.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.5.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.5.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.6.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.6.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.6.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.6.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.7.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.7.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.7.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.7.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.8.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.8.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.8.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.8.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.9.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.9.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.9.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.9.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.10.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.10.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.10.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.10.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.11.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.11.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.11.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.11.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.12.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.12.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.12.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.12.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.13.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.13.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.13.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.13.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.14.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.14.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.14.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.14.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.15.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.15.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.15.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.15.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.16.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.16.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.16.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.16.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.17.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.17.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.17.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.17.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.18.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.18.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.18.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.18.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.19.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.19.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.19.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.19.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.20.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.20.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.20.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.20.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.21.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.21.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.21.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.21.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.22.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.22.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.22.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.22.final_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.23.self_attn_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.23.self_attn_layer_norm.bias', torch.float32),\n",
              " ('decoder.layers.23.final_layer_norm.weight', torch.float32),\n",
              " ('decoder.layers.23.final_layer_norm.bias', torch.float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "def trainable_parms(model):\n",
        "    parms = [(name, param.dtype) for name, param in model.named_parameters() if param.requires_grad]\n",
        "    return parms\n",
        "\n",
        "trainable_parms(model_q4.model)"
      ],
      "id": "49a9abe7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2132acb9"
      },
      "source": [
        "Actually, there’s one more thing we need to do, and that’s preparing the model or, more precisely, calling the\n",
        "`prepare_model_for_kbit_training()` method."
      ],
      "id": "2132acb9"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3455ebde",
        "outputId": "29d778e9-e80f-4ab3-9001-cf7d91ad9ef9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "      (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
              "      (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-23): 24 x OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "prepared_model = prepare_model_for_kbit_training(\n",
        "    model_q4,\n",
        "    use_gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={'use_reentrant': False}\n",
        ")\n",
        "prepared_model"
      ],
      "id": "3455ebde"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method wraps the entire protocol for preparing a model before running a training. This includes:\n",
        "\n",
        "- Casting the layer norms to FP32.\n",
        "- Making the output of the embedding layer require grads.\n",
        "- Add the upcasting of the lm_head to FP32.\n",
        "- Freezes the whole model, meaning that none of its trainable layers can be trained anymore.\n",
        "\n",
        "\n",
        "How many trainable parameters does it currently have?"
      ],
      "metadata": {
        "id": "qI18O7fQzmPg"
      },
      "id": "qI18O7fQzmPg"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dbad9bb",
        "outputId": "b67661e6-5a1d-486b-9bef-5140241c7021",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "trainable_parms(prepared_model)"
      ],
      "id": "9dbad9bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s use a new helper function, parms_of_dtype(), to get the names of all the layers whose parameters\n",
        "are of a certain type."
      ],
      "metadata": {
        "id": "h0ilHEjM0XR-"
      },
      "id": "h0ilHEjM0XR-"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9e8be25c"
      },
      "outputs": [],
      "source": [
        "def parms_of_dtype(model, dtype=torch.float32):\n",
        "    parms = [name for name, param in model.named_parameters() if param.dtype == dtype]\n",
        "    return parms"
      ],
      "id": "9e8be25c"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653fa73d",
        "outputId": "009ab139-d610-444f-f4cc-ee29ff98de60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model.decoder.embed_tokens.weight',\n",
              " 'model.decoder.embed_positions.weight',\n",
              " 'model.decoder.layers.0.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.0.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.0.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.0.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.0.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.0.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.0.fc1.bias',\n",
              " 'model.decoder.layers.0.fc2.bias',\n",
              " 'model.decoder.layers.0.final_layer_norm.weight',\n",
              " 'model.decoder.layers.0.final_layer_norm.bias',\n",
              " 'model.decoder.layers.1.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.1.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.1.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.1.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.1.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.1.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.1.fc1.bias',\n",
              " 'model.decoder.layers.1.fc2.bias',\n",
              " 'model.decoder.layers.1.final_layer_norm.weight',\n",
              " 'model.decoder.layers.1.final_layer_norm.bias',\n",
              " 'model.decoder.layers.2.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.2.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.2.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.2.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.2.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.2.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.2.fc1.bias',\n",
              " 'model.decoder.layers.2.fc2.bias',\n",
              " 'model.decoder.layers.2.final_layer_norm.weight',\n",
              " 'model.decoder.layers.2.final_layer_norm.bias',\n",
              " 'model.decoder.layers.3.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.3.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.3.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.3.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.3.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.3.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.3.fc1.bias',\n",
              " 'model.decoder.layers.3.fc2.bias',\n",
              " 'model.decoder.layers.3.final_layer_norm.weight',\n",
              " 'model.decoder.layers.3.final_layer_norm.bias',\n",
              " 'model.decoder.layers.4.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.4.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.4.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.4.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.4.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.4.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.4.fc1.bias',\n",
              " 'model.decoder.layers.4.fc2.bias',\n",
              " 'model.decoder.layers.4.final_layer_norm.weight',\n",
              " 'model.decoder.layers.4.final_layer_norm.bias',\n",
              " 'model.decoder.layers.5.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.5.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.5.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.5.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.5.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.5.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.5.fc1.bias',\n",
              " 'model.decoder.layers.5.fc2.bias',\n",
              " 'model.decoder.layers.5.final_layer_norm.weight',\n",
              " 'model.decoder.layers.5.final_layer_norm.bias',\n",
              " 'model.decoder.layers.6.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.6.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.6.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.6.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.6.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.6.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.6.fc1.bias',\n",
              " 'model.decoder.layers.6.fc2.bias',\n",
              " 'model.decoder.layers.6.final_layer_norm.weight',\n",
              " 'model.decoder.layers.6.final_layer_norm.bias',\n",
              " 'model.decoder.layers.7.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.7.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.7.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.7.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.7.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.7.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.7.fc1.bias',\n",
              " 'model.decoder.layers.7.fc2.bias',\n",
              " 'model.decoder.layers.7.final_layer_norm.weight',\n",
              " 'model.decoder.layers.7.final_layer_norm.bias',\n",
              " 'model.decoder.layers.8.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.8.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.8.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.8.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.8.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.8.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.8.fc1.bias',\n",
              " 'model.decoder.layers.8.fc2.bias',\n",
              " 'model.decoder.layers.8.final_layer_norm.weight',\n",
              " 'model.decoder.layers.8.final_layer_norm.bias',\n",
              " 'model.decoder.layers.9.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.9.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.9.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.9.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.9.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.9.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.9.fc1.bias',\n",
              " 'model.decoder.layers.9.fc2.bias',\n",
              " 'model.decoder.layers.9.final_layer_norm.weight',\n",
              " 'model.decoder.layers.9.final_layer_norm.bias',\n",
              " 'model.decoder.layers.10.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.10.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.10.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.10.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.10.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.10.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.10.fc1.bias',\n",
              " 'model.decoder.layers.10.fc2.bias',\n",
              " 'model.decoder.layers.10.final_layer_norm.weight',\n",
              " 'model.decoder.layers.10.final_layer_norm.bias',\n",
              " 'model.decoder.layers.11.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.11.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.11.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.11.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.11.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.11.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.11.fc1.bias',\n",
              " 'model.decoder.layers.11.fc2.bias',\n",
              " 'model.decoder.layers.11.final_layer_norm.weight',\n",
              " 'model.decoder.layers.11.final_layer_norm.bias',\n",
              " 'model.decoder.layers.12.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.12.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.12.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.12.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.12.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.12.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.12.fc1.bias',\n",
              " 'model.decoder.layers.12.fc2.bias',\n",
              " 'model.decoder.layers.12.final_layer_norm.weight',\n",
              " 'model.decoder.layers.12.final_layer_norm.bias',\n",
              " 'model.decoder.layers.13.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.13.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.13.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.13.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.13.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.13.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.13.fc1.bias',\n",
              " 'model.decoder.layers.13.fc2.bias',\n",
              " 'model.decoder.layers.13.final_layer_norm.weight',\n",
              " 'model.decoder.layers.13.final_layer_norm.bias',\n",
              " 'model.decoder.layers.14.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.14.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.14.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.14.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.14.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.14.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.14.fc1.bias',\n",
              " 'model.decoder.layers.14.fc2.bias',\n",
              " 'model.decoder.layers.14.final_layer_norm.weight',\n",
              " 'model.decoder.layers.14.final_layer_norm.bias',\n",
              " 'model.decoder.layers.15.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.15.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.15.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.15.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.15.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.15.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.15.fc1.bias',\n",
              " 'model.decoder.layers.15.fc2.bias',\n",
              " 'model.decoder.layers.15.final_layer_norm.weight',\n",
              " 'model.decoder.layers.15.final_layer_norm.bias',\n",
              " 'model.decoder.layers.16.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.16.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.16.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.16.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.16.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.16.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.16.fc1.bias',\n",
              " 'model.decoder.layers.16.fc2.bias',\n",
              " 'model.decoder.layers.16.final_layer_norm.weight',\n",
              " 'model.decoder.layers.16.final_layer_norm.bias',\n",
              " 'model.decoder.layers.17.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.17.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.17.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.17.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.17.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.17.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.17.fc1.bias',\n",
              " 'model.decoder.layers.17.fc2.bias',\n",
              " 'model.decoder.layers.17.final_layer_norm.weight',\n",
              " 'model.decoder.layers.17.final_layer_norm.bias',\n",
              " 'model.decoder.layers.18.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.18.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.18.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.18.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.18.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.18.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.18.fc1.bias',\n",
              " 'model.decoder.layers.18.fc2.bias',\n",
              " 'model.decoder.layers.18.final_layer_norm.weight',\n",
              " 'model.decoder.layers.18.final_layer_norm.bias',\n",
              " 'model.decoder.layers.19.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.19.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.19.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.19.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.19.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.19.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.19.fc1.bias',\n",
              " 'model.decoder.layers.19.fc2.bias',\n",
              " 'model.decoder.layers.19.final_layer_norm.weight',\n",
              " 'model.decoder.layers.19.final_layer_norm.bias',\n",
              " 'model.decoder.layers.20.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.20.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.20.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.20.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.20.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.20.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.20.fc1.bias',\n",
              " 'model.decoder.layers.20.fc2.bias',\n",
              " 'model.decoder.layers.20.final_layer_norm.weight',\n",
              " 'model.decoder.layers.20.final_layer_norm.bias',\n",
              " 'model.decoder.layers.21.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.21.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.21.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.21.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.21.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.21.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.21.fc1.bias',\n",
              " 'model.decoder.layers.21.fc2.bias',\n",
              " 'model.decoder.layers.21.final_layer_norm.weight',\n",
              " 'model.decoder.layers.21.final_layer_norm.bias',\n",
              " 'model.decoder.layers.22.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.22.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.22.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.22.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.22.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.22.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.22.fc1.bias',\n",
              " 'model.decoder.layers.22.fc2.bias',\n",
              " 'model.decoder.layers.22.final_layer_norm.weight',\n",
              " 'model.decoder.layers.22.final_layer_norm.bias',\n",
              " 'model.decoder.layers.23.self_attn.k_proj.bias',\n",
              " 'model.decoder.layers.23.self_attn.v_proj.bias',\n",
              " 'model.decoder.layers.23.self_attn.q_proj.bias',\n",
              " 'model.decoder.layers.23.self_attn.out_proj.bias',\n",
              " 'model.decoder.layers.23.self_attn_layer_norm.weight',\n",
              " 'model.decoder.layers.23.self_attn_layer_norm.bias',\n",
              " 'model.decoder.layers.23.fc1.bias',\n",
              " 'model.decoder.layers.23.fc2.bias',\n",
              " 'model.decoder.layers.23.final_layer_norm.weight',\n",
              " 'model.decoder.layers.23.final_layer_norm.bias']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "parms_of_dtype(prepared_model)"
      ],
      "id": "653fa73d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "How much memory footprint?"
      ],
      "metadata": {
        "id": "PL53pfzd0owh"
      },
      "id": "PL53pfzd0owh"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "025520ae",
        "outputId": "d0026d64-69c5-4f6a-b68a-9ff1eb7750cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "264.15104"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "prepared_model.get_memory_footprint()/1e6"
      ],
      "id": "025520ae"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Parameter-Efficient Fine-Tuning(PEFT)*"
      ],
      "metadata": {
        "id": "2Fx7RcPC6tfe"
      },
      "id": "2Fx7RcPC6tfe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00981764"
      },
      "source": [
        "\n",
        "\"_🤗 PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a model’s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware._\n",
        "\n",
        "_PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference._\"\n",
        "\n",
        "****\n",
        "**Summary of \"PEFT\"**\n",
        "- the basic configuration below should work well in many cases\n",
        "```python\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "peft_model = get_peft_model(model, config)\n",
        "```\n",
        "- ranks of 8, 16, or 32 are typical, but using higher values shouldn’t significantly impact the model’s memory footprint.\n",
        "- the scaling factor, `lora_alpha` is typically twice the rank.\n",
        "- if your model has `Conv1D` layers, add `fan_in_fan_out=True` to your configuration\n",
        "- if your model was recently released, you may need to specify the `target_modules` manually\n",
        "  - typically, use the names of the massive linear layers in the attention module.\n",
        "- by default, only the adapters are trainable\n",
        "  - if you'd like to train other layers, such as layer norms, add them to the `modules_to_save` argument\n",
        "  - if you're adding your own tokens to the tokenizer, you'll need to also train vocabulary-related layers such as embeddings and the model's head\n",
        "****"
      ],
      "id": "00981764"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be9679cd",
        "outputId": "e95bb5f9-9567-467e-84b3-2b7f0f2b3af0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LoraConfig(task_type=None, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules=None, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "lora_config = LoraConfig()\n",
        "lora_config"
      ],
      "id": "be9679cd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the typical—and\n",
        "minimal—configuration looks like this:"
      ],
      "metadata": {
        "id": "T1qaKIUp7WIF"
      },
      "id": "T1qaKIUp7WIF"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "13211b1e"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "id": "13211b1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c79b1ea"
      },
      "source": [
        "**`target_modules`**\n",
        "\n",
        "Since there are new models and architectures being released on a weekly basis, chances are that there is no preconfigured list of target layers in your currently installed version of the PEFT library. In this case, you’ll be greeted with the following error:\n",
        "\n",
        "***\n",
        "`ValueError: Please specify `target_modules` in `peft_config``\n",
        "***\n",
        "\n",
        "Once you have the names, you can use yet another configuration argument: target_modules, which is either\n",
        "the name or a list of the names of the modules to which you want to apply the adapters.\n",
        "\n",
        "**Supported Models**\n",
        "\n",
        "If you'd like to check if a given model's architecture is already supported by the installed version of the `peft` package, you can do the following:"
      ],
      "id": "1c79b1ea"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a3bd853",
        "outputId": "a9e406f2-611f-474d-876f-f23ba0e8206d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['t5', 'mt5', 'bart', 'gpt2', 'bloom', 'blip-2', 'opt', 'gptj', 'gpt_neox', 'gpt_neo', 'bert', 'roberta', 'xlm-roberta', 'electra', 'deberta-v2', 'deberta', 'layoutlm', 'llama', 'llama4', 'chatglm', 'gpt_bigcode', 'mpt', 'RefinedWebModel', 'RefinedWeb', 'falcon', 'btlm', 'codegen', 'mistral', 'mixtral', 'stablelm', 'phi', 'gemma', 'gemma2', 'gemma3_text', 'qwen2', 'qwen3', 'rwkv', 'rwkv7'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "from peft.utils.constants import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
        "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.keys()"
      ],
      "id": "9a3bd853"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the supported models for Phi."
      ],
      "metadata": {
        "id": "aN7Tm8ie-z1s"
      },
      "id": "aN7Tm8ie-z1s"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a729f4bb",
        "outputId": "552240e7-3d9c-4712-9798-bfc30ae2104f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['q_proj', 'v_proj', 'fc1', 'fc2']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['phi']"
      ],
      "id": "a729f4bb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the supported models for llama4."
      ],
      "metadata": {
        "id": "dCPvOXdA--cO"
      },
      "id": "dCPvOXdA--cO"
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['llama4']"
      ],
      "metadata": {
        "id": "Egh5SekI_C3y",
        "outputId": "f8ec0859-49bb-40fa-9000-7ec37999fc4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Egh5SekI_C3y",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['q_proj', 'v_proj']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the supported models for qwen3."
      ],
      "metadata": {
        "id": "2YknKzl4_MG-"
      },
      "id": "2YknKzl4_MG-"
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['qwen3']"
      ],
      "metadata": {
        "id": "cqnby7_y_PKY",
        "outputId": "ced2e98f-e0a5-4d71-f8de-2184b5a4f932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cqnby7_y_PKY",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['q_proj', 'v_proj']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b9a83df"
      },
      "source": [
        "## The PEFT Model"
      ],
      "id": "7b9a83df"
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, though, we’re doing it ourselves to better understand how the model is effectively modified. It’s\n",
        "pretty easy:"
      ],
      "metadata": {
        "id": "4i-XSXS6_mq7"
      },
      "id": "4i-XSXS6_mq7"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35c51eff",
        "outputId": "1db1807c-8176-47d7-baa3-c7841d206b34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): OPTForCausalLM(\n",
              "      (model): OPTModel(\n",
              "        (decoder): OPTDecoder(\n",
              "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
              "          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
              "          (layers): ModuleList(\n",
              "            (0-23): 24 x OPTDecoderLayer(\n",
              "              (self_attn): OPTAttention(\n",
              "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                (v_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.05, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (q_proj): lora.Linear4bit(\n",
              "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                  (lora_dropout): ModuleDict(\n",
              "                    (default): Dropout(p=0.05, inplace=False)\n",
              "                  )\n",
              "                  (lora_A): ModuleDict(\n",
              "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "                  )\n",
              "                  (lora_B): ModuleDict(\n",
              "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                  )\n",
              "                  (lora_embedding_A): ParameterDict()\n",
              "                  (lora_embedding_B): ParameterDict()\n",
              "                  (lora_magnitude_vector): ModuleDict()\n",
              "                )\n",
              "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              )\n",
              "              (activation_fn): ReLU()\n",
              "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "peft_model = get_peft_model(prepared_model, config, adapter_name='default')\n",
        "peft_model"
      ],
      "id": "35c51eff"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fe2884e",
        "outputId": "2f60f4c7-9d20-4063-ef28-039fd9da04c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['q_proj', 'v_proj']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['opt']"
      ],
      "id": "0fe2884e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a closer look at the modified layer:"
      ],
      "metadata": {
        "id": "IXZHakdKAE7G"
      },
      "id": "IXZHakdKAE7G"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc0992b0",
        "outputId": "b98634d3-ec1f-4eb4-a0e9-794b83aa1ef6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear4bit(in_features=1024, out_features=1024, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.k_proj\n",
        "lin"
      ],
      "id": "dc0992b0"
    },
    {
      "cell_type": "code",
      "source": [
        "lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj\n",
        "lin"
      ],
      "metadata": {
        "id": "VOIaemJCAROK",
        "outputId": "90a7d0d0-5fdc-455d-a0e8-cedd62911e9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VOIaemJCAROK",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lora.Linear4bit(\n",
              "  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "  (lora_dropout): ModuleDict(\n",
              "    (default): Dropout(p=0.05, inplace=False)\n",
              "  )\n",
              "  (lora_A): ModuleDict(\n",
              "    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  )\n",
              "  (lora_B): ModuleDict(\n",
              "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "  )\n",
              "  (lora_embedding_A): ParameterDict()\n",
              "  (lora_embedding_B): ParameterDict()\n",
              "  (lora_magnitude_vector): ModuleDict()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.v_proj\n",
        "lin"
      ],
      "metadata": {
        "id": "5Rr0qHrzApSH",
        "outputId": "6e1e8c0c-3640-4b5a-c0c0-a54e3814802e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5Rr0qHrzApSH",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lora.Linear4bit(\n",
              "  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "  (lora_dropout): ModuleDict(\n",
              "    (default): Dropout(p=0.05, inplace=False)\n",
              "  )\n",
              "  (lora_A): ModuleDict(\n",
              "    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  )\n",
              "  (lora_B): ModuleDict(\n",
              "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "  )\n",
              "  (lora_embedding_A): ParameterDict()\n",
              "  (lora_embedding_B): ParameterDict()\n",
              "  (lora_magnitude_vector): ModuleDict()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc = peft_model.base_model.model.model.decoder.layers[0].fc1\n",
        "fc"
      ],
      "metadata": {
        "id": "DCtwFqRjBQT5",
        "outputId": "7c891165-d7eb-4c8e-fc79-6cf3c6b713a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DCtwFqRjBQT5",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear4bit(in_features=1024, out_features=4096, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fc = peft_model.base_model.model.model.decoder.layers[0].fc2\n",
        "fc"
      ],
      "metadata": {
        "id": "mNIe9EA0CBHf",
        "outputId": "4cc24f92-eed4-4732-f6b8-5e5049522e13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mNIe9EA0CBHf",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear4bit(in_features=4096, out_features=1024, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you wonder how many trainable parameters our model has now?"
      ],
      "metadata": {
        "id": "NwhUwYJ7CWTv"
      },
      "id": "NwhUwYJ7CWTv"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77fa2a75",
        "outputId": "5b6058bf-2c3b-4499-bd0e-e98b1a8416b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 786,432 || all params: 331,982,848 || trainable%: 0.2369\n"
          ]
        }
      ],
      "source": [
        "peft_model.print_trainable_parameters()"
      ],
      "id": "77fa2a75"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trainable parameters are—all of them—from the attached adapters: they are layers A (named lora_A) and\n",
        "B (named lora_B)."
      ],
      "metadata": {
        "id": "QEZcFdauCkP0"
      },
      "id": "QEZcFdauCkP0"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca0a102e",
        "outputId": "4be30f5c-3533-429c-dfb8-9039d8ec6ad7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight',\n",
              "  torch.float32),\n",
              " ('model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight',\n",
              "  torch.float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "trainable_parms(peft_model.base_model.model)"
      ],
      "id": "ca0a102e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e8fefa"
      },
      "source": [
        "**`modules_to_save`**\n",
        "\n",
        "The good news is that you can pick and\n",
        "choose which layers to unfreeze (e.g., layer norms, embeddings, the head)."
      ],
      "id": "a9e8fefa"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7eb95263"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=['layer_norm'] # unfreezing layer norms\n",
        ")"
      ],
      "id": "7eb95263"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "57fb1553"
      },
      "outputs": [],
      "source": [
        "# Since the model is modified in-place, we need to unload adapters\n",
        "# from previous configuration to avoid mixing them.\n",
        "# In a regular workflow, you'd load configuration only once and\n",
        "# this wouldn't be needed.\n",
        "_ = peft_model.unload()"
      ],
      "id": "57fb1553"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9052a377",
        "outputId": "e315ea36-ed18-4475-d720-68a4673ec753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 884,736 || all params: 332,081,152 || trainable%: 0.2664\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(prepared_model, config)\n",
        "peft_model.print_trainable_parameters()"
      ],
      "id": "9052a377"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keeping the layer norms trainable added roughly 100,000 parameters that need to be trained."
      ],
      "metadata": {
        "id": "xO58tNNnER00"
      },
      "id": "xO58tNNnER00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a620eaf7"
      },
      "source": [
        "## Embeddings"
      ],
      "id": "a620eaf7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you need to add new tokens to the tokenizer’s vocabulary, you may need to resize your embedding layers and model’s head."
      ],
      "metadata": {
        "id": "FnTbRJBLjLdT"
      },
      "id": "FnTbRJBLjLdT"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "13f7e0b2"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=['layer_norm', 'embed_tokens'] # unfreezing embedding layers as well\n",
        ")"
      ],
      "id": "13f7e0b2"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "bc90eaa5"
      },
      "outputs": [],
      "source": [
        "# Since the model is modified in-place, we need to unload adapters\n",
        "# from previous configuration to avoid mixing them.\n",
        "# In a regular workflow, you'd load configuration only once and\n",
        "# this wouldn't be needed.\n",
        "_ = peft_model.unload()"
      ],
      "id": "bc90eaa5"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80178211",
        "outputId": "a7ae0d18-3421-4b55-c4ca-72b7a14a4696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 26,624,000 || all params: 383,559,680 || trainable%: 6.9413\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(prepared_model, config)\n",
        "peft_model.print_trainable_parameters()"
      ],
      "id": "80178211"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embedding layer alone has approximately 26 million parameters, roughly 7% of the base model’s total\n",
        "size.\n",
        "\n",
        "Let’s add it to the list of target\n",
        "modules instead:"
      ],
      "metadata": {
        "id": "34d14HhCkQXK"
      },
      "id": "34d14HhCkQXK"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "04e7735c"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['embed_tokens', 'q_proj', 'v_proj']\n",
        ")"
      ],
      "id": "04e7735c"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "e1d01087"
      },
      "outputs": [],
      "source": [
        "# Since the model is modified in-place, we need to unload adapters\n",
        "# from previous configuration to avoid mixing them.\n",
        "# In a regular workflow, you'd load configuration only once and\n",
        "# this wouldn't be needed.\n",
        "_ = peft_model.unload()"
      ],
      "id": "e1d01087"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a09a856",
        "outputId": "edec770c-7dea-4431-f560-433585500ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,192,704 || all params: 358,128,384 || trainable%: 0.3330\n"
          ]
        }
      ],
      "source": [
        "peft_model = get_peft_model(prepared_model, config)\n",
        "peft_model.print_trainable_parameters()"
      ],
      "id": "3a09a856"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check it out: the embedding layer uses the lora_embedding_A and lora_embedding_B attributes."
      ],
      "metadata": {
        "id": "6G168VQQk0j2"
      },
      "id": "6G168VQQk0j2"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f13bade5",
        "outputId": "dd9246ed-f33a-4ab6-980a-496ad1cae1b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lora.Embedding(\n",
              "  (base_layer): Embedding(50272, 512, padding_idx=1)\n",
              "  (lora_dropout): ModuleDict(\n",
              "    (default): Dropout(p=0.05, inplace=False)\n",
              "  )\n",
              "  (lora_A): ModuleDict()\n",
              "  (lora_B): ModuleDict()\n",
              "  (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 8x50272 (cuda:0)])\n",
              "  (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x8 (cuda:0)])\n",
              "  (lora_magnitude_vector): ModuleDict()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "lin = peft_model.base_model.model.model.decoder.embed_tokens\n",
        "lin"
      ],
      "id": "f13bade5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9875a42d"
      },
      "source": [
        "## Managing Adapters"
      ],
      "id": "9875a42d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46154ae3",
        "outputId": "7e74c29f-7089-490e-f2cd-99904997c0b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModuleDict(\n",
              "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.load_adapter('dvgodoy/opt-350m-lora-yoda', adapter_name='yoda')\n",
        "lora_A = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj.lora_A\n",
        "lora_A"
      ],
      "id": "46154ae3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cfaf80a",
        "outputId": "7b6ec260-4988-465c-ea76-759acc5ab81b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModuleDict(\n",
              "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (third): Linear(in_features=1024, out_features=8, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.add_adapter(adapter_name='third', peft_config=config)\n",
        "lora_A"
      ],
      "id": "9cfaf80a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a9d787c",
        "outputId": "3bd55add-b059-4be1-85b5-5516f2089e4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModuleDict(\n",
              "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.delete_adapter(adapter_name='third')\n",
        "lora_A"
      ],
      "id": "2a9d787c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60106701",
        "outputId": "580075aa-669d-41e9-fa3f-c779cfc4caa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['default', 'yoda'])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.peft_config.keys()"
      ],
      "id": "60106701"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e69e9f7c",
        "outputId": "ea949b66-f88f-41b0-83de-2dc3d83aa7b8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'default'"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.active_adapter"
      ],
      "id": "e69e9f7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "563a806c",
        "outputId": "a166dbb0-ccfe-49b5-dde2-d349290566ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yoda'"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.set_adapter('yoda')\n",
        "peft_model.active_adapter"
      ],
      "id": "563a806c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c5f03c6"
      },
      "source": [
        "```python\n",
        "with peft_model.disable_adapter():\n",
        "    original_outputs = peft_model(inputs)\n",
        "\n",
        "original_outputs = peft_model.base_model(inputs)\n",
        "```"
      ],
      "id": "8c5f03c6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8358ebf9",
        "outputId": "00aa69eb-f21e-4f5e-e9ec-8c054ebe06b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ModuleDict(\n",
              "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
              "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.merge_adapter(adapter_names=['yoda'])\n",
        "lora_A"
      ],
      "id": "8358ebf9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdea3313",
        "outputId": "ce884503-0035-4640-b9be-f9b3a84cc0c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): OPTForCausalLM(\n",
              "      (model): OPTModel(\n",
              "        (decoder): OPTDecoder(\n",
              "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
              "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
              "          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
              "          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
              "          (layers): ModuleList(\n",
              "            (0-23): 24 x OPTDecoderLayer(\n",
              "              (self_attn): OPTAttention(\n",
              "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
              "              )\n",
              "              (activation_fn): ReLU()\n",
              "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
              "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
              "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_model.unload()\n",
        "peft_model.base_model.model.model.decoder.layers[0].self_attn"
      ],
      "id": "cdea3313"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dda37040"
      },
      "source": [
        "### Coming Up in \"Fine-Tuning LLMs\"\n",
        "\n",
        "Low-rank adapters saved the day by swooping in and enabling fast and cheap fine-tuning for LLMs. These humongous models, although powerful, are masters of a single trade—predicting the next token—thus remaining limited by the structure of their inputs. A new kind of input must be developed to enable these creatures to chat. Learn more about the incredible tale of chat templates in the next chapter of \"Fine-Tuning LLMs.\""
      ],
      "id": "dda37040"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97bb4d8ce37a4ee9a2abf5bef5e8b7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3182b97ab0cb44eba8117d20f2ece923",
              "IPY_MODEL_cdbdbae83c964ef8b3a654209fc410c0",
              "IPY_MODEL_7723631cb5a64fd783390140f16d4011"
            ],
            "layout": "IPY_MODEL_64b5e519320e4af59150ebe7e3dd5af3"
          }
        },
        "3182b97ab0cb44eba8117d20f2ece923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0115bb773c2241fa8e2b2cadd96889df",
            "placeholder": "​",
            "style": "IPY_MODEL_57b478e5b0104a90b45456015f30e77d",
            "value": "config.json: 100%"
          }
        },
        "cdbdbae83c964ef8b3a654209fc410c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d13cd490a141b99fab0daffbfd7bfa",
            "max": 644,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_285a3a0de119473494d1a9c295d024bb",
            "value": 644
          }
        },
        "7723631cb5a64fd783390140f16d4011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_022b675e3ecb42cfbf12ebeaaf6bb1a9",
            "placeholder": "​",
            "style": "IPY_MODEL_00e2d6f84c6b4355bc70b4919b1f2cb4",
            "value": " 644/644 [00:00&lt;00:00, 21.2kB/s]"
          }
        },
        "64b5e519320e4af59150ebe7e3dd5af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0115bb773c2241fa8e2b2cadd96889df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b478e5b0104a90b45456015f30e77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0d13cd490a141b99fab0daffbfd7bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "285a3a0de119473494d1a9c295d024bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "022b675e3ecb42cfbf12ebeaaf6bb1a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00e2d6f84c6b4355bc70b4919b1f2cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4f1e1f7a885463fbf07931af3cfa026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_691580236976472a9469b72dd833ceac",
              "IPY_MODEL_5b72c21145794498b43365d831267e43",
              "IPY_MODEL_f5db76abd50e49d79fc670bbfab32aaa"
            ],
            "layout": "IPY_MODEL_5b26f64e774d4df89d0e864529c3e77b"
          }
        },
        "691580236976472a9469b72dd833ceac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a44552df0704b76a931ac41a0a6f7d5",
            "placeholder": "​",
            "style": "IPY_MODEL_525a94dc1d4d48349f9ab234c6ff0468",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "5b72c21145794498b43365d831267e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a462604e5c1744a18e368fd616667903",
            "max": 662513657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d7f9490a88e41a9af688955ef320cd5",
            "value": 662513657
          }
        },
        "f5db76abd50e49d79fc670bbfab32aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fe5ddeb468e411eadc0c0fc1014ab90",
            "placeholder": "​",
            "style": "IPY_MODEL_b494d6709f654afb89023591ca853b53",
            "value": " 663M/663M [00:05&lt;00:00, 230MB/s]"
          }
        },
        "5b26f64e774d4df89d0e864529c3e77b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a44552df0704b76a931ac41a0a6f7d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "525a94dc1d4d48349f9ab234c6ff0468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a462604e5c1744a18e368fd616667903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d7f9490a88e41a9af688955ef320cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fe5ddeb468e411eadc0c0fc1014ab90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b494d6709f654afb89023591ca853b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03415c41a50547929591d403267c6c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23955ab06baa448c87123eb4ff5963b5",
              "IPY_MODEL_8df3c4c2072b461a8acc98e1c277bedd",
              "IPY_MODEL_c25f032081104e5fb9fe7f37a14e130e"
            ],
            "layout": "IPY_MODEL_5bfad2d137a847e0b0d06cdda3344b98"
          }
        },
        "23955ab06baa448c87123eb4ff5963b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94a840cddf234586b35ebd27ec96992e",
            "placeholder": "​",
            "style": "IPY_MODEL_14f97e9d26b6439f8efb8e4f86b9ce47",
            "value": "model.safetensors: 100%"
          }
        },
        "8df3c4c2072b461a8acc98e1c277bedd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68b53a35545c4cf89b6236471fdf7f5c",
            "max": 662435448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb1c40f8a6744c4899555af9a0584d69",
            "value": 662435448
          }
        },
        "c25f032081104e5fb9fe7f37a14e130e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7eacbe5e2a4c9c933f0c56840cd2c8",
            "placeholder": "​",
            "style": "IPY_MODEL_6ea586980a114d96b06e0e6de5c3c2a9",
            "value": " 662M/662M [00:11&lt;00:00, 61.0MB/s]"
          }
        },
        "5bfad2d137a847e0b0d06cdda3344b98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94a840cddf234586b35ebd27ec96992e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f97e9d26b6439f8efb8e4f86b9ce47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b53a35545c4cf89b6236471fdf7f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb1c40f8a6744c4899555af9a0584d69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b7eacbe5e2a4c9c933f0c56840cd2c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ea586980a114d96b06e0e6de5c3c2a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68cc5d5c1a4c4ddc92914ed00ce6f8c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88710d99585f4b3ea90c81bac3000670",
              "IPY_MODEL_66219dbfa8004ac9b67c5215d6733a0d",
              "IPY_MODEL_ccd17b8c996f42bdbf3f1f2b58d8f2b7"
            ],
            "layout": "IPY_MODEL_e5f2b354ee6a4e848aabf6dee0a166d8"
          }
        },
        "88710d99585f4b3ea90c81bac3000670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c054d6b87ca42efb23ff6c0ffb3fd00",
            "placeholder": "​",
            "style": "IPY_MODEL_7c6a09831f8e473288e27da21f6e6176",
            "value": "generation_config.json: 100%"
          }
        },
        "66219dbfa8004ac9b67c5215d6733a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_206a2383769e4d6f9ccd750a72848492",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f249493f1a464961b917fde6ae74358c",
            "value": 137
          }
        },
        "ccd17b8c996f42bdbf3f1f2b58d8f2b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_569f93ac04b545cf86a8818b2451120a",
            "placeholder": "​",
            "style": "IPY_MODEL_f327bde1365345d3973404b8a3f435d9",
            "value": " 137/137 [00:00&lt;00:00, 5.92kB/s]"
          }
        },
        "e5f2b354ee6a4e848aabf6dee0a166d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c054d6b87ca42efb23ff6c0ffb3fd00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c6a09831f8e473288e27da21f6e6176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "206a2383769e4d6f9ccd750a72848492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f249493f1a464961b917fde6ae74358c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "569f93ac04b545cf86a8818b2451120a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f327bde1365345d3973404b8a3f435d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}