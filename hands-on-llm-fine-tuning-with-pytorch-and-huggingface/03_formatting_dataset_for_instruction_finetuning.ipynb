{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/small-language-models-fine-tuning/blob/main/hands-on-llm-fine-tuning-with-pytorch-and-huggingface/03_formatting_dataset_for_instruction_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2faf67d8",
      "metadata": {
        "id": "2faf67d8"
      },
      "source": [
        "## Chapter 4: Formatting Your Dataset\n",
        "\n",
        "### Spoilers\n",
        "\n",
        "In this chapter, we will:\n",
        "\n",
        "- Understand the importance of defining a proper chat template\n",
        "- Discuss several formatting alternatives, including custom formatting functions and templates\n",
        "- Configure the tokenizer and the model’s embedding layer\n",
        "- Explore packed datasets and different data collators for loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9628f687",
      "metadata": {
        "id": "9628f687"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8bb20be1",
      "metadata": {
        "id": "8bb20be1"
      },
      "outputs": [],
      "source": [
        "# If you're running on Colab\n",
        "%%capture\n",
        "\n",
        "!pip install datasets bitsandbytes trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ee97ba75",
      "metadata": {
        "id": "ee97ba75"
      },
      "outputs": [],
      "source": [
        "# If you're running on runpod.io's Jupyter Template\n",
        "#!pip install datasets bitsandbytes trl transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "22a9ba58",
      "metadata": {
        "id": "22a9ba58",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
        "from datasets import load_dataset, Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, DataCollatorForLanguageModeling, DataCollatorWithPadding, DataCollatorWithFlattening, BitsAndBytesConfig\n",
        "from trl.data_utils import pack_dataset\n",
        "# from trl.extras.dataset_formatting import FORMAT_MAPPING, instructions_formatting_function, conversations_formatting_function\n",
        "# from trl import setup_chat_format #, DataCollatorForCompletionOnlyLM - removed in version 0.20\n",
        "# from trl.trainer import ConstantLengthDataset - removed in version 0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wFdoiGOXlRuQ",
      "metadata": {
        "id": "wFdoiGOXlRuQ"
      },
      "outputs": [],
      "source": [
        "# If you're running on Colab, you need to download the replacements of removed functions\n",
        "!wget https://raw.githubusercontent.com/dvgodoy/FineTuningLLMs/refs/heads/main/compatibility_functions.py\n",
        "\n",
        "from compatibility_functions import DataCollatorForCompletionOnlyLM, ConstantLengthDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80bd6389",
      "metadata": {
        "id": "80bd6389"
      },
      "source": [
        "## The Goal\n",
        "\n",
        "We format the dataset to provide structure and cues to the LLM. We can easily steer its behavior (e.g., instruction-tuning) by carefully wrapping each component—the user’s prompt and the model’s completion—with appropriate tags and special tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380bf61f",
      "metadata": {
        "id": "380bf61f"
      },
      "source": [
        "**Formatting in a Nutshell**\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/base_prompt.png?raw=True)\n",
        "<center>Figure 4.1 - Base model’s next token prediction</center>\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/fine_tuned_prompt.png?raw=True)\n",
        "<center>Figure 4.2 - Fine-tuned model triggered by response template</center>\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_prompt_new.png?raw=True)\n",
        "<center>Figure 4.3 - Chat model using chat template</center>\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/chat_example_new.png?raw=True)\n",
        "<center>Figure 4.4 - General structure of a chat template</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a379eb0",
      "metadata": {
        "id": "0a379eb0"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3ec5f2",
      "metadata": {
        "id": "9d3ec5f2"
      },
      "outputs": [],
      "source": [
        "supported = torch.cuda.is_bf16_supported(including_emulation=False)\n",
        "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "   load_in_4bit=True,\n",
        "   bnb_4bit_quant_type=\"nf4\",\n",
        "   bnb_4bit_use_double_quant=True,\n",
        "   bnb_4bit_compute_dtype=compute_dtype\n",
        ")\n",
        "\n",
        "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
        "                                                device_map='cuda:0',\n",
        "                                                dtype=compute_dtype,\n",
        "                                                quantization_config=nf4_config)\n",
        "\n",
        "model_q4 = prepare_model_for_kbit_training(model_q4)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "peft_model = get_peft_model(model_q4, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3e76cbf",
      "metadata": {
        "id": "c3e76cbf"
      },
      "source": [
        "## Applying Templates\n",
        "\n",
        "****\n",
        "**Summary of \"Applying Templates\"**\n",
        "\n",
        "You have three options for formatting your dataset:\n",
        "1. Your dataset is in one of the **two formats supported by the `STTrainer` class** (conversational or instruction):\n",
        "   - Your **tokenizer must have a chat template** configured.\n",
        "   - No need to define a formatting function or format the dataset before training.\n",
        "   - **IMPORTANT**: **the instruction format is not properly supported anymore by recent versions of the `trl` package**\n",
        "2. You want to use a **custom formatting function** (see \"BYOFF, Bring Your Own Formatting Function\"):\n",
        "   - The custom function should be provided as the **`formatting_func` argument of the `SFTTrainer` class** (see Chapter 5).\n",
        "   - Your formatting function **must handle batches of data**.\n",
        "     - Test it by calling the dataset's `map()` method with `batched=True`.\n",
        "    - No need to apply the function to the dataset before training.\n",
        "    - If your tokenizer already **has a chat template**:\n",
        "      - You may call its `apply_chat_template()` method in your function.\n",
        "      - Stick to the template's general format (instruction and response templates).\n",
        "      - If the template doesn’t include one, **you may append an `EOS` token to the end of the formatted output**.\n",
        "   - If your tokenizer **does not have a chat template**:\n",
        "     - You're free to define the general format, including instruction and response templates (see \"Advanced—BYOT, Bring Your Own Template\")\n",
        "3. Your dataset is **already formatted** (see \"BYOFD, Bring Your Own Formatted Data\"):\n",
        "   - The column containing the formatted data should be provided as the **`dataset_text_field` argument of the `SFTTrainer` class** (see Chapter 5).\n",
        "   - Even though you can use your own formatting function to preprocess your dataset, it won't be used by the trainer class.\n",
        "   - Ensure your **data is compatible with the tokenizer's template**.\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a quick look at Phi-3’s chat template."
      ],
      "metadata": {
        "id": "3I4Z_uG1MlJB"
      },
      "id": "3I4Z_uG1MlJB"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8f92fae6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f92fae6",
        "outputId": "a1d74fb4-b95c-4ff3-8640-fd56d0342662",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% else %}{{ eos_token }}{% endif %}\n"
          ]
        }
      ],
      "source": [
        "tokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
        "print(tokenizer_phi.chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try it out by calling the tokenizer’s apply_chat_template() method with a set of messages."
      ],
      "metadata": {
        "id": "WU6f44LgNXfi"
      },
      "id": "WU6f44LgNXfi"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e134f99c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e134f99c",
        "outputId": "a2a3658a-cba8-4c32-f4a8-2200178c02a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a helpful AI assistant.<|end|>\n",
            "<|user|>\n",
            "What is the capital of Argentina?<|end|>\n",
            "<|assistant|>\n",
            "Buenos Aires.<|end|>\n",
            "<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
        "    {'role': 'user', 'content': 'What is the capital of Argentina?'},\n",
        "    {'role': 'assistant', 'content': 'Buenos Aires.'}\n",
        "]\n",
        "\n",
        "formatted = tokenizer_phi.apply_chat_template(\n",
        "    conversation=messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=False # since this set of messages already contains both the user’s prompt and the assistant’s completion\n",
        ")\n",
        "print(formatted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later on, after fine-tuning the model, we’ll have to trigger the model to do its thing and generate a completion\n",
        "to our prompt. That’s when we need to add a generation prompt."
      ],
      "metadata": {
        "id": "1p3BepkEONA1"
      },
      "id": "1p3BepkEONA1"
    },
    {
      "cell_type": "code",
      "source": [
        "messages[:-1]"
      ],
      "metadata": {
        "id": "TD-l2GZqOaIR",
        "outputId": "99460a66-711b-4d25-aabc-4afb4a7a2fc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TD-l2GZqOaIR",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system', 'content': 'You are a helpful AI assistant.'},\n",
              " {'role': 'user', 'content': 'What is the capital of Argentina?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8a5014cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a5014cf",
        "outputId": "16f3eee7-d2f3-4ab2-a499-89cb281cb947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>\n",
            "You are a helpful AI assistant.<|end|>\n",
            "<|user|>\n",
            "What is the capital of Argentina?<|end|>\n",
            "<|assistant|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inference_input = tokenizer_phi.apply_chat_template(\n",
        "    conversation=messages[:-1], # removing assistant part\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "print(inference_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We suppressed the last message (the assistant’s completion), but its \"cue\" (<|assistant|>) was appended to\n",
        "the end to let the model know it’s its turn to \"talk.\""
      ],
      "metadata": {
        "id": "5lWA9WitO0Kk"
      },
      "id": "5lWA9WitO0Kk"
    },
    {
      "cell_type": "markdown",
      "id": "a7c3a356",
      "metadata": {
        "id": "a7c3a356"
      },
      "source": [
        "##Supported Format"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversational**"
      ],
      "metadata": {
        "id": "K1St-xBlPFi9"
      },
      "id": "K1St-xBlPFi9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60a40284",
      "metadata": {
        "id": "60a40284"
      },
      "outputs": [],
      "source": [
        "conversation_ds = Dataset.from_list([{'messages': messages}])\n",
        "conversation_ds.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48fd0626",
      "metadata": {
        "id": "48fd0626"
      },
      "outputs": [],
      "source": [
        "FORMAT_MAPPING['chatml'] == conversation_ds.features['messages']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0129a7b9",
      "metadata": {
        "id": "0129a7b9"
      },
      "outputs": [],
      "source": [
        "formatting_func = conversations_formatting_function(tokenizer_phi, messages_field='messages')\n",
        "\n",
        "print(formatting_func(conversation_ds[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c525cc9c",
      "metadata": {
        "id": "c525cc9c"
      },
      "source": [
        "```python\n",
        "# formatting function for conversational format\n",
        "def format_dataset(examples):\n",
        "    if isinstance(examples[messages_field][0], list):\n",
        "        output_texts = []\n",
        "        for i in range(len(examples[messages_field])):\n",
        "            output_texts.append(tokenizer.apply_chat_template(examples[messages_field][i], tokenize=False))\n",
        "        return output_texts\n",
        "    else:\n",
        "        return tokenizer.apply_chat_template(examples[messages_field], tokenize=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371de955",
      "metadata": {
        "id": "371de955"
      },
      "source": [
        "##### Instruction\n",
        "\n",
        "****\n",
        "**IMPORTANT UPDATE**: unfortunately, in more recent versions of the `trl` library, the \"instruction\" format is not properly supported anymore, thus leading to the chat template not being applied to the dataset. In order to avoid this issue, it is recommended to use the \"conversational\" format instead.\n",
        "****\n",
        "\n",
        "```python\n",
        "instructions = [{'prompt': 'What is the capital of Argentina?',\n",
        "                 'completion': 'Buenos Aires.'}]\n",
        "\n",
        "instruction_ds = Dataset.from_list(instructions)\n",
        "instruction_ds.features\n",
        "\n",
        "{'prompt': Value('string'), 'completion': Value('string')}\n",
        "\n",
        "FORMAT_MAPPING['instruction'] == instruction_ds.features\n",
        "\n",
        "True\n",
        "\n",
        "formatting_func = instructions_formatting_function(tokenizer_phi)\n",
        "formatting_func\n",
        "\n",
        "trl.extras.dataset_formatting.instructions_formatting_function.<locals>.format_dataset\n",
        "def format_dataset(examples)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZZhAjQyfJBA7",
      "metadata": {
        "id": "ZZhAjQyfJBA7"
      },
      "outputs": [],
      "source": [
        "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
        "# Converts dataset from prompt/completion format (not supported anymore)\n",
        "# to the conversational format\n",
        "def format_dataset(examples):\n",
        "    if isinstance(examples[\"prompt\"], list):\n",
        "        output_texts = []\n",
        "        for i in range(len(examples[\"prompt\"])):\n",
        "            converted_sample = [\n",
        "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
        "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
        "            ]\n",
        "            output_texts.append(converted_sample)\n",
        "        return {'messages': output_texts}\n",
        "    else:\n",
        "        converted_sample = [\n",
        "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
        "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
        "        ]\n",
        "        return {'messages': converted_sample}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb8c7c80",
      "metadata": {
        "id": "fb8c7c80"
      },
      "outputs": [],
      "source": [
        "batch_prompts_completions = {\n",
        "    'prompt': ['What is the capital of Argentina?',\n",
        "               'What is the capital of the United States?'],\n",
        "    'completion': ['Buenos Aires.',\n",
        "                    'Washington D.C.']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RamTCjmVIn4F",
      "metadata": {
        "id": "RamTCjmVIn4F"
      },
      "outputs": [],
      "source": [
        "batch_messages = format_dataset(batch_prompts_completions)['messages']\n",
        "batch_messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50481b56",
      "metadata": {
        "id": "50481b56"
      },
      "source": [
        "#### BYOFF (Bring Your Own Formatting Function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c55159",
      "metadata": {
        "id": "b7c55159"
      },
      "outputs": [],
      "source": [
        "def byo_formatting_func1(examples):\n",
        "    messages = examples[\"messages\"]\n",
        "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return output_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efbffc17",
      "metadata": {
        "id": "efbffc17"
      },
      "outputs": [],
      "source": [
        "ds_msg = Dataset.from_dict({'messages': batch_messages})\n",
        "ds_msg.map(lambda v: tokenizer_phi(byo_formatting_func1(v)), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6488d9c0",
      "metadata": {
        "id": "6488d9c0"
      },
      "outputs": [],
      "source": [
        "def byo_formatting_func2(examples):\n",
        "    response_template = '### Answer:'\n",
        "    text = f\"### Question: {examples['prompt']}\\n{response_template} {examples['completion']}\"\n",
        "    text += tokenizer_phi.eos_token\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1f7c32",
      "metadata": {
        "id": "6a1f7c32"
      },
      "outputs": [],
      "source": [
        "ds_prompt = Dataset.from_dict(batch_prompts_completions)\n",
        "print(byo_formatting_func2(ds_prompt[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf14c94",
      "metadata": {
        "id": "5bf14c94"
      },
      "outputs": [],
      "source": [
        "# this is going to raise an exception\n",
        "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func2(v)), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9591c014",
      "metadata": {
        "id": "9591c014"
      },
      "outputs": [],
      "source": [
        "def byo_formatting_func3(examples):\n",
        "    output_texts = []\n",
        "    response_template = '### Answer:'\n",
        "    for i in range(len(examples['prompt'])):\n",
        "        text = f\"### Question: {examples['prompt'][i]}\\n {response_template} {examples['completion'][i]}\"\n",
        "        text += tokenizer_phi.eos_token\n",
        "        output_texts.append(text)\n",
        "    return output_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0aeda6d",
      "metadata": {
        "id": "d0aeda6d"
      },
      "outputs": [],
      "source": [
        "ds_prompt.map(lambda v: tokenizer_phi(byo_formatting_func3(v)), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b04e6a",
      "metadata": {
        "id": "c0b04e6a"
      },
      "source": [
        "#### BYOFD (Bring Your Own Formatted Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a6a04d4",
      "metadata": {
        "id": "1a6a04d4"
      },
      "outputs": [],
      "source": [
        "def byofd_formatting_func(examples):\n",
        "    messages = examples[\"messages\"]\n",
        "    output_texts = tokenizer_phi.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return {'text': output_texts}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5772c71a",
      "metadata": {
        "id": "5772c71a"
      },
      "outputs": [],
      "source": [
        "formatted_ds = ds_msg.map(byofd_formatting_func, batched=True)\n",
        "formatted_ds['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78305ca1",
      "metadata": {
        "id": "78305ca1"
      },
      "source": [
        "#### Showdown\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/formatting_flow.png?raw=True)\n",
        "\n",
        "<center>Figure 4.5 - Choosing the right configuration for your formatting needs</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4753ab0a",
      "metadata": {
        "id": "4753ab0a"
      },
      "source": [
        "### The Tokenizer\n",
        "\n",
        "****\n",
        "**Summary of \"The Tokenizer\"**\n",
        "- The **tokenizer's vocabulary** is usually **shorter than the model's embedding layer**.\n",
        "  - The difference in size consists of, quite literally, \"empty slots\" that you can use to **create new tokens without resizing** the embedding layer.\n",
        "  - The **size of the embedding layer** is often a **multiple of a power of two** (32, 64, etc.) to optimize **memory allocation**.\n",
        "- The `EOS` token should be **used solely to mark the end of the text** and nothing else.\n",
        "  - Using the `EOS` token for padding may lead to _endless token generation_.\n",
        "- The `PAD` token is often undefined, but you might still need it:\n",
        "  - **DO NOT** assign the `EOS` token as the `PAD` token.\n",
        "  - If the `UNK` token is defined, it is fine to assign it as the `PAD` token.\n",
        "  - If the `UNK` token is undefined, create a new special token as the `PAD` token.\n",
        "  - **WATCH OUT**: If the `PAD` token is left **undefined**, many libraries will **default to assigning it the `EOS` token** instead!\n",
        "- For **generative** models, **padding** should be performed on the **left** side.\n",
        "  - Padding on the _right_ side will train the model to generate _endless sequences of padding tokens_.\n",
        "  - Many tutorials use `tokenizer.padding_side='right'` due to reported overflow issues with the `SFTTrainer` class.\n",
        "    - This is fine **only if you're using packing or packing-like collators** (see the \"Packed Dataset\" section) instead of standard padding.\n",
        "- If you **create new special tokens**, in theory, you should also **fine-tune the embedding layer** (since you're using those \"empty slots\").\n",
        "  - In practice, your model _may_ still work if you **keep the embeddings frozen**.\n",
        "  - Even though the new tokens' representation is _random_ (their embeddings aren't trained), the other trainable parts of the model may still learn to use them \"as is.\"\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29252e77",
      "metadata": {
        "id": "29252e77"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
        "config_phi = AutoConfig.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4328bccd",
      "metadata": {
        "id": "4328bccd"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi(\"Let's tokenize this sentence!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b82f1150",
      "metadata": {
        "id": "b82f1150"
      },
      "source": [
        "#### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a0766d",
      "metadata": {
        "id": "e8a0766d"
      },
      "outputs": [],
      "source": [
        "len(tokenizer_phi), config_phi.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e74a2f6",
      "metadata": {
        "id": "0e74a2f6"
      },
      "outputs": [],
      "source": [
        "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163e5b4e",
      "metadata": {
        "id": "163e5b4e"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi.eos_token, tokenizer_phi.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d7b370",
      "metadata": {
        "id": "c0d7b370"
      },
      "source": [
        "#### The Tokenizer 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1658f5",
      "metadata": {
        "id": "9a1658f5"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi.all_special_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d361cad",
      "metadata": {
        "id": "7d361cad"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95090969",
      "metadata": {
        "id": "95090969"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi.cls_token, tokenizer_phi.sep_token, tokenizer_phi.mask_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f65debf1",
      "metadata": {
        "id": "f65debf1"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi.add_special_tokens({'cls_token': '<cls>', 'sep_token': '<sep>', 'mask_token': '<mask>'})\n",
        "tokenizer_phi.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dc17b7d",
      "metadata": {
        "id": "2dc17b7d"
      },
      "outputs": [],
      "source": [
        "sorted(tokenizer_phi.vocab.items(), key=lambda t: -t[1])[:13]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a33b3026",
      "metadata": {
        "id": "a33b3026"
      },
      "source": [
        "#### The `EOS` Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add45042",
      "metadata": {
        "id": "add45042"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi.pad_token = tokenizer_phi.unk_token\n",
        "tokenizer_phi.pad_token_id = tokenizer_phi.unk_token_id\n",
        "\n",
        "tokenizer_phi.special_tokens_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "167cb4f4",
      "metadata": {
        "id": "167cb4f4"
      },
      "source": [
        "```python\n",
        "# Updating model's configuration for the modified PAD token\n",
        "if getattr(model, \"config\", None) is not None:\n",
        "    model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
        "if (getattr(model, \"generation_config\", None) s not None):\n",
        "    model.config.pad_token_id = tokenizer_phi.pad_token_id\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3333bb99",
      "metadata": {
        "id": "3333bb99"
      },
      "source": [
        "#### The `PAD` Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe01c37",
      "metadata": {
        "id": "bfe01c37"
      },
      "outputs": [],
      "source": [
        "tokenizer_phi.pad_token, tokenizer_phi.padding_side"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d36e92",
      "metadata": {
        "id": "50d36e92"
      },
      "source": [
        "### Data Collators\n",
        "\n",
        "****\n",
        "**Summary of \"Data Collators\"**\n",
        "- You can specify the `data_collator` argument in the `SFTTrainer` class (see Chapter 5).\n",
        "- `DataCollatorForLanguageModeling` is the **default** collator for the `SFTTrainer` class:\n",
        "  - It automatically **replicates the token IDs as labels**.\n",
        "  - It **doesn't shift the labels**, as this is **handled automatically by the model**.\n",
        "  - It includes the full text (both prompt and completion) as labels, making it ideal for instruction-tuning.\n",
        "- If you're further fine-tuning an instruction or chat model, you can use `DataCollatorForCompletionOnlyLM` to **train only on the model's answer (completion)**.\n",
        "  - It also replicates the token IDs as labels but **masks the prompt tokens by replacing their IDs with `-100`**.\n",
        "  - In a **single interaction** (one prompt and one completion), the **response template is enough** to locate the completion.\n",
        "  - In **multiple interactions** (a sequence of prompts and completions), both the **instruction and response templates** are needed to correctly identify and mask the prompt tokens.\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eeacb63",
      "metadata": {
        "id": "8eeacb63"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
        "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
        "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
        "# converts prompt/completion pairs to conversational messages\n",
        "dataset = dataset.map(format_dataset)\n",
        "dataset = dataset.remove_columns([\"prompt\", \"completion\", \"translation\"])\n",
        "len(dataset), dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9841e37c",
      "metadata": {
        "id": "9841e37c"
      },
      "outputs": [],
      "source": [
        "# formatting_func = instructions_formatting_function(tokenizer_phi)\n",
        "formatting_func = conversations_formatting_function(tokenizer_phi, messages_field='messages')\n",
        "dataset = dataset.map(lambda row: {'text': formatting_func(row)}, batched=True, batch_size=32)\n",
        "sequences = dataset['text']\n",
        "print(sequences[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e400ae2",
      "metadata": {
        "id": "2e400ae2"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(lambda row: tokenizer_phi(row['text']))\n",
        "tokenized_dataset = tokenized_dataset.select_columns(['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a276bd5",
      "metadata": {
        "id": "6a276bd5"
      },
      "source": [
        "#### `DataCollatorWithPadding`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fcce437",
      "metadata": {
        "id": "9fcce437"
      },
      "outputs": [],
      "source": [
        "pad_collator = DataCollatorWithPadding(tokenizer_phi)\n",
        "pad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=pad_collator)\n",
        "pad_batch = next(iter(pad_dloader))\n",
        "pad_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07a36a82",
      "metadata": {
        "id": "07a36a82"
      },
      "source": [
        "#### Dude, Where's My Label?\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/shift_labels.png?raw=True)\n",
        "\n",
        "<center>Figure 4.6 - Inputs and their corresponding shifted labels</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "606662ee",
      "metadata": {
        "id": "606662ee"
      },
      "source": [
        "#### `DataCollatorForLanguageModeling`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22713c7",
      "metadata": {
        "id": "b22713c7"
      },
      "outputs": [],
      "source": [
        "lm_collator = DataCollatorForLanguageModeling(tokenizer_phi, mlm=False)\n",
        "lm_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=lm_collator)\n",
        "lm_batch = next(iter(lm_dloader))\n",
        "lm_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b3ef2f1",
      "metadata": {
        "id": "2b3ef2f1"
      },
      "source": [
        "#### `DataCollatorForCompletionOnlyLM`\n",
        "\n",
        "****\n",
        "**IMPORTANT UPDATE**: The `DataCollatorForCompletionOnlyLM` was removed in `trl` version 0.20. This collator masked the user prompt to train only on the LM's completion, excluding prompt tokens from the loss computation by using the response template to detect the start of the completion.\n",
        "\n",
        "In newer versions, this logic is built-in: tokens are automatically ignored during loss computation if `completion_only_loss` or `assistant_only_loss` are set in the `SFTConfig` object. While simpler, this approach is less flexible, as it depends on a compatible chat template.\n",
        "\n",
        "To preserve flexibility and demonstrate the masking process now handled internally, I've copied the original `DataCollatorForCompletionOnlyLM` implementation into the `compatibility_functions.py` (imported at the start of this chapter) and will continue using it for completion-only training.\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c58443",
      "metadata": {
        "id": "c9c58443"
      },
      "outputs": [],
      "source": [
        "response_template = '<|assistant|>' # token id 32001\n",
        "completion_collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
        "                                                      tokenizer=tokenizer_phi)\n",
        "completion_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_collator)\n",
        "completion_batch = next(iter(completion_dloader))\n",
        "completion_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ebcd7dd",
      "metadata": {
        "id": "1ebcd7dd"
      },
      "outputs": [],
      "source": [
        "labels = completion_batch['labels'][0]\n",
        "valid_tokens = (labels >= 0)\n",
        "tokenizer_phi.decode(labels[valid_tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "281b7d1d",
      "metadata": {
        "id": "281b7d1d"
      },
      "source": [
        "##### Multiple Interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58067e2e",
      "metadata": {
        "id": "58067e2e"
      },
      "outputs": [],
      "source": [
        "dummy_chat = \"\"\"<|user|>Hello\n",
        "<|assistant|>How are you?\n",
        "<|user|>I'm fine! You?\n",
        "<|assistant|>I'm fine too!\n",
        "<|endoftext|>\"\"\"\n",
        "\n",
        "dummy_ds = Dataset.from_dict({'text': [dummy_chat]})\n",
        "dummy_ds = dummy_ds.map(lambda row: tokenizer_phi(row['text'])).select_columns(['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7408873c",
      "metadata": {
        "id": "7408873c"
      },
      "outputs": [],
      "source": [
        "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
        "completion_batch = next(iter(completion_dloader))\n",
        "completion_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18bc8194",
      "metadata": {
        "id": "18bc8194"
      },
      "outputs": [],
      "source": [
        "labels = completion_batch['labels']\n",
        "tokenizer_phi.decode(labels[labels >= 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c1a80c5",
      "metadata": {
        "id": "8c1a80c5"
      },
      "outputs": [],
      "source": [
        "instruction_template = '<|user|>'\n",
        "response_template = '<|assistant|>'\n",
        "completion_collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template,\n",
        "                                                      response_template=response_template,\n",
        "                                                      tokenizer=tokenizer_phi)\n",
        "completion_dloader = DataLoader(dummy_ds, batch_size=1, collate_fn=completion_collator)\n",
        "completion_batch = next(iter(completion_dloader))\n",
        "completion_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a052edea",
      "metadata": {
        "id": "a052edea"
      },
      "outputs": [],
      "source": [
        "labels = completion_batch['labels']\n",
        "tokenizer_phi.decode(labels[labels >= 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203234de",
      "metadata": {
        "id": "203234de"
      },
      "source": [
        "#### Label Shifting\n",
        "\n",
        "```python\n",
        "if labels is not None:\n",
        "    # move labels to correct device to enable model parallelism\n",
        "    labels = labels.to(lm_logits.device)\n",
        "    # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "    shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "    labels = labels[:, 1:].contiguous()\n",
        "    loss_fct = CrossEntropyLoss()\n",
        "    lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f742073f",
      "metadata": {
        "id": "f742073f"
      },
      "source": [
        "### Packed Dataset\n",
        "\n",
        "****\n",
        "**IMPORTANT UPDATE**: the `ConstantLengthDataset` has been removed since `trl` version 0.20. The packing of an already tokenized dataset is now performed by the `pack_dataset()` function from `trl.data_utils`. It is possible to approximate the packing behavior of previous versions of `trl` by setting `packing_strategy='wrapped'` in the `SFTConfig`.\n",
        "****\n",
        "\n",
        "****\n",
        "**Summary of \"Packed Dataset\"**\n",
        "- Packing **concatenates** sequences and **splits** them into **equal-sized packs**:\n",
        "  - **No padding tokens** are used.\n",
        "  - Each pack's length must not exceed the **model’s maximum sequence length**.\n",
        "- Packing is natively supported by the `SFTTrainer`:\n",
        "  - Set its `packing` argument to `True`.\n",
        "  - ~It creates an internal `ConstantLengthDataset` to handle the packing~ [removed in v0.20].\n",
        "  - It uses the `pack_dataset()` function to handle the packing.\n",
        "  - You can set `packing_strategy` to `wrapped` to approximate the original packing behavior.\n",
        "  - By default, you **cannot use packing and a collator simultaneously**.\n",
        "- Some **collators can effectively pack** sequences:\n",
        "  - In this case, the **`packing` argument must be set to `False`**, and the collator performs the packing.\n",
        "  - `DataCollatorWithFlattening` is the packing equivalent of `DataCollatorForLanguageModeling`.\n",
        "  - `DataCollatorForCompletionOnlyLM` includes a new argument (`padding_free`) that makes the completion-only collator function like packing.\n",
        "  - Certain models (e.g. Llama, Phi, Mistral, Gemma, OLMo, and a few others) support these collators with Flash Attention 2:\n",
        "    - These models use `position_ids` to **mark the boundaries** between the original sequences packed together.\n",
        "****\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packed_seq.png?raw=True)\n",
        "\n",
        "<center>Figure 4.7 - Packed sequences</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7519b0",
      "metadata": {
        "id": "ad7519b0"
      },
      "outputs": [],
      "source": [
        "sequences = dataset['text']\n",
        "print(sequences[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ee9300-09dd-4ec6-b510-10d6376eb6c5",
      "metadata": {
        "id": "52ee9300-09dd-4ec6-b510-10d6376eb6c5"
      },
      "source": [
        "****\n",
        "**IMPORTANT UPDATE**: the `ConstantLengthDataset` was removed in `trl` v0.20, but we reproduce the original behavior below (the class was imported from `compatibility_functions.py`) to compare its output to the new version's output, using `pack_dataset()` with the `wrapped` strategy.\n",
        "****\n",
        "\n",
        "**BEFORE**: `ConstantLengthDataset`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc4a53a8",
      "metadata": {
        "id": "bc4a53a8"
      },
      "outputs": [],
      "source": [
        "iterator = ConstantLengthDataset(tokenizer_phi, dataset,\n",
        "                                 dataset_text_field='text',\n",
        "                                 seq_length=64, shuffle=False)\n",
        "\n",
        "def data_generator(iterator):\n",
        "    yield from iterator\n",
        "\n",
        "packed_dataset = Dataset.from_generator(\n",
        "    data_generator,\n",
        "    gen_kwargs={\"iterator\": iterator}\n",
        ")\n",
        "packed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00d4e10a",
      "metadata": {
        "id": "00d4e10a"
      },
      "outputs": [],
      "source": [
        "input_ids = packed_dataset['input_ids']\n",
        "tokenizer_phi.decode(input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e37b3c9-1db5-4f6e-802b-22e2b20c45f5",
      "metadata": {
        "id": "0e37b3c9-1db5-4f6e-802b-22e2b20c45f5"
      },
      "source": [
        "**AFTER**: `pack_dataset()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56da4676-f767-4a9e-9328-31af1cfd4017",
      "metadata": {
        "id": "56da4676-f767-4a9e-9328-31af1cfd4017"
      },
      "outputs": [],
      "source": [
        "new_packed_dataset = pack_dataset(tokenized_dataset, seq_length=64, strategy='wrapped')\n",
        "new_packed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba150297-281f-495d-ba03-273ba773c264",
      "metadata": {
        "id": "ba150297-281f-495d-ba03-273ba773c264"
      },
      "outputs": [],
      "source": [
        "input_ids = new_packed_dataset['input_ids']\n",
        "tokenizer_phi.decode(input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ad3c82",
      "metadata": {
        "id": "21ad3c82"
      },
      "source": [
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/packing_flow.png?raw=True)\n",
        "\n",
        "<center>Figure 4.8 - Choosing the right configuration for your data</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f5af2d",
      "metadata": {
        "id": "79f5af2d"
      },
      "source": [
        "#### Collators for Packing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad8df4a",
      "metadata": {
        "id": "9ad8df4a"
      },
      "source": [
        "##### `DataCollatorWithFlattening`\n",
        "\n",
        "![](https://github.com/dvgodoy/FineTuningLLMs/blob/main/images/ch4/collator_flat.png?raw=True)\n",
        "\n",
        "<center>Figure 4.9 - Packing-like collator</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bfc8432",
      "metadata": {
        "id": "9bfc8432"
      },
      "outputs": [],
      "source": [
        "flat_collator = DataCollatorWithFlattening()\n",
        "flat_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=flat_collator)\n",
        "flat_batch = next(iter(flat_dloader))\n",
        "flat_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ecb7453",
      "metadata": {
        "id": "0ecb7453"
      },
      "outputs": [],
      "source": [
        "flat_batch['input_ids'].shape, flat_batch['position_ids'].max() + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af81fdd2",
      "metadata": {
        "id": "af81fdd2"
      },
      "source": [
        "##### `DataCollatorForCompletionOnlyLM`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f900880b",
      "metadata": {
        "id": "f900880b"
      },
      "outputs": [],
      "source": [
        "response_template = '<|assistant|>'\n",
        "completion_nopad_collator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n",
        "                                                            tokenizer=tokenizer_phi,\n",
        "                                                            padding_free=True)\n",
        "completion_nopad_dloader = DataLoader(tokenized_dataset, batch_size=2, collate_fn=completion_nopad_collator)\n",
        "completion_nopad_batch = next(iter(completion_nopad_dloader))\n",
        "completion_nopad_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aed9e818",
      "metadata": {
        "id": "aed9e818"
      },
      "source": [
        "### Advanced: BYOT (Bring Your Own Template)\n",
        "\n",
        "****\n",
        "**Summary of \"Advanced: BYOT\"**\n",
        "- Every template must define a **response template** and, ideally, **end with an `EOS` token**.\n",
        "- Double-check your tokenizer's `EOS`, `PAD`, and `UNK` tokens:\n",
        "  - The `EOS` token must be distinct from both `PAD` and `UNK` tokens.\n",
        "  - The `PAD` and `UNK` tokens can be the same.\n",
        "- **Only resize** the embedding layer if absolutely necessary—i.e., if all \"empty slots\" have already been used:\n",
        "  - When calling the model's `resize_token_embeddings()`, use the `pad_to_multiple_of` argument to ensure the size remains a **multiple of a power of two**.\n",
        "- If you don’t want to create a Jinja template yourself, you can use a default template like ChatML.\n",
        "  - The `trl` package provides the `setup_chat_format()` function, but it has some drawbacks:\n",
        "    - It assigns the `EOS` token to the `PAD` token (you'll need to **fix it manually** afterward).\n",
        "    - It resizes the model's embedding layer by default, even if only to make it shorter (though **you can avoid resizing** by selecting the appropriate `resize_to_multiple_of`).\n",
        "- You can define and apply a **custom template using a formatting function** instead of creating a Jinja template for your tokenizer:\n",
        "  - If you specify the `formatting_func` in the `SFTTrainer` class (see Chapter 5), your tokenizer doesn't need to have a chat template.\n",
        "  - Choose your response template carefully:\n",
        "    - Using **regular words** (e.g. \"## Answer:\") **may cause issues**, as some tokenizers are \"context-dependent\" and might split your response template into multiple tokens.\n",
        "    - Creating an **additional special token for your response template** is safer, as it will be encoded as a **single token**.\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23603681",
      "metadata": {
        "id": "23603681"
      },
      "source": [
        "#### Chat Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ccd450",
      "metadata": {
        "id": "30ccd450"
      },
      "outputs": [],
      "source": [
        "model_opt = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "print(tokenizer_opt.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c44376",
      "metadata": {
        "id": "55c44376"
      },
      "outputs": [],
      "source": [
        "tokenizer_opt.special_tokens_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4a1505",
      "metadata": {
        "id": "dd4a1505"
      },
      "source": [
        "**ChatML**\n",
        "****\n",
        "[ChatML](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md), short for Chat Markup Language, was developed by OpenAI:\n",
        "\n",
        "_____\n",
        "\"_Traditionally, GPT models consumed unstructured text. ChatGPT models instead expect a structured format, called Chat Markup Language (ChatML for short). ChatML documents consist of a sequence of messages._\"\n",
        "_____\n",
        "\n",
        "Each message should contain the role of the participant and their corresponding content, like the conversational format introduced earlier. This is ChatML's Jinja template:\n",
        "\n",
        "```\n",
        "{% for message in messages %}\n",
        "  {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n",
        "{% endfor %}\n",
        "```\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e439158",
      "metadata": {
        "id": "6e439158"
      },
      "outputs": [],
      "source": [
        "len(tokenizer_opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d70086b2",
      "metadata": {
        "id": "d70086b2"
      },
      "outputs": [],
      "source": [
        "model_opt.config.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b91f663",
      "metadata": {
        "id": "9b91f663"
      },
      "outputs": [],
      "source": [
        "def get_multiple_of(vocab_size):\n",
        "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
        "\n",
        "pad_to_multiple_of = get_multiple_of(model_opt.config.vocab_size)\n",
        "pad_to_multiple_of"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aff379c",
      "metadata": {
        "id": "4aff379c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model_opt.resize_token_embeddings(len(tokenizer_opt),\n",
        "                                  pad_to_multiple_of=pad_to_multiple_of)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c9973e",
      "metadata": {
        "id": "82c9973e"
      },
      "outputs": [],
      "source": [
        "def modify_tokenizer(tokenizer,\n",
        "                     alternative_bos_token='<|im_start|>',\n",
        "                     alternative_unk_token='<unk>',\n",
        "                     special_tokens=None,\n",
        "                     tokens=None):\n",
        "    eos_token, bos_token = tokenizer.eos_token, tokenizer.bos_token\n",
        "    pad_token, unk_token = tokenizer.pad_token, tokenizer.unk_token\n",
        "\n",
        "    # BOS token must be different than EOS token\n",
        "    if bos_token == eos_token:\n",
        "        bos_token = alternative_bos_token\n",
        "\n",
        "    # UNK token must be different than EOS token\n",
        "    if unk_token == eos_token:\n",
        "        unk_token = alternative_unk_token\n",
        "\n",
        "    # PAD token must be different than EOS token\n",
        "    # but can be the same as UNK token\n",
        "    if pad_token == eos_token:\n",
        "        pad_token = unk_token\n",
        "\n",
        "    assert bos_token != eos_token, \"Please choose a different BOS token.\"\n",
        "    assert unk_token != eos_token, \"Please choose a different UNK token.\"\n",
        "\n",
        "    # Creates dict for BOS, PAD, and UNK tokens\n",
        "    # Keeps the EOS token as it was originally defined\n",
        "    special_tokens_dict = {'bos_token': bos_token,\n",
        "                           'pad_token': pad_token,\n",
        "                           'unk_token': unk_token}\n",
        "\n",
        "    # If there are additional special tokens, add them\n",
        "    if special_tokens is not None:\n",
        "        if isinstance(special_tokens, list):\n",
        "            special_tokens_dict.update({'additional_special_tokens': special_tokens})\n",
        "\n",
        "    tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    # If there are new regular (not special) tokens to add\n",
        "    if tokens is not None:\n",
        "        if isinstance(tokens, list):\n",
        "            tokenizer.add_tokens(tokens)\n",
        "\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9377ffe7",
      "metadata": {
        "id": "9377ffe7"
      },
      "outputs": [],
      "source": [
        "def jinja_template(tokenizer):\n",
        "    return (\"{% for message in messages %}\"\n",
        "            f\"{{{{'{tokenizer.bos_token}' + message['role'] + '\\n' + message['content'] + '{tokenizer.eos_token}' + '\\n'}}}}\"\n",
        "            \"{% endfor %}\"\n",
        "            \"{% if add_generation_prompt %}\"\n",
        "            f\"{{{{ '{tokenizer.bos_token}assistant\\n' }}}}\"\n",
        "            \"{% endif %}\")\n",
        "\n",
        "def add_template(tokenizer, chat_template=None):\n",
        "    # If not chat template was given, creates a ChatML template\n",
        "    # using the BOS and EOS tokens\n",
        "    if chat_template is None:\n",
        "        chat_template = jinja_template(tokenizer)\n",
        "\n",
        "    # Assigns chat template to tokenizer\n",
        "    tokenizer.chat_template = chat_template\n",
        "\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6869f622",
      "metadata": {
        "id": "6869f622"
      },
      "outputs": [],
      "source": [
        "def get_multiple_of(vocab_size):\n",
        "    return 2**(bin(vocab_size)[::-1].find('1'))\n",
        "\n",
        "def modify_model(model, tokenizer):\n",
        "    # If new tokenizer length exceeds vocabulary size\n",
        "    # resizes it while keeping it a multiple of the same value\n",
        "    if len(tokenizer) > model.config.vocab_size:\n",
        "        pad_to_multiple_of = get_multiple_of(model.vocab_size)\n",
        "        model.resize_token_embeddings(len(tokenizer),\n",
        "                                      pad_to_multiple_of=pad_to_multiple_of)\n",
        "\n",
        "    # Updates token ids on model configurations\n",
        "    if getattr(model, \"config\", None) is not None:\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "        model.config.bos_token_id = tokenizer.bos_token_id\n",
        "        model.config.eos_token_id = tokenizer.eos_token_id\n",
        "    if getattr(model, \"generation_config\", None) is not None:\n",
        "        model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
        "        model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3459c8",
      "metadata": {
        "id": "1b3459c8"
      },
      "outputs": [],
      "source": [
        "tokenizer_opt = modify_tokenizer(tokenizer_opt)\n",
        "tokenizer_opt = add_template(tokenizer_opt)\n",
        "model_opt = modify_model(model_opt, tokenizer_opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b7bf5da",
      "metadata": {
        "id": "4b7bf5da"
      },
      "outputs": [],
      "source": [
        "tokenizer_opt.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96ae3a5",
      "metadata": {
        "id": "d96ae3a5"
      },
      "outputs": [],
      "source": [
        "len(tokenizer_opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a2e433",
      "metadata": {
        "id": "04a2e433"
      },
      "outputs": [],
      "source": [
        "tokenizer_opt.convert_ids_to_tokens(50265)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec72d660",
      "metadata": {
        "id": "ec72d660"
      },
      "outputs": [],
      "source": [
        "model_opt.get_input_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e77014",
      "metadata": {
        "id": "f9e77014"
      },
      "outputs": [],
      "source": [
        "print(tokenizer_opt.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197e5d83",
      "metadata": {
        "id": "197e5d83"
      },
      "outputs": [],
      "source": [
        "messages = ds_msg['messages'][0]\n",
        "print(tokenizer_opt.apply_chat_template(messages, tokenize=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aea9914",
      "metadata": {
        "id": "7aea9914"
      },
      "source": [
        "#### Custom Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41741481",
      "metadata": {
        "id": "41741481"
      },
      "outputs": [],
      "source": [
        "model_opt = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "tokenizer_opt = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "\n",
        "response_template = '##[YODA]##>'\n",
        "tokenizer_opt = modify_tokenizer(tokenizer_opt, special_tokens=[response_template])\n",
        "model_opt = modify_model(model_opt, tokenizer_opt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab49d36",
      "metadata": {
        "id": "2ab49d36"
      },
      "outputs": [],
      "source": [
        "def formatting_func_builder(response_template):\n",
        "    def formatting_func(examples, add_generation_prompt=False):\n",
        "        output_texts = []\n",
        "        for i in range(len(examples['prompt'])):\n",
        "            text = f\"{examples['prompt'][i]}\"\n",
        "            try:\n",
        "                text += f\" {response_template} {examples['completion'][i]}{tokenizer_opt.eos_token}\"\n",
        "            except KeyError:\n",
        "                if add_generation_prompt:\n",
        "                    text += f\" {response_template} \"\n",
        "            output_texts.append(text)\n",
        "        return output_texts\n",
        "    return formatting_func\n",
        "\n",
        "yoda_formatting_func = formatting_func_builder(response_template)\n",
        "yoda_formatting_func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d5d7fa",
      "metadata": {
        "id": "46d5d7fa"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
        "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
        "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
        "\n",
        "formatted_seqs = yoda_formatting_func(dataset)\n",
        "formatted_seqs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b45c968b",
      "metadata": {
        "id": "b45c968b"
      },
      "outputs": [],
      "source": [
        "tokenizer_opt(formatted_seqs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fba69d1",
      "metadata": {
        "id": "4fba69d1"
      },
      "outputs": [],
      "source": [
        "tokenizer_opt.convert_ids_to_tokens(50266)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482f5c0e",
      "metadata": {
        "id": "482f5c0e"
      },
      "outputs": [],
      "source": [
        "yoda_formatting_func({'prompt': ['The Force is strong in you.',\n",
        "                                 'I am your father!']},\n",
        "                     add_generation_prompt=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934abad6",
      "metadata": {
        "id": "934abad6"
      },
      "source": [
        "#### Special Tokens FTW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db3646e",
      "metadata": {
        "id": "1db3646e"
      },
      "outputs": [],
      "source": [
        "tokenizer_llama = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "tokenizer_llama.pad_token = tokenizer_llama.unk_token\n",
        "tokenizer_llama.pad_token_id = tokenizer_llama.unk_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cabcec4b",
      "metadata": {
        "id": "cabcec4b"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b327b52c",
      "metadata": {
        "id": "b327b52c"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer_llama.tokenize(prompt, add_special_tokens=False)\n",
        "token_ids = tokenizer_llama.encode(prompt, add_special_tokens=False)\n",
        "list(zip(tokens, token_ids))[6:11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c09206",
      "metadata": {
        "id": "56c09206"
      },
      "outputs": [],
      "source": [
        "response_template = \"### Assistant:\"\n",
        "tokens = tokenizer_llama.tokenize(response_template, add_special_tokens=False)\n",
        "token_ids = tokenizer_llama.encode(response_template, add_special_tokens=False)\n",
        "list(zip(tokens, token_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2683ec6",
      "metadata": {
        "id": "e2683ec6"
      },
      "outputs": [],
      "source": [
        "dummy_ds = Dataset.from_dict({'text': [prompt]})\n",
        "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])\n",
        "\n",
        "response_template = \"### Assistant:\"\n",
        "\n",
        "bad_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
        "bad_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=bad_collator)\n",
        "bad_batch = next(iter(bad_dloader))\n",
        "bad_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "817f5914",
      "metadata": {
        "id": "817f5914"
      },
      "outputs": [],
      "source": [
        "modified_response_template = \"\\n### Assistant:\"\n",
        "tokens = tokenizer_llama.tokenize(modified_response_template, add_special_tokens=False)\n",
        "token_ids = tokenizer_llama.encode(modified_response_template, add_special_tokens=False)\n",
        "list(zip(tokens, token_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "402905c0",
      "metadata": {
        "id": "402905c0"
      },
      "outputs": [],
      "source": [
        "fixed_token_ids = token_ids[2:]\n",
        "fixed_collator = DataCollatorForCompletionOnlyLM(fixed_token_ids, tokenizer=tokenizer_llama)\n",
        "fixed_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=fixed_collator)\n",
        "fixed_batch = next(iter(fixed_dloader))\n",
        "fixed_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d77bdb",
      "metadata": {
        "id": "86d77bdb"
      },
      "outputs": [],
      "source": [
        "response_template = \"### Assistant:\"\n",
        "tokenizer_llama.add_special_tokens({'additional_special_tokens': [response_template]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb6c648",
      "metadata": {
        "id": "cfb6c648"
      },
      "outputs": [],
      "source": [
        "dummy_tokenized = dummy_ds.map(lambda row: tokenizer_llama(row['text'])).select_columns(['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07db988",
      "metadata": {
        "id": "f07db988"
      },
      "outputs": [],
      "source": [
        "special_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer_llama)\n",
        "special_dloader = DataLoader(dummy_tokenized, batch_size=1, collate_fn=special_collator)\n",
        "special_batch = next(iter(special_dloader))\n",
        "special_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486c75e1",
      "metadata": {
        "id": "486c75e1"
      },
      "source": [
        "### Coming Up in \"Fine-Tuning LLMs\"\n",
        "\n",
        "Chat templates are key to reining in the untamed LLM monsters and teaching them how to have proper conversations with us humans. Cleverly placing cues, or special tokens, along the conversation enables them to learn how to respond when triggered by the right commanding keyword. The training procedure, though, is not without its perils: activations, gradients, and the optimizer all demand huge portions of precious RAM in order to do their jobs. Appeasing these memory-hungry components will take both skill and effort. Configuring the training loop isn’t for the faint of heart. Don’t miss the next challenging chapter of \"Fine-Tuning LLMs.\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}